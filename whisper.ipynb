{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gwilliams2023 with batch type audio\n",
      "Data partitioned on studies ['gwilliams2023'].\n",
      "Train: 135, Unseen Task: 12, Unseen Subject: 45, Unseen Both: 4.\n",
      "\n",
      "SimpleConv initialized with 9295984 parameters, cond: ['study', 'subject']\n",
      "Merger False, merger channels 0\n",
      "ConvBlocks: 6, hidden_dim: 384, params 8858112\n",
      "encoder.named_modules(): [('', WhisperEncoder(\n",
      "  (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (embed_positions): Embedding(1500, 512)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x WhisperEncoderLayer(\n",
      "      (self_attn): WhisperSdpaAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): GELUActivation()\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('conv1', Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))), ('conv2', Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))), ('embed_positions', Embedding(1500, 512)), ('layers', ModuleList(\n",
      "  (0-5): 6 x WhisperEncoderLayer(\n",
      "    (self_attn): WhisperSdpaAttention(\n",
      "      (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): GELUActivation()\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")), ('layers.0', WhisperEncoderLayer(\n",
      "  (self_attn): WhisperSdpaAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('layers.0.self_attn', WhisperSdpaAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")), ('layers.0.self_attn.k_proj', Linear(in_features=512, out_features=512, bias=False)), ('layers.0.self_attn.v_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.0.self_attn.q_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.0.self_attn.out_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.0.self_attn_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.0.activation_fn', GELUActivation()), ('layers.0.fc1', Linear(in_features=512, out_features=2048, bias=True)), ('layers.0.fc2', Linear(in_features=2048, out_features=512, bias=True)), ('layers.0.final_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.1', WhisperEncoderLayer(\n",
      "  (self_attn): WhisperSdpaAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('layers.1.self_attn', WhisperSdpaAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")), ('layers.1.self_attn.k_proj', Linear(in_features=512, out_features=512, bias=False)), ('layers.1.self_attn.v_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.1.self_attn.q_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.1.self_attn.out_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.1.self_attn_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.1.activation_fn', GELUActivation()), ('layers.1.fc1', Linear(in_features=512, out_features=2048, bias=True)), ('layers.1.fc2', Linear(in_features=2048, out_features=512, bias=True)), ('layers.1.final_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.2', WhisperEncoderLayer(\n",
      "  (self_attn): WhisperSdpaAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('layers.2.self_attn', WhisperSdpaAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")), ('layers.2.self_attn.k_proj', Linear(in_features=512, out_features=512, bias=False)), ('layers.2.self_attn.v_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.2.self_attn.q_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.2.self_attn.out_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.2.self_attn_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.2.activation_fn', GELUActivation()), ('layers.2.fc1', Linear(in_features=512, out_features=2048, bias=True)), ('layers.2.fc2', Linear(in_features=2048, out_features=512, bias=True)), ('layers.2.final_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.3', WhisperEncoderLayer(\n",
      "  (self_attn): WhisperSdpaAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('layers.3.self_attn', WhisperSdpaAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")), ('layers.3.self_attn.k_proj', Linear(in_features=512, out_features=512, bias=False)), ('layers.3.self_attn.v_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.3.self_attn.q_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.3.self_attn.out_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.3.self_attn_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.3.activation_fn', GELUActivation()), ('layers.3.fc1', Linear(in_features=512, out_features=2048, bias=True)), ('layers.3.fc2', Linear(in_features=2048, out_features=512, bias=True)), ('layers.3.final_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.4', WhisperEncoderLayer(\n",
      "  (self_attn): WhisperSdpaAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('layers.4.self_attn', WhisperSdpaAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")), ('layers.4.self_attn.k_proj', Linear(in_features=512, out_features=512, bias=False)), ('layers.4.self_attn.v_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.4.self_attn.q_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.4.self_attn.out_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.4.self_attn_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.4.activation_fn', GELUActivation()), ('layers.4.fc1', Linear(in_features=512, out_features=2048, bias=True)), ('layers.4.fc2', Linear(in_features=2048, out_features=512, bias=True)), ('layers.4.final_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.5', WhisperEncoderLayer(\n",
      "  (self_attn): WhisperSdpaAttention(\n",
      "    (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")), ('layers.5.self_attn', WhisperSdpaAttention(\n",
      "  (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      ")), ('layers.5.self_attn.k_proj', Linear(in_features=512, out_features=512, bias=False)), ('layers.5.self_attn.v_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.5.self_attn.q_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.5.self_attn.out_proj', Linear(in_features=512, out_features=512, bias=True)), ('layers.5.self_attn_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layers.5.activation_fn', GELUActivation()), ('layers.5.fc1', Linear(in_features=512, out_features=2048, bias=True)), ('layers.5.fc2', Linear(in_features=2048, out_features=512, bias=True)), ('layers.5.final_layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True)), ('layer_norm', LayerNorm((512,), eps=1e-05, elementwise_affine=True))]\n",
      "AdaLora target modules: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target modules [] not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 131\u001b[0m\n\u001b[1;32m     20\u001b[0m model_config \u001b[38;5;241m=\u001b[39m SimpleConvConfig(\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Str to list of possible conditions\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     mel_normalization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     transformer_decoder_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     87\u001b[0m config \u001b[38;5;241m=\u001b[39m TrainingConfigV1(\n\u001b[1;32m     88\u001b[0m     brain_encoder_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[1;32m     89\u001b[0m     data_partition\u001b[38;5;241m=\u001b[39mdata_partition,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     latent_alignment_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    129\u001b[0m )\n\u001b[0;32m--> 131\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingSessionV1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_partition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/brain/data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaves/phase2/objectives/CLIP_MSE_TEST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclear_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m#     session.train(\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m#         device=\"cuda\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# except KeyboardInterrupt as e:\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m#     print(\"Exited\")\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/brain-decoding/train/training_session_v1.py:66\u001b[0m, in \u001b[0;36mTrainingSessionV1.__init__\u001b[0;34m(self, config, studies, data_path, save_path, clear_cache, max_cache_size, cache_name)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     55\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     56\u001b[0m     studies\u001b[38;5;241m=\u001b[39mstudies,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     cache_name\u001b[38;5;241m=\u001b[39mcache_name,\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# MODEL\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperAlignment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbrain_module_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbrain_encoder_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43madalora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madalora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayers_to_align\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatent_alignment_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m AdamW(\n\u001b[1;32m     74\u001b[0m         [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad],\n\u001b[1;32m     75\u001b[0m         lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m     76\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mweight_decay,\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m~/brain-decoding/models/whisper_alignment.py:81\u001b[0m, in \u001b[0;36mWhisperAlignment.__init__\u001b[0;34m(self, brain_module_config, adalora_config, layers_to_align, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdaLora target modules: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madalora_config\u001b[38;5;241m.\u001b[39mtarget_modules \u001b[38;5;241m=\u001b[39m target_modules\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madalora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/peft/mapping.py:132\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftMixedModel(model, peft_config, adapter_name\u001b[38;5;241m=\u001b[39madapter_name)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPeftModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    135\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/peft/peft_model.py:129\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/peft/tuners/adalora/model.py:65\u001b[0m, in \u001b[0;36mAdaLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     traininable_mode_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/peft/tuners/lora/model.py:136\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:148\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\u001b[38;5;241m.\u001b[39mupdate(peft_config)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:328\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key\u001b[38;5;241m=\u001b[39mkey)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m     )\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_only_adapters_as_trainable(model)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\u001b[38;5;241m.\u001b[39minference_mode:\n",
      "\u001b[0;31mValueError\u001b[0m: Target modules [] not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "# del session.logger\n",
    "# del session\n",
    "\n",
    "import multiprocessing\n",
    "from train.training_session_v1 import TrainingSessionV1\n",
    "from config import TrainingConfigV1\n",
    "from config import SimpleConvConfig\n",
    "\n",
    "data_partition = {\n",
    "    \"gwilliams2023\": {\n",
    "        \"testing_subjects\": [19, 20, 21],\n",
    "        \"testing_tasks\": [0],\n",
    "    },\n",
    "    # \"armeini2022\": {\n",
    "    #     \"testing_subjects\": [],\n",
    "    #     \"testing_tasks\": [8, 9],\n",
    "    # },\n",
    "}\n",
    "\n",
    "model_config = SimpleConvConfig(\n",
    "    # Str to list of possible conditions\n",
    "    mel_normalization=False,\n",
    "    conditions={\n",
    "        \"study\": [],\n",
    "        \"subject\": [],\n",
    "    },\n",
    "    # Channels\n",
    "    in_channels=208,\n",
    "    out_channels=80,\n",
    "    hidden_dim=384,\n",
    "    dropout=0.2,\n",
    "    initial_batch_norm=True,\n",
    "    # Sensor layout settings\n",
    "    layout_dim=2,\n",
    "    layout_proj=True,\n",
    "    layout_scaling=\"minmax\",\n",
    "    # Merger with spatial attn\n",
    "    merger=False,\n",
    "    merger_emb_type=None,\n",
    "    merger_emb_dim=0,\n",
    "    merger_channels=0,\n",
    "    merger_dropout=0.0,  # Float\n",
    "    merger_conditional=None,\n",
    "    # Inital\n",
    "    initial_linear=384,\n",
    "    initial_depth=1,\n",
    "    # Conditional layers\n",
    "    conditional_layers=False,\n",
    "    conditional_layers_dim=None,  # input or hidden_dim\n",
    "    # Conv layer overall structure\n",
    "    depth=6,\n",
    "    kernel_size=3,\n",
    "    growth=1.0,\n",
    "    dilation_growth=2,\n",
    "    dilation_period=5,\n",
    "    glu=1,\n",
    "    conv_dropout=0.2,\n",
    "    dropout_input=0.2,\n",
    "    batch_norm=True,\n",
    "    half=True,\n",
    "    cnn_pos_encoding=False,\n",
    "    # Quantizer\n",
    "    quantizer=False,\n",
    "    num_codebooks=0,\n",
    "    codebook_size=0,\n",
    "    quantizer_commitment=0,\n",
    "    quantizer_temp_init=0,\n",
    "    quantizer_temp_min=0,\n",
    "    quantizer_temp_decay=0,\n",
    "    # Transformers Encoders\n",
    "    transformer_input=None,\n",
    "    transformer_encoder_emb=None,\n",
    "    transformer_encoder_layers=0,\n",
    "    transformer_encoder_heads=0,\n",
    "    # Conformer encoder variant\n",
    "    rnn_type=\"conformer\",\n",
    "    depthwise_conv_kernel_size=31,\n",
    "    use_group_norm=True,\n",
    "    convolution_first=False,\n",
    "    # Transformer Decoders\n",
    "    transformer_decoder_emb=None,\n",
    "    transformer_decoder_layers=0,\n",
    "    transformer_decoder_heads=0,\n",
    "    transformer_decoder_dim=0,\n",
    ")\n",
    "\n",
    "config = TrainingConfigV1(\n",
    "    brain_encoder_config=model_config,\n",
    "    data_partition=data_partition,\n",
    "    # Ada lora settings\n",
    "    adalora_init_r=12,\n",
    "    adalora_target_r=4,\n",
    "    adalora_tinit=100,\n",
    "    adalora_tfinal=1000,\n",
    "    adalora_deltaT=10,\n",
    "    adalora_lora_alpha=32,\n",
    "    adalora_lora_dropout=0.1,\n",
    "    adalora_total_step=100000,\n",
    "    # Pre-processing parameters\n",
    "    # Brain\n",
    "    new_freq=200,\n",
    "    frequency_bands={\"all\": (0.5, 80)},\n",
    "    max_random_shift=1.0,\n",
    "    window_size=4,\n",
    "    window_stride=1,\n",
    "    brain_clipping=None,\n",
    "    baseline_window=0.5,\n",
    "    notch_filter=True,\n",
    "    scaling=\"both\",\n",
    "    delay=0.15,\n",
    "    # Hyperparameters\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    random_test_size=10,\n",
    "    seed=42,\n",
    "    mel_alignment_objectives={\n",
    "        \"clip_loss\": 0.6,\n",
    "        \"mse_loss\": 0.4,\n",
    "        \"commitment_loss\": 0.0,\n",
    "    },\n",
    "    latent_alignment_objectives={\n",
    "        \"cosine_similarity\": 0.4,\n",
    "        \"mse_loss\": 0.4,\n",
    "        \"clip_loss\": 0.2,\n",
    "    },\n",
    "    latent_alignment_layers=[-1],\n",
    ")\n",
    "\n",
    "session = TrainingSessionV1(\n",
    "    config=config,\n",
    "    studies={study: \"audio\" for study in data_partition.keys()},\n",
    "    data_path=\"/home/ubuntu/brain/data\",\n",
    "    save_path=\"saves/phase2/objectives/CLIP_MSE_TEST\",\n",
    "    clear_cache=False,\n",
    "    cache_name=\"cache\",\n",
    ")\n",
    "\n",
    "# try:\n",
    "#     session.train(\n",
    "#         device=\"cuda\",\n",
    "#         buffer_size=30,\n",
    "#         num_workers=(multiprocessing.cpu_count() - 2),\n",
    "#         max_cache_size=400,\n",
    "#         current_epoch=0,\n",
    "#     )\n",
    "# except KeyboardInterrupt as e:\n",
    "#     print(\"Exited\")\n",
    "\n",
    "try:\n",
    "    session.pre_process_all_recordings(\n",
    "        buffer_size=30, num_workers=multiprocessing.cpu_count() - 2, max_cache_size=400\n",
    "    )\n",
    "except KeyboardInterrupt as e:\n",
    "    print(\"Exited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.get_device_capability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in session.model.encoder.named_parameters():\n",
    "    if \"lora_A\" in name or \"lora_B\" in name or \"lora_E\" in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
