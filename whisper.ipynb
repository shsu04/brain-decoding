{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SimpleConvConfig\n",
    "from models.simpleconv import SimpleConv\n",
    "import torch\n",
    "\n",
    "model_config = SimpleConvConfig(\n",
    "    # Str to list of possible conditions\n",
    "    mel_normalization=False,\n",
    "    conditions={\n",
    "        \"study\": [],\n",
    "        \"subject\": [],\n",
    "    },\n",
    "    # Channels\n",
    "    in_channels=208,\n",
    "    out_channels=128,\n",
    "    hidden_dim=384,\n",
    "    dropout=0.2,\n",
    "    initial_batch_norm=True,\n",
    "    # Sensor layout settings\n",
    "    layout_dim=2,\n",
    "    layout_proj=True,\n",
    "    layout_scaling=\"minmax\",\n",
    "    # Merger with spatial attn\n",
    "    merger=False,\n",
    "    merger_emb_type=None,\n",
    "    merger_emb_dim=0,\n",
    "    merger_channels=0,\n",
    "    merger_dropout=False,\n",
    "    merger_conditional=None,\n",
    "    # Inital\n",
    "    initial_linear=384,\n",
    "    initial_depth=1,\n",
    "    # Conditional layers\n",
    "    conditional_layers=False,\n",
    "    conditional_layers_dim=None,  # input or hidden_dim\n",
    "    # Conv layer overall structure\n",
    "    depth=6,\n",
    "    kernel_size=3,\n",
    "    growth=1.0,\n",
    "    dilation_growth=2,\n",
    "    dilation_period=5,\n",
    "    glu=1,\n",
    "    conv_dropout=0.2,\n",
    "    dropout_input=0.2,\n",
    "    batch_norm=True,\n",
    "    half=True,\n",
    "    cnn_pos_encoding=False,\n",
    "    # Quantizer\n",
    "    quantizer=False,\n",
    "    num_codebooks=0,\n",
    "    codebook_size=0,\n",
    "    quantizer_commitment=0,\n",
    "    quantizer_temp_init=0,\n",
    "    quantizer_temp_min=0,\n",
    "    quantizer_temp_decay=0,\n",
    "    # Transformers Encoders\n",
    "    transformer_input=None,\n",
    "    transformer_encoder_emb=None,\n",
    "    transformer_encoder_layers=0,\n",
    "    transformer_encoder_heads=0,\n",
    "    # Conformer encoder variant\n",
    "    rnn_type=\"conformer\",\n",
    "    depthwise_conv_kernel_size=31,\n",
    "    use_group_norm=True,\n",
    "    convolution_first=False,\n",
    "    # Transformer Decoders\n",
    "    transformer_decoder_emb=None,\n",
    "    transformer_decoder_layers=0,\n",
    "    transformer_decoder_heads=0,\n",
    "    transformer_decoder_dim=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import typing as tp\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import typing as tp\n",
    "import json\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import torch\n",
    "from transformers import WhisperModel\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from dataloader import DataLoader\n",
    "from dataloader.audio_batch import AudioBatch\n",
    "from config import TrainingConfigV1\n",
    "from losses.mse import mse_loss_per_batch\n",
    "from losses.cos_sim import cosine_similarity_loss\n",
    "from train.training_session import TrainingSession\n",
    "from models.whisper_alignment import WhisperAlignment\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "class TrainingSessionV1(TrainingSession):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TrainingConfigV1 = None,\n",
    "        studies: tp.Dict[str, str] = None,\n",
    "        data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "        save_path: str = \"/home/ubuntu/brain-decoding/saves\",\n",
    "        clear_cache: bool = False,\n",
    "        max_cache_size: int = 100,\n",
    "        cache_name: str = \"cache\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a training session with the provided configuration and data.\n",
    "        This version deals with audio batches for Whisper latent alignment,\n",
    "        architecture exploration, and dataset integration.\n",
    "\n",
    "        Arguments:\n",
    "            config -- The configuration for the training session.\n",
    "            studies -- dict of studies, batch type. Partition policy determined in TrainingConfig\n",
    "                    Batch type determines how to load data from study.\n",
    "\n",
    "            data_path -- The path to the data directory.\n",
    "            save_path -- The path to the directory where the model and logs will be saved.\n",
    "            clear_cache -- Whether to clear the cache for the studies.\n",
    "            max_cache_size -- The maximum number of stimulis in cache.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            config=config,\n",
    "            studies=studies,\n",
    "            data_path=data_path,\n",
    "            save_path=save_path,\n",
    "            clear_cache=clear_cache,\n",
    "            cache_enabled=True,\n",
    "            max_cache_size=max_cache_size,\n",
    "            cache_name=cache_name,\n",
    "        )\n",
    "\n",
    "        # MODEL\n",
    "        self.model = WhisperAlignment(\n",
    "            brain_module_config=config.brain_encoder_config,\n",
    "            adalora_config=config.adalora_config,\n",
    "            layers_to_align=config.latent_alignment_layers,\n",
    "            use_compile=False,\n",
    "        )\n",
    "        self.model = torch.compile(\n",
    "            self.model.forward, mode=\"reduce-overhead\", fullgraph=True\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.optimizer = AdamW(\n",
    "                [p for p in self.model.parameters() if p.requires_grad],\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.weight_decay,\n",
    "            )\n",
    "            self.scaler = torch.amp.GradScaler(device=device)\n",
    "        else:\n",
    "            self.optimizer = None\n",
    "            self.scaler = None\n",
    "            print(\"CUDA is not available. Optimizer and scaler not initialized\")\n",
    "\n",
    "        self.clip_loss, self.mse_loss, self.cosine_similarity_loss = (\n",
    "            self.model.brain_module.clip_loss,\n",
    "            mse_loss_per_batch,\n",
    "            cosine_similarity_loss,\n",
    "        )\n",
    "\n",
    "        # Frozen whisper model for alignment\n",
    "        frozen_whisper_model = WhisperModel.from_pretrained(\n",
    "            \"openai/whisper-large-v3\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True,\n",
    "        ).to(device)\n",
    "        self.frozen_encoder = frozen_whisper_model.get_encoder()._freeze_parameters()\n",
    "\n",
    "        del frozen_whisper_model.decoder\n",
    "        del frozen_whisper_model\n",
    "\n",
    "        self.frozen_encoder = torch.compile(\n",
    "            self.frozen_encoder.forward, mode=\"reduce-overhead\"\n",
    "        )\n",
    "\n",
    "        self.adalora_steps = 0\n",
    "        self.lowest_final_layer_total_loss = float(\"inf\")\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        device: str,\n",
    "        buffer_size: int,\n",
    "        num_workers: int,\n",
    "        max_cache_size: int,\n",
    "        current_epoch: int = 0,\n",
    "    ):\n",
    "        \"\"\"Max cache size for the cache dir in GB\"\"\"\n",
    "\n",
    "        # Set all training parameters\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.clip_loss.to(device)\n",
    "        training_size = len(self.dataset[\"train\"])\n",
    "\n",
    "        for epoch in range(current_epoch + 1, self.config.epochs + 1):\n",
    "            try:\n",
    "                self.model.to(device).train()\n",
    "                epoch_start_time = time.time()\n",
    "                self.logger.info(f\"Starting epoch {epoch}.\")\n",
    "\n",
    "                # Shuffle for each epoch, and start fetching\n",
    "                epoch_training_dataset = self.dataset[\"train\"].copy()\n",
    "\n",
    "                # Fetch recordings\n",
    "                dataloader = self.get_dataloader(\n",
    "                    buffer_size=buffer_size,\n",
    "                    num_workers=num_workers,\n",
    "                    max_cache_size=max_cache_size,\n",
    "                )\n",
    "\n",
    "                # For reproducibility\n",
    "                self.set_seed(int(self.config.seed + epoch))\n",
    "                random.shuffle(epoch_training_dataset)\n",
    "                dataloader.start_fetching(epoch_training_dataset, cache=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log_print(f\"Error in epoch {epoch} during initialization, {e}\")\n",
    "                self.save(f\"error_epoch_{epoch}\")\n",
    "\n",
    "            pbar = tqdm(\n",
    "                total=len(epoch_training_dataset), desc=\"Training Epoch \" + str(epoch)\n",
    "            )\n",
    "\n",
    "            all_metrics, total_batches = [], 0\n",
    "\n",
    "            # Run each batch\n",
    "            while True:\n",
    "\n",
    "                batch = dataloader.get_recording()\n",
    "                if batch is None:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    results, num_batches = self.run_batch(batch, train=True)\n",
    "                    all_metrics.append(results)\n",
    "                    total_batches += num_batches\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Do log errors\n",
    "                    self.log_print(\n",
    "                        f\"Error in epoch {epoch}, {batch.recording.study_name} {batch.recording.subject_id} {batch.recording.session_id} {batch.recording.task_id}. Skipping. {e}\"\n",
    "                    )\n",
    "                    self.save(f\"error_epoch_{epoch}\")\n",
    "                    raise e\n",
    "\n",
    "                del batch\n",
    "                gc.collect()\n",
    "\n",
    "                pbar.update(1)\n",
    "            pbar.close()\n",
    "\n",
    "            final_metrics = {\n",
    "                \"loss\": sum([batch[\"loss\"] for batch in all_metrics]) / training_size,\n",
    "                \"mel_loss\": sum([batch[\"mel_loss\"] for batch in all_metrics])\n",
    "                / training_size,\n",
    "                \"clip_loss\": sum([batch[\"clip_loss\"] for batch in all_metrics])\n",
    "                / training_size,\n",
    "                \"mse_loss\": sum([batch[\"mse_loss\"] for batch in all_metrics])\n",
    "                / training_size,\n",
    "                \"commitment_loss\": sum(\n",
    "                    [batch[\"commitment_loss\"] for batch in all_metrics]\n",
    "                )\n",
    "                / training_size,\n",
    "                \"perplexity\": sum([batch[\"perplexity\"] for batch in all_metrics])\n",
    "                / training_size,\n",
    "                \"alignment_losses\": {\n",
    "                    key: [\n",
    "                        sum(\n",
    "                            [batch[\"alignment_losses\"][key][i] for batch in all_metrics]\n",
    "                        )\n",
    "                        / training_size\n",
    "                        for i in range(len(self.config.latent_alignment_layers))\n",
    "                    ]\n",
    "                    for key in all_metrics[0][\"alignment_losses\"]\n",
    "                },\n",
    "                \"final_layer_losses\": {\n",
    "                    key: sum(\n",
    "                        [batch[\"final_layer_losses\"][key] for batch in all_metrics]\n",
    "                    )\n",
    "                    / training_size\n",
    "                    for key in all_metrics[0][\"final_layer_losses\"]\n",
    "                },\n",
    "                \"accuracy\": sum([batch[\"accuracy\"] for batch in all_metrics])\n",
    "                / training_size,\n",
    "                \"top_5_accuracy\": sum(\n",
    "                    [batch[\"top_5_accuracy\"] for batch in all_metrics]\n",
    "                )\n",
    "                / training_size,\n",
    "                \"top_10_accuracy\": sum(\n",
    "                    [batch[\"top_10_accuracy\"] for batch in all_metrics]\n",
    "                )\n",
    "                / training_size,\n",
    "            }\n",
    "            self.metrics[\"train\"].append(final_metrics)\n",
    "\n",
    "            self.log_print(\n",
    "                f\"Epoch {epoch}, Loss: {final_metrics['loss']:.4f}, Mel Loss: {final_metrics['mel_loss']:.4f}\"\n",
    "            )\n",
    "            self.log_print(\n",
    "                f\"Clip Loss: {final_metrics['clip_loss']:.4f}, MSE Loss: {final_metrics['mse_loss']:.4f}, Commitment Loss: {final_metrics['commitment_loss']:.4f}\"\n",
    "            )\n",
    "            self.log_print(\n",
    "                f\"Perplexity: {final_metrics['perplexity']:.4f}, Accuracy: {final_metrics['accuracy']:.4f}, Top 5 Accuracy: {final_metrics['top_5_accuracy']:.4f}, Top 10 Accuracy: {final_metrics['top_10_accuracy']:.4f}\"\n",
    "            )\n",
    "            self.log_print(\n",
    "                f\"Final Layer Clip Loss: {final_metrics['final_layer_losses']['clip_loss']:.4f}, Final Layer MSE Loss: {final_metrics['final_layer_losses']['mse_loss']:.4f}, Final Layer Cosine Similarity Loss: {final_metrics['final_layer_losses']['cosine_similarity']:.4f}, Final Layer Total Loss: {final_metrics['final_layer_losses']['total']:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Testing\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    self.logger.info(f\"Starting testing for epoch {epoch}.\")\n",
    "                    self.test(\n",
    "                        buffer_size=buffer_size,\n",
    "                        num_workers=num_workers,\n",
    "                        max_cache_size=max_cache_size,\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                self.log_print(f\"Error in epoch {epoch} during testing, {e}\")\n",
    "                self.save(f\"error_epoch_{epoch}\")\n",
    "                raise e\n",
    "\n",
    "            elapsed_minutes = (time.time() - epoch_start_time) / 60\n",
    "\n",
    "            self.log_print(\n",
    "                f\"Epoch {epoch} completed in {elapsed_minutes:.2f}m. {elapsed_minutes / training_size:.2f}m per recording.\"\n",
    "            )\n",
    "\n",
    "            # Early stopping\n",
    "            average_test_accuracy = (\n",
    "                sum(\n",
    "                    [\n",
    "                        self.metrics[\"test\"][test][-1][\"accuracy\"]\n",
    "                        for test in self.metrics[\"test\"].keys()\n",
    "                    ]\n",
    "                )\n",
    "                / 3\n",
    "            )\n",
    "            average_final_layer_total_loss = (\n",
    "                sum(\n",
    "                    [\n",
    "                        self.metrics[\"test\"][test][-1][\"final_layer_losses\"][\"total\"]\n",
    "                        for test in self.metrics[\"test\"].keys()\n",
    "                    ]\n",
    "                )\n",
    "                / 3\n",
    "            )\n",
    "\n",
    "            if (average_test_accuracy > self.highest_average_test_accuracy) and (\n",
    "                average_final_layer_total_loss < self.lowest_final_layer_total_loss\n",
    "            ):\n",
    "                self.highest_average_test_accuracy = average_test_accuracy\n",
    "                self.lowest_final_layer_total_loss = average_final_layer_total_loss\n",
    "                self.highest_epoch = epoch\n",
    "                self.highest_metrics = {\n",
    "                    test: self.metrics[\"test\"][test][-1]\n",
    "                    for test in self.metrics[\"test\"].keys()\n",
    "                }\n",
    "                self.log_print(\n",
    "                    f\"New highest average test accuracy: {self.highest_average_test_accuracy:.4f}, lowest final layer total loss: {self.lowest_final_layer_total_loss:.4f} at epoch {self.highest_epoch}.\"\n",
    "                )\n",
    "                # Save model\n",
    "                self.save(f\"epoch_{epoch}\")\n",
    "\n",
    "            if epoch - self.highest_epoch > 10:\n",
    "                self.log_print(\n",
    "                    f\"Early stopping at epoch {epoch}. Highest top 10 accuracy at epoch {self.highest_epoch}.\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "        self.log_print(\"Training completed.\")\n",
    "        for test, metrics in self.highest_metrics.items():\n",
    "            self.log_print(\n",
    "                f\"{test}: Acc: {metrics['accuracy']:.4f}, Top 5: {metrics['top_5_accuracy']:.4f}, Top 10: {metrics['top_10_accuracy']:.4f}\"\n",
    "            )\n",
    "            self.log_print(\n",
    "                f\"Final Layer Clip Loss: {metrics['final_layer_losses']['clip_loss']:.4f}, Final Layer MSE Loss: {metrics['final_layer_losses']['mse_loss']:.4f}\"\n",
    "            )\n",
    "            self.log_print(\n",
    "                f'Final Layer Cosine Similarity Loss: {metrics['final_layer_losses']['cosine_similarity']:.4f}, Final Layer Total Loss: {metrics['final_layer_losses']['total']:.4f}'\n",
    "            )\n",
    "\n",
    "    def run_batch(self, batch: AudioBatch, train: bool) -> tp.Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Per recording processing for training and testing. Returns average metrics\n",
    "        and losses for the recording. Returns metrics on CPU.\n",
    "        \"\"\"\n",
    "        # Some processing to ensure dims match\n",
    "        brain_segments, audio_segments, recording = (\n",
    "            batch.brain_segments[\"all\"],\n",
    "            batch.audio_segments,\n",
    "            batch.recording,\n",
    "        )\n",
    "        brain_segments, audio_segments = self.discard_nan(\n",
    "            brain_segments, audio_segments\n",
    "        )\n",
    "\n",
    "        # Initialize recording metrics\n",
    "        (\n",
    "            recording_loss,\n",
    "            recording_mel_loss,\n",
    "            recording_clip_loss,\n",
    "            recording_mse_loss,\n",
    "            recording_commitment_loss,\n",
    "        ) = (0, 0, 0, 0, 0)\n",
    "\n",
    "        recording_latent_alignment_losses = {\n",
    "            \"cosine_similarity\": [0.0 for _ in self.config.latent_alignment_layers],\n",
    "            \"mse_loss\": [0.0 for _ in self.config.latent_alignment_layers],\n",
    "            \"clip_loss\": [0.0 for _ in self.config.latent_alignment_layers],\n",
    "            \"total\": [0.0 for _ in self.config.latent_alignment_layers],\n",
    "        }\n",
    "\n",
    "        (total, missed_recordings, missed_batches) = (\n",
    "            brain_segments.shape[0],\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "        (\n",
    "            recording_correct,\n",
    "            recording_top_5,\n",
    "            recording_top_10,\n",
    "            recording_perplexity,\n",
    "            recording_temp,\n",
    "        ) = (\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        # Models config decides if it is used\n",
    "        conditions = {\n",
    "            \"study\": f\"{recording.study_name}\",\n",
    "            \"subject\": f\"{recording.study_name}_{recording.subject_id}\",\n",
    "        }\n",
    "\n",
    "        # Shuffle segments\n",
    "        shuffle_indices = torch.randperm(brain_segments.shape[0])\n",
    "        brain_segments, audio_segments = (\n",
    "            brain_segments[shuffle_indices],\n",
    "            audio_segments[shuffle_indices],\n",
    "        )  # [B, C, T], [B, mel_bins, T]\n",
    "\n",
    "        # Process by specified batch size\n",
    "        batch_indices = [\n",
    "            (i, min(i + self.config.batch_size, total))\n",
    "            for i in range(0, total, self.config.batch_size)\n",
    "        ]\n",
    "\n",
    "        with torch.amp.autocast(dtype=self.autocast_dtype, device_type=device):\n",
    "            for i, (start, end) in enumerate(batch_indices):\n",
    "                try:\n",
    "                    if train:\n",
    "                        self.optimizer.zero_grad()\n",
    "\n",
    "                    # Slice by batch\n",
    "                    brain_batch, audio_batch = (\n",
    "                        brain_segments[start:end].to(self.device),\n",
    "                        audio_segments[start:end].to(self.device),\n",
    "                    )\n",
    "\n",
    "                    # Brain module\n",
    "\n",
    "                    (\n",
    "                        x,  # [B, C, T]\n",
    "                        quantizer_metrics,\n",
    "                        channel_weights,\n",
    "                        hidden_outputs,\n",
    "                        encoder_hidden_states,  # L * [B, T, D]\n",
    "                    ) = self.model(\n",
    "                        x=[brain_batch],\n",
    "                        recording=[recording],\n",
    "                        conditions=[conditions],\n",
    "                        mel=[audio_batch],\n",
    "                        train=train,\n",
    "                        return_hidden_outputs=False,\n",
    "                    )\n",
    "                    del channel_weights, hidden_outputs, brain_batch\n",
    "\n",
    "                    # Frozen module\n",
    "                    outputs = self.frozen_encoder(\n",
    "                        nn.functional.pad(\n",
    "                            audio_batch,\n",
    "                            (0, 3000 - self.config.window_size),\n",
    "                            mode=\"constant\",\n",
    "                            value=0.0,\n",
    "                        ),\n",
    "                        output_hidden_states=True,\n",
    "                    )\n",
    "                    frozen_encoder_outputs = [\n",
    "                        outputs.hidden_states[l] for l in self.layers_to_align\n",
    "                    ]\n",
    "\n",
    "                    del outputs\n",
    "                    gc.collect()\n",
    "\n",
    "                    # Brain module losses\n",
    "                    mse_loss = self.mse_loss(pred=x, target=audio_batch)\n",
    "\n",
    "                    clip_results = self.clip_loss(x_1=x, x_2=audio_batch)\n",
    "                    clip_loss, clip_metrics = (\n",
    "                        clip_results[\"loss\"],\n",
    "                        clip_results[\"metrics\"],\n",
    "                    )\n",
    "\n",
    "                    # Sum loss based on config\n",
    "                    mel_loss = (\n",
    "                        self.config.mel_alignment_objectives[\"clip_loss\"] * clip_loss\n",
    "                        + self.config.mel_alignment_objectives[\"mse_loss\"] * mse_loss\n",
    "                    )\n",
    "\n",
    "                    if quantizer_metrics is not None:\n",
    "                        if \"commitment_loss\" in quantizer_metrics:\n",
    "                            mel_loss += (\n",
    "                                self.config.mel_alignment_objectives[\"commitment_loss\"]\n",
    "                                * quantizer_metrics[\"commitment_loss\"]\n",
    "                            )\n",
    "\n",
    "                    # Losses by layer\n",
    "                    latent_alignment_losses = {\n",
    "                        \"cosine_similarity\": [],\n",
    "                        \"mse_loss\": [],\n",
    "                        \"clip_loss\": [],\n",
    "                        \"total\": [],\n",
    "                    }\n",
    "\n",
    "                    for l, (frozen_encoder_output, hidden_output) in enumerate(\n",
    "                        zip(frozen_encoder_outputs, encoder_hidden_states)\n",
    "                    ):\n",
    "                        # Align latent spaces\n",
    "                        latent_alignment_losses[\"cosine_similarity\"].append(\n",
    "                            self.cosine_similarity_loss(\n",
    "                                frozen_encoder_output, hidden_output\n",
    "                            )\n",
    "                        )\n",
    "                        latent_alignment_losses[\"mse_loss\"].append(\n",
    "                            self.mse_loss(hidden_output, frozen_encoder_output)\n",
    "                        )\n",
    "                        latent_alignment_clip_results = self.clip_loss(\n",
    "                            hidden_output, frozen_encoder_output\n",
    "                        )\n",
    "                        latent_alignment_losses[\"clip_loss\"].append(\n",
    "                            latent_alignment_clip_results[\"loss\"]\n",
    "                        )\n",
    "                        # Avoid recalculation\n",
    "                        latent_alignment_losses[\"total\"].append(\n",
    "                            self.config.latent_alignment_objectives[\"cosine_similarity\"]\n",
    "                            * latent_alignment_losses[\"cosine_similarity\"][l]\n",
    "                            + self.config.latent_alignment_objectives[\"mse_loss\"]\n",
    "                            * latent_alignment_losses[\"mse_loss\"][l]\n",
    "                            + self.config.latent_alignment_objectives[\"clip_loss\"]\n",
    "                            * latent_alignment_losses[\"clip_loss\"][l]\n",
    "                        )\n",
    "\n",
    "                    loss = sum(latent_alignment_losses[\"total\"]) + mel_loss\n",
    "\n",
    "                    # Backward pass\n",
    "                    if not torch.isnan(loss).any():\n",
    "                        if train:\n",
    "                            self.scaler.scale(loss).backward()\n",
    "                            self.scaler.step(self.optimizer)\n",
    "                            self.scaler.update()\n",
    "                            self.model.encoder.update_and_allocate(self.adalora_steps)\n",
    "                            self.adalora_steps += 1\n",
    "                            self.optimizer.zero_grad()\n",
    "\n",
    "                        # Store brain losses, move to CPU\n",
    "                        recording_loss += loss.detach().to(\"cpu\").item()\n",
    "                        recording_clip_loss += clip_loss.detach().to(\"cpu\").item()\n",
    "                        recording_mse_loss += mse_loss.detach().to(\"cpu\").item()\n",
    "                        recording_mel_loss += mel_loss.detach().to(\"cpu\").item()\n",
    "\n",
    "                        # Quantizer metrics\n",
    "                        if quantizer_metrics is not None:\n",
    "                            if \"perplexity\" in quantizer_metrics:\n",
    "                                perplexity = (\n",
    "                                    quantizer_metrics[\"perplexity\"]\n",
    "                                    .detach()\n",
    "                                    .to(\"cpu\")\n",
    "                                    .mean(dim=0)\n",
    "                                )\n",
    "                                recording_perplexity += perplexity.item()\n",
    "                            if \"temp\" in quantizer_metrics:\n",
    "                                recording_temp += (\n",
    "                                    quantizer_metrics[\"temp\"].detach().to(\"cpu\").item()\n",
    "                                )\n",
    "                            if \"commitment_loss\" in quantizer_metrics:\n",
    "                                recording_commitment_loss += (\n",
    "                                    quantizer_metrics[\"commitment_loss\"]\n",
    "                                    .detach()\n",
    "                                    .to(\"cpu\")\n",
    "                                    .item()\n",
    "                                )\n",
    "\n",
    "                        # Store latent alignment losses\n",
    "                        for key in latent_alignment_losses:\n",
    "                            for l, value in enumerate(latent_alignment_losses[key]):\n",
    "                                recording_latent_alignment_losses[key][l] += (\n",
    "                                    value.detach().to(\"cpu\").item()\n",
    "                                )\n",
    "\n",
    "                        # Store metrics, already on CPU\n",
    "                        recording_correct += clip_metrics[\"correct\"]\n",
    "                        recording_top_5 += clip_metrics[\"top_5_correct\"]\n",
    "                        recording_top_10 += clip_metrics[\"top_10_correct\"]\n",
    "\n",
    "                    else:\n",
    "                        self.log_print(\n",
    "                            f\"Loss is NaN for {recording.study_name} {recording.subject_id} {recording.session_id} {recording.task_id}.\"\n",
    "                        )\n",
    "                        missed_recordings += end - start\n",
    "                        missed_batches += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.log_print(\n",
    "                        f\"Error in processing {recording.study_name} {recording.subject_id} {recording.session_id} {recording.task_id}.\"\n",
    "                    )\n",
    "                    missed_recordings += end - start\n",
    "                    missed_batches += 1\n",
    "                    raise e\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Correct for missed recordings and batches\n",
    "        total -= missed_recordings\n",
    "        batches = len(batch_indices) - missed_batches\n",
    "\n",
    "        for key in recording_latent_alignment_losses:\n",
    "            for l in range(len(recording_latent_alignment_losses[key])):\n",
    "                recording_latent_alignment_losses[key][l] /= batches\n",
    "\n",
    "        final_layer_losses = {\n",
    "            key: recording_latent_alignment_losses[key][-1]\n",
    "            for key in recording_latent_alignment_losses\n",
    "        }\n",
    "\n",
    "        metrics = {\n",
    "            \"loss\": recording_loss / batches,\n",
    "            \"mel_loss\": recording_mel_loss / batches,\n",
    "            \"clip_loss\": recording_clip_loss / batches,\n",
    "            \"mse_loss\": recording_mse_loss / batches,\n",
    "            \"commitment_loss\": (recording_commitment_loss) / batches,\n",
    "            \"perplexity\": recording_perplexity / batches,\n",
    "            \"alignment_losses\": recording_latent_alignment_losses,\n",
    "            \"final_layer_losses\": final_layer_losses,\n",
    "            \"accuracy\": recording_correct / total,\n",
    "            \"top_5_accuracy\": recording_top_5 / total,\n",
    "            \"top_10_accuracy\": recording_top_10 / total,\n",
    "        }\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"{recording.study_name} {recording.subject_id} {recording.session_id} {recording.task_id}, Loss: {metrics['loss']:.4f}\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"Mel Loss: {metrics['mel_loss']:.4f}, Clip Loss: {metrics['clip_loss']:.4f}, MSE Loss: {metrics['mse_loss']:.4f}, Commitment Loss: {metrics['commitment_loss']:.4f}\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"Perplexity: {metrics['perplexity']:.4f}, Accuracy: {metrics['accuracy']:.4f}, Top 5 Accuracy: {metrics['top_5_accuracy']:.4f}, Top 10 Accuracy: {metrics['top_10_accuracy']:.4f}\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"Final Layer Clip Loss: {final_layer_losses['clip_loss']:.4f}, Final Layer MSE Loss: {final_layer_losses['mse_loss']:.4f}, Final Layer Cosine Similarity Loss: {final_layer_losses['cosine_similarity']:.4f}, Final Layer Total Loss: {final_layer_losses['total']:.4f}\"\n",
    "        )\n",
    "\n",
    "        return metrics, total\n",
    "\n",
    "    def test(self, buffer_size: int, num_workers: int, max_cache_size: int):\n",
    "        \"\"\"Max cache size in GB\"\"\"\n",
    "        self.model.eval().to(self.device)\n",
    "        self.set_seed(int(self.config.seed))\n",
    "        test_start_time = time.time()\n",
    "\n",
    "        test_datasets, test_sizes, test_dataloader = {}, {}, {}\n",
    "\n",
    "        # Create dataset and loader\n",
    "        for test in self.dataset[\"test\"].keys():\n",
    "            # Randomly subsample recordings for each type of test\n",
    "            if len(self.dataset[\"test\"][test]) < self.config.random_test_size:\n",
    "                test_datasets[test] = self.dataset[\"test\"][test]\n",
    "            else:\n",
    "                test_datasets[test] = random.sample(\n",
    "                    self.dataset[\"test\"][test], self.config.random_test_size\n",
    "                )\n",
    "\n",
    "            test_sizes[test] = len(test_datasets[test])\n",
    "            test_dataloader[test] = self.get_dataloader(\n",
    "                buffer_size=test_sizes[test],\n",
    "                num_workers=test_sizes[test],\n",
    "                max_cache_size=max_cache_size,\n",
    "            )\n",
    "            test_dataloader[test].start_fetching(test_datasets[test], cache=True)\n",
    "\n",
    "        # Run tests\n",
    "        for test in test_datasets.keys():\n",
    "            all_metrics, total_batches, i = [], 0, 0\n",
    "            while True:\n",
    "                batch = test_dataloader[test].get_recording()\n",
    "                if batch is None:\n",
    "                    break\n",
    "                try:\n",
    "                    results, num_batches = self.run_batch(batch, train=False)\n",
    "                    all_metrics.append(results)\n",
    "                    total_batches += num_batches\n",
    "                    i += 1\n",
    "                except Exception as e:\n",
    "                    self.log_print(\n",
    "                        f\"Error in testing {test}, {batch.recording.study_name} {batch.recording.subject_id} {batch.recording.session_id} {batch.recording.task_id}. Skipping.\"\n",
    "                    )\n",
    "                    test_sizes[test] -= 1\n",
    "                    continue\n",
    "                del batch\n",
    "                gc.collect()\n",
    "        \n",
    "            final_metrics = {\n",
    "                \"loss\": sum([batch[\"loss\"] for batch in all_metrics]) / test_sizes[test],\n",
    "                \"mel_loss\": sum([batch[\"mel_loss\"] for batch in all_metrics])\n",
    "                / test_sizes[test],\n",
    "                \"clip_loss\": sum([batch[\"clip_loss\"] for batch in all_metrics])\n",
    "                / test_sizes[test],\n",
    "                \"mse_loss\": sum([batch[\"mse_loss\"] for batch in all_metrics])\n",
    "                / test_sizes[test],\n",
    "                \"commitment_loss\": sum(\n",
    "                    [batch[\"commitment_loss\"] for batch in all_metrics]\n",
    "                )\n",
    "                / test_sizes[test],\n",
    "                \"perplexity\": sum([batch[\"perplexity\"] for batch in all_metrics])\n",
    "                / test_sizes[test],\n",
    "                \"alignment_losses\": {\n",
    "                    key: [\n",
    "                        sum(\n",
    "                            [batch[\"alignment_losses\"][key][i] for batch in all_metrics]\n",
    "                        )\n",
    "                        / test_sizes[test]\n",
    "                        for i in range(len(self.config.latent_alignment_layers))\n",
    "                    ]\n",
    "                    for key in all_metrics[0][\"alignment_losses\"]\n",
    "                },\n",
    "                \"final_layer_losses\": {\n",
    "                    key: sum(\n",
    "                        [batch[\"final_layer_losses\"][key] for batch in all_metrics]\n",
    "                    )\n",
    "                    / test_sizes[test]\n",
    "                    for key in all_metrics[0][\"final_layer_losses\"]\n",
    "                },\n",
    "                \"accuracy\": sum([batch[\"accuracy\"] for batch in all_metrics])\n",
    "                / test_sizes[test],\n",
    "                \"top_5_accuracy\": sum(\n",
    "                    [batch[\"top_5_accuracy\"] for batch in all_metrics]\n",
    "                )\n",
    "                / test_sizes[test],\n",
    "                \"top_10_accuracy\": sum(\n",
    "                    [batch[\"top_10_accuracy\"] for batch in all_metrics]\n",
    "                )\n",
    "                / test_sizes[test],\n",
    "            }\n",
    "            self.metrics[\"test\"][test].append(final_metrics)\n",
    "            \n",
    "            self.log_print(\n",
    "                f\"Test {test} completed., Loss: {final_metrics['loss']:.4f}, Mel Loss: {final_metrics['mel_loss']:.4f}\"\n",
    "            )\n",
    "            self.log_print(\n",
    "                f\"Clip Loss: {final_metrics['clip_loss']:.4f}, MSE Loss: {final_metrics['mse_loss']:.4f}, Commitment Loss: {final_metrics['commitment_loss']:.4f}\"\n",
    "            )\n",
    "            self.log_print(\n",
    "                f\"Perplexity: {final_metrics['perplexity']:.4f}, Accuracy: {final_metrics['accuracy']:.4f}, Top 5 Accuracy: {final_metrics['top_5_accuracy']:.4f}, Top 10 Accuracy: {final_metrics['top_10_accuracy']:.4f}\"\n",
    "            )\n",
    "            self.log_print(\n",
    "                f\"Final Layer Clip Loss: {final_metrics['final_layer_losses']['clip_loss']:.4f}, Final Layer MSE Loss: {final_metrics['final_layer_losses']['mse_loss']:.4f}, Final Layer Cosine Similarity Loss: {final_metrics['final_layer_losses']['cosine_similarity']:.4f}, Final Layer Total Loss: {final_metrics['final_layer_losses']['total']:.4f}\"\n",
    "            )\n",
    "            test_dataloader[test].stop()\n",
    "\n",
    "        # Log info\n",
    "        elapsed_minutes = (time.time() - test_start_time) / 60\n",
    "        self.log_print(f\"Testing completed in {elapsed_minutes:.2f}m.\")\n",
    "        return\n",
    "\n",
    "    def get_dataloader(self, buffer_size, num_workers, max_cache_size):\n",
    "        dataloader = DataLoader(\n",
    "            buffer_size=buffer_size,\n",
    "            max_cache_size_gb=max_cache_size,\n",
    "            cache_dir=\"cache\",\n",
    "            notch_filter=self.config.notch_filter,\n",
    "            frequency_bands=self.config.frequency_bands,\n",
    "            scaling=self.config.scaling,\n",
    "            brain_clipping=self.config.brain_clipping,\n",
    "            baseline_window=self.config.baseline_window,\n",
    "            new_freq=self.config.new_freq,\n",
    "            delay=self.config.delay,\n",
    "            batch_types={\"audio\": num_workers},\n",
    "            batch_kwargs={\n",
    "                \"audio\": {\n",
    "                    \"max_random_shift\": self.config.max_random_shift,\n",
    "                    \"window_size\": self.config.window_size,\n",
    "                    \"window_stride\": self.config.window_stride,\n",
    "                    \"audio_sample_rate\": self.config.audio_sample_rate,\n",
    "                    \"hop_length\": self.config.hop_length,\n",
    "                    \"audio_processor\": self.config.audio_model,\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def discard_nan(\n",
    "        self,\n",
    "        brain: torch.Tensor,\n",
    "        audio: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        If any nan in brain or audio data, discard the batch.\n",
    "\n",
    "        Arguments:\n",
    "            brain -- The brain data tensor, [B, C, T]\n",
    "            audio -- The audio data, [B, mel_bins, T]\n",
    "        \"\"\"\n",
    "\n",
    "        valid_mask = ~(\n",
    "            torch.isnan(brain).any(dim=(1, 2)) | torch.isnan(audio).any(dim=(1, 2))\n",
    "        )\n",
    "\n",
    "        if valid_mask.all():\n",
    "            return brain, audio\n",
    "\n",
    "        # Apply the same mask to both tensors\n",
    "        filtered_brain = brain[valid_mask]\n",
    "        filtered_audio = audio[valid_mask]\n",
    "\n",
    "        if filtered_brain.shape[0] != filtered_audio.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Filtered brain and audio data must have the same number of samples\"\n",
    "            )\n",
    "\n",
    "        return filtered_brain, filtered_audio\n",
    "\n",
    "    def pre_process_all_recordings(\n",
    "        self, buffer_size: int, num_workers: int, max_cache_size: int\n",
    "    ):\n",
    "        \"\"\"Pre-processes all data and saves as .pt in cache at once.\"\"\"\n",
    "\n",
    "        if self.recordings is None:\n",
    "            self.partition_datasets()\n",
    "\n",
    "        dataloader = self.get_dataloader(buffer_size, num_workers, max_cache_size)\n",
    "\n",
    "        total_recordings, remaining = len(self.recordings), len(self.recordings)\n",
    "        pbar = tqdm(total=total_recordings, desc=\"Loading recordings\")\n",
    "\n",
    "        dataloader.start_fetching(self.recordings)\n",
    "\n",
    "        while True:\n",
    "            recording = dataloader.get_recording()\n",
    "            if recording is None:\n",
    "                break\n",
    "            remaining -= 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    def save(self, name: str):\n",
    "        \"\"\"Saves the model and logs to the save path.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            self.delete_subdirectories(self.save_path)\n",
    "            \n",
    "            # Training session config\n",
    "            if not os.path.exists(self.save_path):\n",
    "                os.makedirs(self.save_path)\n",
    "            \n",
    "            config = self.config.to_dict()\n",
    "            with open(self.save_path + \"/training_config.json\", \"w\") as json_file:\n",
    "                json.dump(config, json_file, indent=4)\n",
    "            checkpoint_path = f\"{self.save_path}/{name}\"\n",
    "            os.makedirs(checkpoint_path, exist_ok=True)\n",
    "                \n",
    "            # Save model\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"config\": self.config.to_dict(),\n",
    "                    \"model\": self.model.cpu().state_dict(),\n",
    "                    \"conditions\": self.model.condition_to_idx,\n",
    "                },\n",
    "                f\"{checkpoint_path}/model.pt\",\n",
    "            )\n",
    "            \n",
    "            # Save metrics\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"metrics\": self.metrics,\n",
    "                    \"error\": str(self.error) if self.error else \"No errors.\",\n",
    "                    \"highest_epoch\": self.highest_epoch,\n",
    "                    \"highest_metrics\": self.highest_metrics,\n",
    "                    \"lowest_final_layer_total_loss\": self.lowest_final_layer_total_loss,\n",
    "                    \"highest_average_test_accuracy\": self.highest_average_test_accuracy,\n",
    "                    \"optimizer\": self.optimizer.state_dict(),\n",
    "                    \"adalora_steps\": self.adalora_steps,\n",
    "                    \"scaler\": self.scaler.state_dict(),\n",
    "                },\n",
    "                f\"{checkpoint_path}/metrics.pt\",\n",
    "            )\n",
    "            \n",
    "        self.model.to(self.device)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return\n",
    "            \n",
    "def load_training_session(\n",
    "    save_path: str,\n",
    "    studies: tp.Dict[str, str] = None,\n",
    "    data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "    clear_cache: bool = False,\n",
    "    cache_enabled: bool = True,\n",
    "    max_cache_size: int = 100,\n",
    "):\n",
    "    \"\"\"Loads a training session from the save path.\"\"\"\n",
    "    # Load training session config\n",
    "    if not os.path.exists(save_path):\n",
    "        raise FileNotFoundError(f\"Save path {save_path} does not exist.\")\n",
    "\n",
    "    try:\n",
    "        load = torch.load(f\"{save_path}/model.pt\", map_location=torch.device(\"cpu\"))\n",
    "        config = load[\"config\"]\n",
    "        config = TrainingConfigV1(\n",
    "            brain_encoder_config=None, data_partition=None\n",
    "        ).from_dict(config)\n",
    "\n",
    "        training_session = TrainingSessionV1(\n",
    "            config=config,\n",
    "            studies=studies,\n",
    "            data_path=data_path,\n",
    "            save_path=\"temp\",\n",
    "            clear_cache=clear_cache,\n",
    "            cache_enabled=cache_enabled,\n",
    "            max_cache_size=max_cache_size,\n",
    "        )\n",
    "        training_session.save_path = save_path\n",
    "        \n",
    "        # Load model\n",
    "        training_session.model.load_state_dict(load[\"model\"])\n",
    "        # Load metrics\n",
    "        metrics_path = os.path.join(save_path, \"metrics.pt\")\n",
    "        \n",
    "        if os.path.exists(metrics_path):\n",
    "            metrics = torch.load(metrics_path)\n",
    "            training_session.metrics = metrics.get(\"metrics\", {})\n",
    "            training_session.highest_epoch = metrics.get(\"highest_epoch\", 0)\n",
    "            training_session.highest_metrics = metrics.get(\"highest_metrics\", {})\n",
    "            training_session.lowest_final_layer_total_loss = metrics.get(\n",
    "                \"lowest_final_layer_total_loss\", float(\"inf\")\n",
    "            )\n",
    "            training_session.highest_average_test_accuracy = metrics.get(\n",
    "                \"highest_average_test_accuracy\", 0\n",
    "            )\n",
    "            training_session.adalora_steps = metrics.get(\"adalora_steps\", 0)\n",
    "            \n",
    "            training_session.optimizer.load_state_dict(metrics[\"optimizer\"])\n",
    "            training_session.scaler.load_state_dict(metrics[\"scaler\"])\n",
    "            training_session.error = metrics[\"error\"]\n",
    "        else:\n",
    "            training_session.metrics = {}\n",
    "            training_session.logger.warning(\n",
    "                f\"Metrics file not found at {metrics_path}.\"\n",
    "            )\n",
    "\n",
    "        if training_session.model.condition_to_idx != load[\"conditions\"]:\n",
    "            raise ValueError(\"Condition to idx mismatch.\")\n",
    "        \n",
    "        shutil.rmtree(\"temp\")\n",
    "        \n",
    "        return training_session\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading training session config, {e}\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
