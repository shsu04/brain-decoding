{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SimpleConvConfig\n",
    "from models.simpleconv import SimpleConv\n",
    "import torch\n",
    "\n",
    "model_config = SimpleConvConfig(\n",
    "    # Str to list of possible conditions\n",
    "    mel_normalization=False,\n",
    "    conditions={\n",
    "        \"study\": [],\n",
    "        \"subject\": [],\n",
    "    },\n",
    "    # Channels\n",
    "    in_channels=208,\n",
    "    out_channels=128,\n",
    "    hidden_dim=384,\n",
    "    dropout=0.2,\n",
    "    initial_batch_norm=True,\n",
    "    # Sensor layout settings\n",
    "    layout_dim=2,\n",
    "    layout_proj=True,\n",
    "    layout_scaling=\"minmax\",\n",
    "    # Merger with spatial attn\n",
    "    merger=False,\n",
    "    merger_emb_type=None,\n",
    "    merger_emb_dim=0,\n",
    "    merger_channels=0,\n",
    "    merger_dropout=False,\n",
    "    merger_conditional=None,\n",
    "    # Inital\n",
    "    initial_linear=384,\n",
    "    initial_depth=1,\n",
    "    # Conditional layers\n",
    "    conditional_layers=False,\n",
    "    conditional_layers_dim=None,  # input or hidden_dim\n",
    "    # Conv layer overall structure\n",
    "    depth=6,\n",
    "    kernel_size=3,\n",
    "    growth=1.0,\n",
    "    dilation_growth=2,\n",
    "    dilation_period=5,\n",
    "    glu=1,\n",
    "    conv_dropout=0.2,\n",
    "    dropout_input=0.2,\n",
    "    batch_norm=True,\n",
    "    half=True,\n",
    "    cnn_pos_encoding=False,\n",
    "    # Quantizer\n",
    "    quantizer=False,\n",
    "    num_codebooks=0,\n",
    "    codebook_size=0,\n",
    "    quantizer_commitment=0,\n",
    "    quantizer_temp_init=0,\n",
    "    quantizer_temp_min=0,\n",
    "    quantizer_temp_decay=0,\n",
    "    # Transformers Encoders\n",
    "    transformer_input=None,\n",
    "    transformer_encoder_emb=None,\n",
    "    transformer_encoder_layers=0,\n",
    "    transformer_encoder_heads=0,\n",
    "    # Transformer Decoders\n",
    "    transformer_decoder_emb=None,\n",
    "    transformer_decoder_layers=0,\n",
    "    transformer_decoder_heads=0,\n",
    "    transformer_decoder_dim=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import typing as tp\n",
    "import json\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from dataloader import DataLoader\n",
    "from dataloader.audio_batch import AudioBatch\n",
    "from config import TrainingConfigV1\n",
    "from losses.mse import mse_loss_per_batch\n",
    "from losses.cos_sim import cosine_similarity_loss\n",
    "from train.training_session import TrainingSession\n",
    "from models.whisper_alignment import WhisperAlignment\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "class TrainingSessionV1(TrainingSession):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TrainingConfigV1 = None,\n",
    "        studies: tp.Dict[str, str] = None,\n",
    "        data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "        save_path: str = \"/home/ubuntu/brain-decoding/saves\",\n",
    "        clear_cache: bool = False,\n",
    "        max_cache_size: int = 100,\n",
    "        cache_name: str = \"cache\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a training session with the provided configuration and data.\n",
    "        This version deals with audio batches for Whisper latent alignment,\n",
    "        architecture exploration, and dataset integration.\n",
    "\n",
    "        Arguments:\n",
    "            config -- The configuration for the training session.\n",
    "            studies -- dict of studies, batch type. Partition policy determined in TrainingConfig\n",
    "                    Batch type determines how to load data from study.\n",
    "\n",
    "            data_path -- The path to the data directory.\n",
    "            save_path -- The path to the directory where the model and logs will be saved.\n",
    "            clear_cache -- Whether to clear the cache for the studies.\n",
    "            max_cache_size -- The maximum number of stimulis in cache.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            config=config,\n",
    "            studies=studies,\n",
    "            data_path=data_path,\n",
    "            save_path=save_path,\n",
    "            clear_cache=clear_cache,\n",
    "            cache_enabled=True,\n",
    "            max_cache_size=max_cache_size,\n",
    "            cache_name=cache_name,\n",
    "        )\n",
    "\n",
    "        # MODEL\n",
    "        self.model = WhisperAlignment(\n",
    "            brain_module_config=config.brain_encoder_config,\n",
    "            adalora_config=config.adalora_config,\n",
    "            layers_to_align=config.latent_alignment_layers,\n",
    "            use_compile=False,\n",
    "        )\n",
    "\n",
    "        self.optimizer = AdamW(\n",
    "            [p for p in self.model.parameters() if p.requires_grad],\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "        )\n",
    "        self.scaler = torch.amp.GradScaler(device=device)\n",
    "\n",
    "        self.clip_loss, self.mse_loss, self.cosine_similarity_loss = (\n",
    "            self.model.brain_module.clip_loss,\n",
    "            mse_loss_per_batch,\n",
    "            cosine_similarity_loss,\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        device: str,\n",
    "        buffer_size: int,\n",
    "        num_workers: int,\n",
    "        max_cache_size: int,\n",
    "        current_epoch: int = 0,\n",
    "    ):\n",
    "        \"\"\"Max cache size for the cache dir in GB\"\"\"\n",
    "\n",
    "        # Set all training parameters\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.clip_loss.to(device)\n",
    "        training_size = len(self.dataset[\"train\"])\n",
    "\n",
    "        for epoch in range(current_epoch + 1, self.config.epochs + 1):\n",
    "            try:\n",
    "                self.model.to(device).train()\n",
    "                epoch_start_time = time.time()\n",
    "\n",
    "                # Shuffle for each epoch, and start fetching\n",
    "                epoch_training_dataset = self.dataset[\"train\"].copy()\n",
    "\n",
    "                # Fetch recordings\n",
    "                dataloader = self.get_dataloader(\n",
    "                    buffer_size=buffer_size,\n",
    "                    num_workers=num_workers,\n",
    "                    max_cache_size=max_cache_size,\n",
    "                )\n",
    "\n",
    "                # For reproducibility\n",
    "                self.set_seed(int(self.config.seed + epoch))\n",
    "                random.shuffle(epoch_training_dataset)\n",
    "                dataloader.start_fetching(epoch_training_dataset, cache=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log_print(f\"Error in epoch {epoch} during initialization, {e}\")\n",
    "                self.save(f\"error_epoch_{epoch}\")\n",
    "\n",
    "            pbar = tqdm(\n",
    "                total=len(epoch_training_dataset), desc=\"Training Epoch \" + str(epoch)\n",
    "            )\n",
    "\n",
    "            all_metrics, total_batches = [], 0\n",
    "\n",
    "            while True:\n",
    "\n",
    "                # Combines multiple recordings so that there is some mixing of\n",
    "                # studies and subjects in each batch\n",
    "                multi_batch = []\n",
    "\n",
    "        #     self.model.encoder.update_and_allocate(step)\n",
    "\n",
    "    def get_dataloader(self, buffer_size, num_workers, max_cache_size):\n",
    "        dataloader = DataLoader(\n",
    "            buffer_size=buffer_size,\n",
    "            max_cache_size_gb=max_cache_size,\n",
    "            cache_dir=\"cache\",\n",
    "            notch_filter=self.config.notch_filter,\n",
    "            frequency_bands=self.config.frequency_bands,\n",
    "            scaling=self.config.scaling,\n",
    "            brain_clipping=self.config.brain_clipping,\n",
    "            baseline_window=self.config.baseline_window,\n",
    "            new_freq=self.config.new_freq,\n",
    "            delay=self.config.delay,\n",
    "            batch_types={\"audio\": num_workers},\n",
    "            batch_kwargs={\n",
    "                \"audio\": {\n",
    "                    \"max_random_shift\": self.config.max_random_shift,\n",
    "                    \"window_size\": self.config.window_size,\n",
    "                    \"window_stride\": self.config.window_stride,\n",
    "                    \"audio_sample_rate\": self.config.audio_sample_rate,\n",
    "                    \"hop_length\": self.config.hop_length,\n",
    "                    \"audio_processor\": self.config.audio_model,\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def discard_nan(\n",
    "        self,\n",
    "        brain: torch.Tensor,\n",
    "        audio: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        If any nan in brain or audio data, discard the batch.\n",
    "\n",
    "        Arguments:\n",
    "            brain -- The brain data tensor, [B, C, T]\n",
    "            audio -- The audio data, [B, mel_bins, T]\n",
    "        \"\"\"\n",
    "\n",
    "        valid_mask = ~(\n",
    "            torch.isnan(brain).any(dim=(1, 2)) | torch.isnan(audio).any(dim=(1, 2))\n",
    "        )\n",
    "\n",
    "        if valid_mask.all():\n",
    "            return brain, audio\n",
    "\n",
    "        # Apply the same mask to both tensors\n",
    "        filtered_brain = brain[valid_mask]\n",
    "        filtered_audio = audio[valid_mask]\n",
    "\n",
    "        if filtered_brain.shape[0] != filtered_audio.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Filtered brain and audio data must have the same number of samples\"\n",
    "            )\n",
    "\n",
    "        return filtered_brain, filtered_audio\n",
    "\n",
    "    def pre_process_all_recordings(\n",
    "        self, buffer_size: int, num_workers: int, max_cache_size: int\n",
    "    ):\n",
    "        \"\"\"Pre-processes all data and saves as .pt in cache at once.\"\"\"\n",
    "\n",
    "        if self.recordings is None:\n",
    "            self.partition_datasets()\n",
    "\n",
    "        dataloader = self.get_dataloader(buffer_size, num_workers, max_cache_size)\n",
    "\n",
    "        total_recordings, remaining = len(self.recordings), len(self.recordings)\n",
    "        pbar = tqdm(total=total_recordings, desc=\"Loading recordings\")\n",
    "\n",
    "        dataloader.start_fetching(self.recordings)\n",
    "\n",
    "        while True:\n",
    "            recording = dataloader.get_recording()\n",
    "            if recording is None:\n",
    "                break\n",
    "            remaining -= 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    def save(self, name: str):\n",
    "        pass\n",
    "\n",
    "\n",
    "def load_training_session(\n",
    "    save_path: str,\n",
    "    studies: tp.Dict[str, str] = None,\n",
    "    data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "    clear_cache: bool = False,\n",
    "    cache_enabled: bool = True,\n",
    "    max_cache_size: int = 100,\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AdaLoraConfig\n",
    "\n",
    "adalora_config = AdaLoraConfig(\n",
    "    peft_type=\"ADALORA\",\n",
    "    task_type=\"CAUSAL_LM\",  # or \"SPEECH_RECOGNITION\" if needed\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # which modules to adapt\n",
    "    init_r=12,  # initial rank\n",
    "    target_r=8,  # final average rank\n",
    "    tinit=100,  # begin rank updates after 100 steps\n",
    "    tfinal=1000,  # finish rank updates by step 1000\n",
    "    deltaT=100,  # re-allocate every 100 steps\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    total_step=2000,  # your total training steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
