{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SimpleConvConfig\n",
    "from models.simpleconv import SimpleConv\n",
    "import torch\n",
    "\n",
    "model_config = SimpleConvConfig(\n",
    "    # Str to list of possible conditions\n",
    "    mel_normalization=False,\n",
    "    conditions={\n",
    "        \"study\": [],\n",
    "        \"subject\": [],\n",
    "    },\n",
    "    # Channels\n",
    "    in_channels=208,\n",
    "    out_channels=128,\n",
    "    hidden_dim=384,\n",
    "    dropout=0.2,\n",
    "    initial_batch_norm=True,\n",
    "    # Sensor layout settings\n",
    "    layout_dim=2,\n",
    "    layout_proj=True,\n",
    "    layout_scaling=\"minmax\",\n",
    "    # Merger with spatial attn\n",
    "    merger=False,\n",
    "    merger_emb_type=None,\n",
    "    merger_emb_dim=0,\n",
    "    merger_channels=0,\n",
    "    merger_dropout=False,\n",
    "    merger_conditional=None,\n",
    "    # Inital\n",
    "    initial_linear=384,\n",
    "    initial_depth=1,\n",
    "    # Conditional layers\n",
    "    conditional_layers=False,\n",
    "    conditional_layers_dim=None,  # input or hidden_dim\n",
    "    # Conv layer overall structure\n",
    "    depth=6,\n",
    "    kernel_size=3,\n",
    "    growth=1.0,\n",
    "    dilation_growth=2,\n",
    "    dilation_period=5,\n",
    "    glu=1,\n",
    "    conv_dropout=0.2,\n",
    "    dropout_input=0.2,\n",
    "    batch_norm=True,\n",
    "    half=True,\n",
    "    cnn_pos_encoding=False,\n",
    "    # Quantizer\n",
    "    quantizer=False,\n",
    "    num_codebooks=0,\n",
    "    codebook_size=0,\n",
    "    quantizer_commitment=0,\n",
    "    quantizer_temp_init=0,\n",
    "    quantizer_temp_min=0,\n",
    "    quantizer_temp_decay=0,\n",
    "    # Transformers Encoders\n",
    "    transformer_input=None,\n",
    "    transformer_encoder_emb=None,\n",
    "    transformer_encoder_layers=0,\n",
    "    transformer_encoder_heads=0,\n",
    "    # Transformer Decoders\n",
    "    transformer_decoder_emb=None,\n",
    "    transformer_decoder_layers=0,\n",
    "    transformer_decoder_heads=0,\n",
    "    transformer_decoder_dim=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import typing as tp\n",
    "import json\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import torch\n",
    "from transformers import WhisperModel\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from dataloader import DataLoader\n",
    "from dataloader.audio_batch import AudioBatch\n",
    "from config import TrainingConfigV1\n",
    "from losses.mse import mse_loss_per_batch\n",
    "from losses.cos_sim import cosine_similarity_loss\n",
    "from train.training_session import TrainingSession\n",
    "from models.whisper_alignment import WhisperAlignment\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "class TrainingSessionV1(TrainingSession):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TrainingConfigV1 = None,\n",
    "        studies: tp.Dict[str, str] = None,\n",
    "        data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "        save_path: str = \"/home/ubuntu/brain-decoding/saves\",\n",
    "        clear_cache: bool = False,\n",
    "        max_cache_size: int = 100,\n",
    "        cache_name: str = \"cache\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a training session with the provided configuration and data.\n",
    "        This version deals with audio batches for Whisper latent alignment,\n",
    "        architecture exploration, and dataset integration.\n",
    "\n",
    "        Arguments:\n",
    "            config -- The configuration for the training session.\n",
    "            studies -- dict of studies, batch type. Partition policy determined in TrainingConfig\n",
    "                    Batch type determines how to load data from study.\n",
    "\n",
    "            data_path -- The path to the data directory.\n",
    "            save_path -- The path to the directory where the model and logs will be saved.\n",
    "            clear_cache -- Whether to clear the cache for the studies.\n",
    "            max_cache_size -- The maximum number of stimulis in cache.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            config=config,\n",
    "            studies=studies,\n",
    "            data_path=data_path,\n",
    "            save_path=save_path,\n",
    "            clear_cache=clear_cache,\n",
    "            cache_enabled=True,\n",
    "            max_cache_size=max_cache_size,\n",
    "            cache_name=cache_name,\n",
    "        )\n",
    "\n",
    "        # MODEL\n",
    "        self.model = WhisperAlignment(\n",
    "            brain_module_config=config.brain_encoder_config,\n",
    "            adalora_config=config.adalora_config,\n",
    "            layers_to_align=config.latent_alignment_layers,\n",
    "            use_compile=False,\n",
    "        )\n",
    "        self.model = torch.compile(\n",
    "            self.model.forward, mode=\"reduce-overhead\", fullgraph=True\n",
    "        )\n",
    "\n",
    "        self.optimizer = AdamW(\n",
    "            [p for p in self.model.parameters() if p.requires_grad],\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "        )\n",
    "        self.scaler = torch.amp.GradScaler(device=device)\n",
    "\n",
    "        self.clip_loss, self.mse_loss, self.cosine_similarity_loss = (\n",
    "            self.model.brain_module.clip_loss,\n",
    "            mse_loss_per_batch,\n",
    "            cosine_similarity_loss,\n",
    "        )\n",
    "\n",
    "        # Frozen whisper model for alignment\n",
    "        frozen_whisper_model = WhisperModel.from_pretrained(\n",
    "            \"openai/whisper-large-v3\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True,\n",
    "        ).to(device)\n",
    "        self.frozen_encoder = frozen_whisper_model.get_encoder()._freeze_parameters()\n",
    "\n",
    "        del frozen_whisper_model.decoder\n",
    "        del frozen_whisper_model\n",
    "\n",
    "        self.frozen_encoder = torch.compile(\n",
    "            self.frozen_encoder.forward, mode=\"reduce-overhead\"\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        device: str,\n",
    "        buffer_size: int,\n",
    "        num_workers: int,\n",
    "        max_cache_size: int,\n",
    "        current_epoch: int = 0,\n",
    "    ):\n",
    "        \"\"\"Max cache size for the cache dir in GB\"\"\"\n",
    "\n",
    "        # Set all training parameters\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        self.clip_loss.to(device)\n",
    "        training_size = len(self.dataset[\"train\"])\n",
    "\n",
    "        for epoch in range(current_epoch + 1, self.config.epochs + 1):\n",
    "            try:\n",
    "                self.model.to(device).train()\n",
    "                epoch_start_time = time.time()\n",
    "\n",
    "                # Shuffle for each epoch, and start fetching\n",
    "                epoch_training_dataset = self.dataset[\"train\"].copy()\n",
    "\n",
    "                # Fetch recordings\n",
    "                dataloader = self.get_dataloader(\n",
    "                    buffer_size=buffer_size,\n",
    "                    num_workers=num_workers,\n",
    "                    max_cache_size=max_cache_size,\n",
    "                )\n",
    "\n",
    "                # For reproducibility\n",
    "                self.set_seed(int(self.config.seed + epoch))\n",
    "                random.shuffle(epoch_training_dataset)\n",
    "                dataloader.start_fetching(epoch_training_dataset, cache=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log_print(f\"Error in epoch {epoch} during initialization, {e}\")\n",
    "                self.save(f\"error_epoch_{epoch}\")\n",
    "\n",
    "            pbar = tqdm(\n",
    "                total=len(epoch_training_dataset), desc=\"Training Epoch \" + str(epoch)\n",
    "            )\n",
    "\n",
    "            all_metrics, total_batches = [], 0\n",
    "\n",
    "            # Run each batch\n",
    "            while True:\n",
    "\n",
    "                batch = dataloader.get_recording()\n",
    "                if batch is None:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    results, num_batches = self.run_batch(batch, train=True)\n",
    "                    all_metrics.append(results)\n",
    "                    total_batches += num_batches\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Do log errors\n",
    "                    self.log_print(\n",
    "                        f\"Error in epoch {epoch}, {batch.recording.study_name} {batch.recording.subject_id} {batch.recording.session_id} {batch.recording.task_id}. Skipping. {e}\"\n",
    "                    )\n",
    "                    self.save(f\"error_epoch_{epoch}\")\n",
    "                    raise e\n",
    "\n",
    "                del batch\n",
    "                gc.collect()\n",
    "\n",
    "                pbar.update(1)\n",
    "            pbar.close()\n",
    "\n",
    "    def run_batch(self, batch: AudioBatch, train: bool) -> tp.Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Per recording processing for training and testing. Returns average metrics\n",
    "        and losses for the recording. Returns metrics on CPU.\n",
    "        \"\"\"\n",
    "        # Some processing to ensure dims match\n",
    "        brain_segments, audio_segments, recording = (\n",
    "            batch.brain_segments[\"all\"],\n",
    "            batch.audio_segments,\n",
    "            batch.recording,\n",
    "        )\n",
    "        brain_segments, audio_segments = self.discard_nan(\n",
    "            brain_segments, audio_segments\n",
    "        )\n",
    "\n",
    "        # Initialize recording metrics\n",
    "        (\n",
    "            recording_loss,\n",
    "            recording_clip_loss,\n",
    "            recording_mse_loss,\n",
    "            recording_commitment_loss,\n",
    "        ) = (0, 0, 0, 0)\n",
    "\n",
    "        recording_latent_alignment_losses = {\n",
    "            \"cosine_similarity\": 0.0,\n",
    "            \"mse_loss\": 0.0,\n",
    "            \"clip_loss\": 0.0,\n",
    "            \"total\": 0.0,\n",
    "        }\n",
    "\n",
    "        (total, missed_recordings, missed_batches) = (\n",
    "            brain_segments.shape[0],\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "        (\n",
    "            recording_correct,\n",
    "            recording_top_5,\n",
    "            recording_top_10,\n",
    "            recording_perplexity,\n",
    "            recording_temp,\n",
    "        ) = (\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        # Models config decides if it is used\n",
    "        conditions = {\n",
    "            \"study\": f\"{recording.study_name}\",\n",
    "            \"subject\": f\"{recording.study_name}_{recording.subject_id}\",\n",
    "        }\n",
    "\n",
    "        # Shuffle segments\n",
    "        shuffle_indices = torch.randperm(brain_segments.shape[0])\n",
    "        brain_segments, audio_segments = (\n",
    "            brain_segments[shuffle_indices],\n",
    "            audio_segments[shuffle_indices],\n",
    "        )  # [B, C, T], [B, mel_bins, T]\n",
    "\n",
    "        # Process by specified batch size\n",
    "        batch_indices = [\n",
    "            (i, min(i + self.config.batch_size, total))\n",
    "            for i in range(0, total, self.config.batch_size)\n",
    "        ]\n",
    "\n",
    "        with torch.amp.autocast(dtype=self.autocast_dtype, device_type=device):\n",
    "            for i, (start, end) in enumerate(batch_indices):\n",
    "                try:\n",
    "                    if train:\n",
    "                        self.optimizer.zero_grad()\n",
    "\n",
    "                    # Slice by batch\n",
    "                    brain_batch, audio_batch = (\n",
    "                        brain_segments[start:end].to(self.device),\n",
    "                        audio_segments[start:end].to(self.device),\n",
    "                    )\n",
    "\n",
    "                    # Brain module\n",
    "                    (\n",
    "                        x,  # [B, C, T]\n",
    "                        quantizer_metrics,\n",
    "                        channel_weights,\n",
    "                        hidden_outputs,\n",
    "                        encoder_hidden_states,  # L * [B, T, D]\n",
    "                    ) = self.model(\n",
    "                        x=[brain_batch],\n",
    "                        recording=[recording],\n",
    "                        conditions=[conditions],\n",
    "                        mel=[audio_batch],\n",
    "                        train=train,\n",
    "                        return_hidden_outputs=False,\n",
    "                    )\n",
    "                    del channel_weights, hidden_outputs, brain_batch\n",
    "\n",
    "                    # Frozen module\n",
    "                    outputs = self.frozen_encoder(\n",
    "                        nn.functional.pad(\n",
    "                            audio_batch,\n",
    "                            (0, 3000 - self.config.window_size),\n",
    "                            mode=\"constant\",\n",
    "                            value=0.0,\n",
    "                        ),\n",
    "                        output_hidden_states=True,\n",
    "                    )\n",
    "                    frozen_encoder_outputs = (\n",
    "                        [outputs.hidden_states[i] for i in self.layers_to_align],\n",
    "                    )\n",
    "\n",
    "                    del outputs\n",
    "                    gc.collect()\n",
    "\n",
    "                    # Brain module losses\n",
    "                    mse_loss = self.mse_loss(pred=x, target=audio_batch)\n",
    "\n",
    "                    clip_results = self.clip_loss(x_1=x, x_2=audio_batch)\n",
    "                    clip_loss, clip_metrics = (\n",
    "                        clip_results[\"loss\"],\n",
    "                        clip_results[\"metrics\"],\n",
    "                    )\n",
    "\n",
    "                    # Sum loss based on config\n",
    "                    mel_loss = (\n",
    "                        self.config.mel_alignment_objectives[\"clip_loss\"] * clip_loss\n",
    "                        + self.config.mel_alignment_objectives[\"mse_loss\"] * mse_loss\n",
    "                    )\n",
    "\n",
    "                    if quantizer_metrics is not None:\n",
    "                        if \"commitment_loss\" in quantizer_metrics:\n",
    "                            mel_loss += (\n",
    "                                self.config.mel_alignment_objectives[\"commitment_loss\"]\n",
    "                                * quantizer_metrics[\"commitment_loss\"]\n",
    "                            )\n",
    "\n",
    "                    # Losses by layer\n",
    "                    latent_alignment_losses = {\n",
    "                        \"cosine_similarity\": [],\n",
    "                        \"mse_loss\": [],\n",
    "                        \"clip_loss\": [],\n",
    "                        \"total\": [],\n",
    "                    }\n",
    "\n",
    "                    for i, (frozen_encoder_output, hidden_output) in enumerate(\n",
    "                        zip(frozen_encoder_outputs, encoder_hidden_states)\n",
    "                    ):\n",
    "                        # Align latent spaces\n",
    "                        latent_alignment_losses[\"cosine_similarity\"].append(\n",
    "                            self.cosine_similarity_loss(\n",
    "                                frozen_encoder_output, hidden_output\n",
    "                            )\n",
    "                        )\n",
    "                        latent_alignment_losses[\"mse_loss\"].append(\n",
    "                            self.mse_loss(hidden_output, frozen_encoder_output)\n",
    "                        )\n",
    "                        latent_alignment_losses[\"clip_loss\"].append(\n",
    "                            self.clip_loss(hidden_output, frozen_encoder_output)\n",
    "                        )\n",
    "                        # Avoid recalculation\n",
    "                        latent_alignment_losses[\"total\"].append(\n",
    "                            self.config.latent_alignment_objectives[\"cosine_similarity\"]\n",
    "                            * latent_alignment_losses[\"cosine_similarity\"][i]\n",
    "                            + self.config.latent_alignment_objectives[\"mse_loss\"]\n",
    "                            * latent_alignment_losses[\"mse_loss\"][i]\n",
    "                            + self.config.latent_alignment_objectives[\"clip_loss\"]\n",
    "                            * latent_alignment_losses[\"clip_loss\"][i]\n",
    "                        )\n",
    "\n",
    "                    # # Sum losses\n",
    "                    # latent_alignment_loss_by_layer = [\n",
    "                    #     self.config.latent_alignment_objectives[\"cosine_similarity\"]\n",
    "                    #     * latent_alignment_losses[\"cosine_similarity\"][i]\n",
    "                    #     + self.config.latent_alignment_objectives[\"mse_loss\"]\n",
    "                    #     * latent_alignment_losses[\"mse_loss\"][i]\n",
    "                    #     + self.config.latent_alignment_objectives[\"clip_loss\"]\n",
    "                    #     * latent_alignment_losses[\"clip_loss\"][i]\n",
    "                    #     for i in range(\n",
    "                    #         len(latent_alignment_losses[\"cosine_similarity\"])\n",
    "                    #     )\n",
    "                    # ]\n",
    "                    # latent_alignment_total_loss = sum(latent_alignment_loss_by_layer)\n",
    "\n",
    "                    # # Total loss\n",
    "                    # loss = mel_loss + latent_alignment_total_loss\n",
    "\n",
    "                    # # Backward pass\n",
    "                    # if not torch.isnan(loss).any():\n",
    "                    #     if train:\n",
    "                    #         self.scaler.scale(loss).backward()\n",
    "                    #         self.scaler.step(self.optimizer)\n",
    "                    #         self.scaler.update()\n",
    "                    #         self.model.encoder.update_and_allocate(i)\n",
    "\n",
    "                    #     # Store brain losses, move to CPU\n",
    "                    #     recording_loss += loss.detach().to(\"cpu\").item()\n",
    "                    #     recording_clip_loss += clip_loss.detach().to(\"cpu\").item()\n",
    "                    #     recording_mse_loss += mse_loss.detach().to(\"cpu\").item()\n",
    "\n",
    "                    #     if (\n",
    "                    #         quantizer_metrics is not None\n",
    "                    #         and \"commitment_loss\" in quantizer_metrics\n",
    "                    #     ):\n",
    "                    #         recording_commitment_loss += (\n",
    "                    #             quantizer_metrics[\"commitment_loss\"]\n",
    "                    #             .detach()\n",
    "                    #             .to(\"cpu\")\n",
    "                    #             .item()\n",
    "                    #         )\n",
    "\n",
    "                    #     # Store latent alignment losses\n",
    "                    #     for key in latent_alignment_losses:\n",
    "                    #         recording_latent_alignment_losses[key] += (\n",
    "                    #             sum(latent_alignment_losses[key])\n",
    "                    #             .detach()\n",
    "                    #             .to(\"cpu\")\n",
    "                    #             .item()\n",
    "                    #         )\n",
    "                    #     recording_latent_alignment_losses[\"total\"] += (\n",
    "                    #         latent_alignment_total_loss.detach().to(\"cpu\").item()\n",
    "                    #     )\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.log_print(\n",
    "                        f\"Error in processing {recording.study_name} {recording.subject_id} {recording.session_id} {recording.task_id}.\"\n",
    "                    )\n",
    "                    missed_recordings += end - start\n",
    "                    missed_batches += 1\n",
    "                    raise e\n",
    "\n",
    "    def get_dataloader(self, buffer_size, num_workers, max_cache_size):\n",
    "        dataloader = DataLoader(\n",
    "            buffer_size=buffer_size,\n",
    "            max_cache_size_gb=max_cache_size,\n",
    "            cache_dir=\"cache\",\n",
    "            notch_filter=self.config.notch_filter,\n",
    "            frequency_bands=self.config.frequency_bands,\n",
    "            scaling=self.config.scaling,\n",
    "            brain_clipping=self.config.brain_clipping,\n",
    "            baseline_window=self.config.baseline_window,\n",
    "            new_freq=self.config.new_freq,\n",
    "            delay=self.config.delay,\n",
    "            batch_types={\"audio\": num_workers},\n",
    "            batch_kwargs={\n",
    "                \"audio\": {\n",
    "                    \"max_random_shift\": self.config.max_random_shift,\n",
    "                    \"window_size\": self.config.window_size,\n",
    "                    \"window_stride\": self.config.window_stride,\n",
    "                    \"audio_sample_rate\": self.config.audio_sample_rate,\n",
    "                    \"hop_length\": self.config.hop_length,\n",
    "                    \"audio_processor\": self.config.audio_model,\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def discard_nan(\n",
    "        self,\n",
    "        brain: torch.Tensor,\n",
    "        audio: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        If any nan in brain or audio data, discard the batch.\n",
    "\n",
    "        Arguments:\n",
    "            brain -- The brain data tensor, [B, C, T]\n",
    "            audio -- The audio data, [B, mel_bins, T]\n",
    "        \"\"\"\n",
    "\n",
    "        valid_mask = ~(\n",
    "            torch.isnan(brain).any(dim=(1, 2)) | torch.isnan(audio).any(dim=(1, 2))\n",
    "        )\n",
    "\n",
    "        if valid_mask.all():\n",
    "            return brain, audio\n",
    "\n",
    "        # Apply the same mask to both tensors\n",
    "        filtered_brain = brain[valid_mask]\n",
    "        filtered_audio = audio[valid_mask]\n",
    "\n",
    "        if filtered_brain.shape[0] != filtered_audio.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Filtered brain and audio data must have the same number of samples\"\n",
    "            )\n",
    "\n",
    "        return filtered_brain, filtered_audio\n",
    "\n",
    "    def pre_process_all_recordings(\n",
    "        self, buffer_size: int, num_workers: int, max_cache_size: int\n",
    "    ):\n",
    "        \"\"\"Pre-processes all data and saves as .pt in cache at once.\"\"\"\n",
    "\n",
    "        if self.recordings is None:\n",
    "            self.partition_datasets()\n",
    "\n",
    "        dataloader = self.get_dataloader(buffer_size, num_workers, max_cache_size)\n",
    "\n",
    "        total_recordings, remaining = len(self.recordings), len(self.recordings)\n",
    "        pbar = tqdm(total=total_recordings, desc=\"Loading recordings\")\n",
    "\n",
    "        dataloader.start_fetching(self.recordings)\n",
    "\n",
    "        while True:\n",
    "            recording = dataloader.get_recording()\n",
    "            if recording is None:\n",
    "                break\n",
    "            remaining -= 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    def save(self, name: str):\n",
    "        pass\n",
    "\n",
    "\n",
    "def load_training_session(\n",
    "    save_path: str,\n",
    "    studies: tp.Dict[str, str] = None,\n",
    "    data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "    clear_cache: bool = False,\n",
    "    cache_enabled: bool = True,\n",
    "    max_cache_size: int = 100,\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AdaLoraConfig\n",
    "\n",
    "adalora_config = AdaLoraConfig(\n",
    "    peft_type=\"ADALORA\",\n",
    "    task_type=\"CAUSAL_LM\",  # or \"SPEECH_RECOGNITION\" if needed\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # which modules to adapt\n",
    "    init_r=12,  # initial rank\n",
    "    target_r=8,  # final average rank\n",
    "    tinit=100,  # begin rank updates after 100 steps\n",
    "    tfinal=1000,  # finish rank updates by step 1000\n",
    "    deltaT=100,  # re-allocate every 100 steps\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    total_step=2000,  # your total training steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
