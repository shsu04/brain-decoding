{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SimpleConvConfig\n",
    "from models.simpleconv import SimpleConv\n",
    "import torch\n",
    "\n",
    "model_config = SimpleConvConfig(\n",
    "    # Str to list of possible conditions\n",
    "    mel_normalization=False,\n",
    "    conditions={\n",
    "        \"study\": [],\n",
    "        \"subject\": [],\n",
    "    },\n",
    "    # Channels\n",
    "    in_channels=208,\n",
    "    out_channels=128,\n",
    "    hidden_dim=384,\n",
    "    dropout=0.2,\n",
    "    initial_batch_norm=True,\n",
    "    # Sensor layout settings\n",
    "    layout_dim=2,\n",
    "    layout_proj=True,\n",
    "    layout_scaling=\"minmax\",\n",
    "    # Merger with spatial attn\n",
    "    merger=False,\n",
    "    merger_emb_type=None,\n",
    "    merger_emb_dim=0,\n",
    "    merger_channels=0,\n",
    "    merger_dropout=False,\n",
    "    merger_conditional=None,\n",
    "    # Inital\n",
    "    initial_linear=384,\n",
    "    initial_depth=1,\n",
    "    # Conditional layers\n",
    "    conditional_layers=False,\n",
    "    conditional_layers_dim=None,  # input or hidden_dim\n",
    "    # Conv layer overall structure\n",
    "    depth=6,\n",
    "    kernel_size=3,\n",
    "    growth=1.0,\n",
    "    dilation_growth=2,\n",
    "    dilation_period=5,\n",
    "    glu=1,\n",
    "    conv_dropout=0.2,\n",
    "    dropout_input=0.2,\n",
    "    batch_norm=True,\n",
    "    half=True,\n",
    "    cnn_pos_encoding=False,\n",
    "    # Quantizer\n",
    "    quantizer=False,\n",
    "    num_codebooks=0,\n",
    "    codebook_size=0,\n",
    "    quantizer_commitment=0,\n",
    "    quantizer_temp_init=0,\n",
    "    quantizer_temp_min=0,\n",
    "    quantizer_temp_decay=0,\n",
    "    # Transformers Encoders\n",
    "    transformer_input=None,\n",
    "    transformer_encoder_emb=None,\n",
    "    transformer_encoder_layers=0,\n",
    "    transformer_encoder_heads=0,\n",
    "    # Transformer Decoders\n",
    "    transformer_decoder_emb=None,\n",
    "    transformer_decoder_layers=0,\n",
    "    transformer_decoder_heads=0,\n",
    "    transformer_decoder_dim=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import typing as tp\n",
    "import json\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from config import SimpleConvConfig, TrainingConfigV1\n",
    "from train.training_session import TrainingSession\n",
    "from models.whisper_alignment import WhisperAlignment\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "class TrainingSessionV1(TrainingSession):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TrainingConfigV1 = None,\n",
    "        studies: tp.Dict[str, str] = None,\n",
    "        data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "        save_path: str = \"/home/ubuntu/brain-decoding/saves\",\n",
    "        clear_cache: bool = False,\n",
    "        max_cache_size: int = 100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a training session with the provided configuration and data.\n",
    "        This version deals with audio batches for Whisper latent alignment,\n",
    "        architecture exploration, and dataset integration.\n",
    "\n",
    "        Arguments:\n",
    "            config -- The configuration for the training session.\n",
    "            studies -- dict of studies, batch type. Partition policy determined in TrainingConfig\n",
    "                    Batch type determines how to load data from study.\n",
    "\n",
    "            data_path -- The path to the data directory.\n",
    "            save_path -- The path to the directory where the model and logs will be saved.\n",
    "            clear_cache -- Whether to clear the cache for the studies.\n",
    "            max_cache_size -- The maximum number of stimulis in cache.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            config=config,\n",
    "            studies=studies,\n",
    "            data_path=data_path,\n",
    "            save_path=save_path,\n",
    "            clear_cache=clear_cache,\n",
    "            cache_enabled=True,\n",
    "            max_cache_size=max_cache_size,\n",
    "        )\n",
    "\n",
    "        # MODEL\n",
    "        self.model = WhisperAlignment(\n",
    "            brain_module_config=config.brain_encoder_config,\n",
    "            adalora_config=config.adalora_config,\n",
    "            layers_to_align=config.latent_alignment_layers,\n",
    "            use_compile=False,\n",
    "        )\n",
    "\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "        )\n",
    "        self.scaler = torch.amp.GradScaler(device=device)\n",
    "\n",
    "        # NOTE: ADD LOSS DICTORARIES\n",
    "        self.losses = {}\n",
    "\n",
    "    # def train(\n",
    "    #     self,\n",
    "    #     device: str,\n",
    "    #     buffer_size: int,\n",
    "    #     num_workers: int,\n",
    "    #     max_cache_size: int,\n",
    "    #     current_epoch: int = 0,\n",
    "    # ):\n",
    "    #     self.model.encoder.update_and_allocate(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AdaLoraConfig\n",
    "\n",
    "adalora_config = AdaLoraConfig(\n",
    "    peft_type=\"ADALORA\",\n",
    "    task_type=\"CAUSAL_LM\",  # or \"SPEECH_RECOGNITION\" if needed\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # which modules to adapt\n",
    "    init_r=12,  # initial rank\n",
    "    target_r=8,  # final average rank\n",
    "    tinit=100,  # begin rank updates after 100 steps\n",
    "    tfinal=1000,  # finish rank updates by step 1000\n",
    "    deltaT=100,  # re-allocate every 100 steps\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    total_step=2000,  # your total training steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
