{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gwilliams2023 with batch type audio\n"
     ]
    }
   ],
   "source": [
    "from studies.gwilliams2023 import Gwilliams2023\n",
    "from studies.armeini2022 import Armeini2022\n",
    "\n",
    "\n",
    "study = Gwilliams2023(\n",
    "    batch_type=\"audio\",\n",
    "    download=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = study.recordings[0][0][0]\n",
    "raw = rec.load_raw(load_data=True)\n",
    "events = rec.load_events(raw, options=\"both\")\n",
    "word_events = events[\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "onset",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "duration",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1c74eaa3-f13d-4a86-811d-a8ca1d51d3d9",
       "rows": [
        [
         "0",
         "23.506",
         "0.3",
         "Tara"
        ],
        [
         "1",
         "23.816",
         "0.24",
         "stood"
        ],
        [
         "2",
         "24.056",
         "0.37",
         "stock"
        ],
        [
         "3",
         "24.586",
         "0.3999999999999999",
         "still"
        ],
        [
         "4",
         "25.136",
         "0.4100000000000001",
         "waiting"
        ],
        [
         "5",
         "25.546",
         "0.1299999999999999",
         "for"
        ],
        [
         "6",
         "25.676",
         "0.0899999999999998",
         "the"
        ],
        [
         "7",
         "25.766",
         "0.27",
         "first"
        ],
        [
         "8",
         "26.046",
         "0.31",
         "tiny"
        ],
        [
         "9",
         "26.356",
         "0.2399999999999997",
         "gleam"
        ],
        [
         "10",
         "26.606",
         "0.1799999999999997",
         "from"
        ],
        [
         "11",
         "26.786",
         "0.0700000000000002",
         "the"
        ],
        [
         "12",
         "26.856",
         "0.29",
         "scout"
        ],
        [
         "13",
         "27.146",
         "0.29",
         "craft"
        ],
        [
         "14",
         "27.446",
         "0.1499999999999999",
         "to"
        ],
        [
         "15",
         "27.596",
         "0.2800000000000002",
         "appear"
        ],
        [
         "16",
         "27.876",
         "0.08",
         "in"
        ],
        [
         "17",
         "27.956",
         "0.0899999999999998",
         "the"
        ],
        [
         "18",
         "28.046",
         "0.4199999999999999",
         "darkness"
        ],
        [
         "19",
         "28.466",
         "0.1399999999999996",
         "of"
        ],
        [
         "20",
         "28.606",
         "0.0900000000000007",
         "the"
        ],
        [
         "21",
         "29.756",
         "0.1399999999999996",
         "The"
        ],
        [
         "22",
         "29.896",
         "0.3900000000000005",
         "gentle"
        ],
        [
         "23",
         "30.286",
         "0.4199999999999999",
         "constant"
        ],
        [
         "24",
         "30.706",
         "0.2099999999999999",
         "breeze"
        ],
        [
         "25",
         "30.916",
         "0.1200000000000001",
         "of"
        ],
        [
         "26",
         "31.036",
         "0.4100000000000001",
         "recycled"
        ],
        [
         "27",
         "31.476",
         "0.160000000000001",
         "air"
        ],
        [
         "28",
         "31.636",
         "0.1599999999999983",
         "from"
        ],
        [
         "29",
         "31.796",
         "0.0900000000000016",
         "the"
        ],
        [
         "30",
         "31.886",
         "0.2399999999999984",
         "vent"
        ],
        [
         "31",
         "32.126",
         "0.200000000000001",
         "above"
        ],
        [
         "32",
         "32.336",
         "0.1999999999999993",
         "blew"
        ],
        [
         "33",
         "32.536",
         "0.1100000000000012",
         "an"
        ],
        [
         "34",
         "32.646",
         "0.3900000000000005",
         "annoying"
        ],
        [
         "35",
         "33.036",
         "0.1699999999999999",
         "hair"
        ],
        [
         "36",
         "33.206",
         "0.3500000000000014",
         "against"
        ],
        [
         "37",
         "33.556",
         "0.1399999999999988",
         "her"
        ],
        [
         "38",
         "33.696",
         "0.3900000000000005",
         "nose"
        ],
        [
         "39",
         "34.246",
         "0.1999999999999993",
         "but"
        ],
        [
         "40",
         "34.446",
         "0.1699999999999999",
         "she"
        ],
        [
         "41",
         "34.616",
         "0.3000000000000007",
         "ignored"
        ],
        [
         "42",
         "34.916",
         "0.1600000000000001",
         "it"
        ],
        [
         "43",
         "35.396",
         "0.1100000000000012",
         "A"
        ],
        [
         "44",
         "35.506",
         "0.4700000000000006",
         "gasp"
        ],
        [
         "45",
         "35.986",
         "0.2699999999999996",
         "from"
        ],
        [
         "46",
         "36.256",
         "0.1300000000000007",
         "the"
        ],
        [
         "47",
         "36.386",
         "0.5399999999999991",
         "psychic"
        ],
        [
         "48",
         "36.926",
         "0.3200000000000003",
         "broke"
        ],
        [
         "49",
         "37.256",
         "0.1999999999999993",
         "her"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 668
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onset</th>\n",
       "      <th>duration</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.506</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Tara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.816</td>\n",
       "      <td>0.24</td>\n",
       "      <td>stood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.056</td>\n",
       "      <td>0.37</td>\n",
       "      <td>stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.586</td>\n",
       "      <td>0.40</td>\n",
       "      <td>still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.136</td>\n",
       "      <td>0.41</td>\n",
       "      <td>waiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>361.097</td>\n",
       "      <td>0.17</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>361.277</td>\n",
       "      <td>0.14</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>361.487</td>\n",
       "      <td>0.58</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>362.207</td>\n",
       "      <td>0.15</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>362.817</td>\n",
       "      <td>0.34</td>\n",
       "      <td>species</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>668 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       onset  duration     word\n",
       "0     23.506      0.30     Tara\n",
       "1     23.816      0.24    stood\n",
       "2     24.056      0.37    stock\n",
       "3     24.586      0.40    still\n",
       "4     25.136      0.41  waiting\n",
       "..       ...       ...      ...\n",
       "663  361.097      0.17      end\n",
       "664  361.277      0.14      for\n",
       "665  361.487      0.58  project\n",
       "666  362.207      0.15      and\n",
       "667  362.817      0.34  species\n",
       "\n",
       "[668 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "from attr import dataclass\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "import torch\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "import shutil\n",
    "from ray.exceptions import RayTaskError\n",
    "import traceback\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from studies import Study, Recording\n",
    "\n",
    "# Change back when in .py file\n",
    "from dataloader.batch import Batch, BatchFetcher\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AudioTextBatch(Batch):\n",
    "    brain_segments: dict[str, torch.Tensor]\n",
    "    audio_segments: torch.Tensor\n",
    "    transcript: list[str]\n",
    "    recording: Recording\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class AudioTextBatchFetcher(BatchFetcher):\n",
    "    \"\"\"\n",
    "    Fetches a single recording from an audio and brain pair dataset, with the\n",
    "    corresponsding text input ids. Tokenizing is done in the training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        notch_filter: bool,\n",
    "        frequency_bands: dict[str, tuple[float, float]],\n",
    "        scaling: str,\n",
    "        brain_clipping: int,\n",
    "        baseline_window: float,\n",
    "        new_freq: int,\n",
    "        delay: float,\n",
    "        seed: int,\n",
    "        # Specific to this batch type\n",
    "        max_random_shift: float,\n",
    "        window_size: int,\n",
    "        window_stride: int,\n",
    "        audio_sample_rate: int,\n",
    "        hop_length: int,\n",
    "        audio_processor: str,\n",
    "        n_jobs: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            max_random_shift -- maximum random shift to apply to the windows\n",
    "            window_size -- size of the window to extract\n",
    "            audio_sample_rate -- sample rate for the audio data\n",
    "            hop_length -- hop length for the audio data\n",
    "            audio_processor -- model to use for audio processing\n",
    "\n",
    "        Keyword Arguments:\n",
    "            notch_filter -- whether to apply notch filter to the raw data to remove powerline\n",
    "            frequency_bands -- dictionary of frequency bands tuple,\n",
    "                brain segements will be returned for each band in the dictionary\n",
    "            scaling -- scaling method to apply to the brain data\n",
    "            brain_clipping -- standard deviation to clip the brain data to\n",
    "            baseline_window -- window size to use for baseline normalization\n",
    "            new_freq -- new frequency to resample the brain data to\n",
    "            delay -- delay to apply to the brain data\n",
    "        \"\"\"\n",
    "        self.notch_filter = notch_filter\n",
    "        self.frequency_bands = frequency_bands\n",
    "        self.scaling = scaling\n",
    "        self.brain_clipping = brain_clipping\n",
    "        self.baseline_window = baseline_window\n",
    "        self.new_freq = new_freq\n",
    "        self.n_jobs = n_jobs\n",
    "        self.delay = delay\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # Specific to this batch type\n",
    "        self.max_random_shift = max_random_shift\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.audio_sample_rate = audio_sample_rate\n",
    "        self.hop_length = hop_length\n",
    "        self.audio_processor = WhisperFeatureExtractor.from_pretrained(audio_processor)\n",
    "\n",
    "    def fetch(self, recording: Recording, cache: bool) -> AudioTextBatch:\n",
    "        \"\"\"\n",
    "        Load, pre-process, and slice audio, brain, and text data into batch segments.\n",
    "        Loads from cache if available. Audio is returned as mel spectrogram features\n",
    "        of shape [B, mel_bins, T], brain data is returned as tensor of shape [B, C, T].\n",
    "        Transcript is a list of strings of length B, contasining words preceeded by their\n",
    "        timestamps. Not yet tokenized.\n",
    "\n",
    "        cache -- whether to save the batch to disk for future use\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Number of brain and audio windows do not match. Skip batch.\n",
    "        \"\"\"\n",
    "        # Initialize cache directory for the first time\n",
    "        if not os.path.exists(recording.cache_path):\n",
    "            os.makedirs(recording.cache_path)\n",
    "        try:\n",
    "            try:\n",
    "                # Try loading from cache first\n",
    "                (\n",
    "                    brain_segments,\n",
    "                    audio_window_timestamps,\n",
    "                    brain_window_timestamps,\n",
    "                    brain_start_time,\n",
    "                    info,\n",
    "                    word_events,\n",
    "                ) = self.fetch_cached_data(recording)\n",
    "\n",
    "                recording.start_time = brain_start_time\n",
    "                recording.info = info\n",
    "\n",
    "                # Segment brain tensors\n",
    "                for band in self.frequency_bands.keys():\n",
    "                    brain_segments[band] = self.segment_brain_tensor(\n",
    "                        brain_tensor=brain_segments[band],\n",
    "                        recording=recording,\n",
    "                        brain_window_timestamps=brain_window_timestamps,\n",
    "                        batch_size=256,\n",
    "                    )\n",
    "\n",
    "            # Alternatively, process raw data\n",
    "            except (ValueError, RayTaskError, FileNotFoundError) as e:\n",
    "                (\n",
    "                    brain_segments,\n",
    "                    audio_window_timestamps,\n",
    "                    brain_window_timestamps,\n",
    "                    word_events,\n",
    "                ) = self.fetch_raw_data(recording, cache=cache)\n",
    "\n",
    "            # AUDIO\n",
    "            audio_segments = self.segment_audio(\n",
    "                recording=recording,\n",
    "                audio_window_timestamps=audio_window_timestamps,\n",
    "            )\n",
    "            # WORD\n",
    "            transcript = self.segment_words_with_timestamps(\n",
    "                word_events=word_events,\n",
    "                brain_window_timestamps=brain_window_timestamps,\n",
    "                time_resolution=0.02,\n",
    "            )\n",
    "\n",
    "            # DIMENSION CHECKS\n",
    "            # if not all of the brain segments are the same length\n",
    "            if not all(\n",
    "                [\n",
    "                    brain_segments[list(self.frequency_bands.keys())[0]].shape[-1]\n",
    "                    == brain_segments[band].shape[-1]\n",
    "                    for band in self.frequency_bands.keys()\n",
    "                ]\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    f\"Brain segments are not the same length: {recording.cache_path}\"\n",
    "                    + f\" {brain_segments[list(self.frequency_bands.keys())[0]].shape[-1]}\"\n",
    "                )\n",
    "            # B mismatch between brain and audio\n",
    "            if (\n",
    "                brain_segments[list(self.frequency_bands.keys())[0]].shape[0]\n",
    "                != audio_segments.shape[0]\n",
    "            ):\n",
    "                raise ValueError(\"Number of brain and audio windows do not match\")\n",
    "            # B mismatch between brain and text\n",
    "            if brain_segments[list(self.frequency_bands.keys())[0]].shape[0] != len(\n",
    "                transcript\n",
    "            ):\n",
    "                raise ValueError(\"Number of brain and text windows do not match\")\n",
    "\n",
    "            return AudioTextBatch(\n",
    "                brain_segments=brain_segments,\n",
    "                audio_segments=audio_segments,\n",
    "                transcript=transcript,\n",
    "                recording=recording,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            # Clean up cache if anything fails\n",
    "            shutil.rmtree(recording.cache_path, ignore_errors=True)\n",
    "\n",
    "            # Capture full traceback for Ray errors\n",
    "            if isinstance(e, RayTaskError):\n",
    "                error_msg = f\"Ray task failed: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            else:\n",
    "                error_msg = str(e)\n",
    "\n",
    "            raise ValueError(\n",
    "                f\"Error fetching batch for {recording.cache_path}: {error_msg}\"\n",
    "            )\n",
    "\n",
    "    def fetch_cached_data(self, recording: Recording) -> Tuple[\n",
    "        Dict[str, torch.Tensor],\n",
    "        torch.Tensor,\n",
    "        torch.Tensor,\n",
    "        float,\n",
    "        mne.Info,\n",
    "        pd.DataFrame,\n",
    "    ]:\n",
    "        \"\"\"Loads unsliced brain tensor, timestamps, and word_events from cache.\n",
    "        Brain start time saved in case of recording does not start at 0,\n",
    "        causing indexing issues when slicing brain tensor.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # IF cached\n",
    "            brain_segments = {\n",
    "                band: torch.load(f\"{recording.cache_path}/{band}.pt\")\n",
    "                for band in self.frequency_bands.keys()\n",
    "            }\n",
    "            timestamps = torch.load(\n",
    "                recording.cache_path + \"/timestamps.pt\"\n",
    "            )  # Load timestamps\n",
    "\n",
    "            info = pickle.load(open(recording.cache_path + \"/info.pkl\", \"rb\"))\n",
    "            word_events = pickle.load(\n",
    "                open(recording.cache_path + \"/word_events.pkl\", \"rb\")\n",
    "            )\n",
    "\n",
    "            return (\n",
    "                brain_segments,\n",
    "                timestamps[\"audio_window_timestamps\"],\n",
    "                timestamps[\"brain_window_timestamps\"],\n",
    "                timestamps[\"brain_start_time\"],\n",
    "                info,\n",
    "                word_events,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Cache loading failed: {str(e)}\")\n",
    "\n",
    "    def fetch_raw_data(\n",
    "        self, recording: Recording, cache: bool\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], torch.Tensor, torch.Tensor, pd.DataFrame]:\n",
    "        \"\"\"Load raw, pre-process, and segment into tensors if not cached.\n",
    "        Saves brain tensor and timestamps to cache for future use, including\n",
    "        brain start time in case of recording does not start at 0. Word events\n",
    "        saved as pandas df with columns = [\"onset\", \"duration\", \"word\"].\n",
    "        \"\"\"\n",
    "        raw = None\n",
    "        try:\n",
    "            # Clear cache\n",
    "            shutil.rmtree(recording.cache_path, ignore_errors=True)\n",
    "            os.makedirs(recording.cache_path)\n",
    "\n",
    "            # IF not cached\n",
    "            # Load the raw data\n",
    "            raw = recording.load_raw(load_data=True)\n",
    "            events = recording.load_events(raw=raw, options=\"both\")\n",
    "            sound_events, word_events = events[\"sound\"], events[\"word\"]\n",
    "\n",
    "            # Generate time stamps for the windows\n",
    "            audio_window_timestamps, brain_window_timestamps = (\n",
    "                self.generate_time_stamps(sound_events)\n",
    "            )\n",
    "\n",
    "            # BRAIN\n",
    "            brain_segments = self.segment_brain_mne(\n",
    "                recording=recording,\n",
    "                raw=raw,\n",
    "                brain_window_timestamps=brain_window_timestamps,\n",
    "                batch_size=256,  # Memory efficient batch size\n",
    "                cache=cache,\n",
    "            )\n",
    "\n",
    "            if cache:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"audio_window_timestamps\": audio_window_timestamps,\n",
    "                        \"brain_window_timestamps\": brain_window_timestamps,\n",
    "                        \"brain_start_time\": recording.start_time,\n",
    "                    },\n",
    "                    recording.cache_path + \"/timestamps.pt\",\n",
    "                )\n",
    "                pickle.dump(\n",
    "                    recording.info, open(recording.cache_path + \"/info.pkl\", \"wb\")\n",
    "                )\n",
    "                pickle.dump(\n",
    "                    word_events, open(recording.cache_path + \"/word_events.pkl\", \"wb\")\n",
    "                )\n",
    "\n",
    "            return (\n",
    "                brain_segments,\n",
    "                audio_window_timestamps,\n",
    "                brain_window_timestamps,\n",
    "                word_events,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Raw data processing failed: {str(e)}\")\n",
    "        finally:\n",
    "            if raw is not None:\n",
    "                del raw\n",
    "\n",
    "    def segment_words_with_timestamps(\n",
    "        self,\n",
    "        word_events: pd.DataFrame,\n",
    "        brain_window_timestamps: torch.Tensor,\n",
    "        time_resolution: float = 0.02,\n",
    "    ) -> list[str]:\n",
    "        \"\"\"For each brain window, produce a transcript string that includes\n",
    "        timestamp tokens for alignment.\n",
    "\n",
    "        Each word is preceded by a token indicating its onset, converted to the\n",
    "        nearest multiple of `time_resolution`.\n",
    "        \"\"\"\n",
    "        transcript_list = []\n",
    "        word_events = word_events.copy()\n",
    "        word_events[\"end\"] = word_events[\"onset\"] + word_events[\"duration\"]\n",
    "\n",
    "        for start, end in brain_window_timestamps:\n",
    "            # Find word occurrences of each window, as df\n",
    "            window_words = word_events[\n",
    "                (word_events[\"onset\"] >= start) & (word_events[\"end\"] <= end)\n",
    "            ]\n",
    "            tokens = []\n",
    "            for _, row in window_words.iterrows():\n",
    "                # Convert onset (or relative onset) to a discrete timestamp\n",
    "                relative_onset = row[\"onset\"] - start\n",
    "\n",
    "                # Round to nearest multiple of time_resolution:\n",
    "                rounded_time = round(relative_onset / time_resolution) * time_resolution\n",
    "\n",
    "                # Create a timestamp token. Adjust the format to match your token vocabulary.\n",
    "                timestamp_token = f\"<|{rounded_time:.2f}|>\"\n",
    "                tokens.append(timestamp_token)\n",
    "                tokens.append(row[\"word\"])\n",
    "\n",
    "            transcript = \" \".join(tokens)\n",
    "            transcript_list.append(transcript)\n",
    "\n",
    "        return transcript_list\n",
    "\n",
    "    def generate_time_stamps(\n",
    "        self, sound_events: pd.DataFrame\n",
    "    ) -> tuple[dict[str, list[tuple[float, float]]], list[tuple[float, float]]]:\n",
    "        \"\"\"Obtain the list of start and end times for the windows. Making sure\n",
    "        windows don't span two different sound files.\n",
    "\n",
    "        Arguments:\n",
    "            sound_events -- DataFrame containing the sound events. Columns are\n",
    "                'onset', 'sound', 'end' onset is event marker in the brain data,\n",
    "                start is the onset in the audio file\n",
    "\n",
    "        Returns:\n",
    "            audio_window_timestamps -- dictionary of list of tuples, list of tuples\n",
    "            brain_window_timestamps -- list of tuples\n",
    "        \"\"\"\n",
    "        audio_window_timestamps, brain_window_timestamps = {}, []\n",
    "\n",
    "        for sound_file in sorted(sound_events[\"sound\"].unique()):\n",
    "\n",
    "            start_time, end_time = (\n",
    "                sound_events[sound_events[\"sound\"] == sound_file][\"onset\"].iloc[0],\n",
    "                sound_events[sound_events[\"sound\"] == sound_file][\"end\"].iloc[0],\n",
    "            )\n",
    "\n",
    "            audio_start_time = copy.deepcopy(start_time)\n",
    "            audio_window_timestamps[sound_file] = []\n",
    "\n",
    "            # This works on onset times (brain)\n",
    "            while start_time + self.window_size < end_time:\n",
    "\n",
    "                brain_window_timestamps.append(\n",
    "                    (start_time, start_time + self.window_size)\n",
    "                )\n",
    "\n",
    "                # Notes the corresponsing timestamps in the audio file\n",
    "                audio_window_timestamps[sound_file].append(\n",
    "                    (\n",
    "                        start_time - audio_start_time,\n",
    "                        start_time + self.window_size - audio_start_time,\n",
    "                    )\n",
    "                )\n",
    "                start_time += np.random.uniform(\n",
    "                    self.window_stride, self.window_stride + self.max_random_shift\n",
    "                )  # some randomness\n",
    "\n",
    "                self.seed += 1\n",
    "                np.random.seed(self.seed)\n",
    "\n",
    "        return audio_window_timestamps, brain_window_timestamps\n",
    "\n",
    "    def segment_audio(\n",
    "        self,\n",
    "        recording: Recording,\n",
    "        audio_window_timestamps: dict[str, list[tuple[float, float]]],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Slice the audio data into segments based on the window time stamps.\n",
    "        Pre-processes into Mel spectrogram features. Returns tensor of shape\n",
    "        [B, mel_bins, T] where B (windows), mel_bins, T (time steps).\n",
    "        \"\"\"\n",
    "        audio_segments = []\n",
    "\n",
    "        # dict of audio_name: audio_data (np.array)\n",
    "        audios = recording.load_stimuli(list(audio_window_timestamps.keys()))\n",
    "\n",
    "        for sound_file in sorted(list(audio_window_timestamps.keys())):\n",
    "\n",
    "            audio_segment = self.pre_process_audio(\n",
    "                audio=audios[sound_file],\n",
    "                time_stamps=audio_window_timestamps[sound_file],\n",
    "            )  # [B, mel_bins, T]\n",
    "            audio_segment = audio_segment[\n",
    "                :,\n",
    "                :,\n",
    "                : int(self.window_size * self.audio_sample_rate / self.hop_length),\n",
    "            ]  # Truncate temporal dim\n",
    "            audio_segments.append(audio_segment)\n",
    "\n",
    "        # Concat along batch dim\n",
    "        audio_segments = torch.cat(audio_segments, dim=0)\n",
    "        return audio_segments\n",
    "\n",
    "    def pre_process_audio(\n",
    "        self,\n",
    "        audio: np.ndarray,\n",
    "        time_stamps: list[tuple[float, float]],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Pre-processes the audio data by segmenting it based on time stamps,\n",
    "        returns Mel spectrogram features. Number of time steps will be\n",
    "        window_size * sample_rate / hop_length.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            audio -- audio data to be pre-processed: [T * sample_rate]\n",
    "            time_stamps -- list of tuples containing the start and end time of segments\n",
    "\n",
    "        Returns:\n",
    "            inputs -- pre-processed audio data, size [B, mel_bins, T]\n",
    "        \"\"\"\n",
    "        audio_segments = []\n",
    "\n",
    "        time_stamps = torch.tensor(time_stamps)  # Shape: [N, 2]\n",
    "        start_samples = (time_stamps[:, 0] * self.audio_sample_rate).to(torch.int64)\n",
    "        end_samples = (time_stamps[:, 1] * self.audio_sample_rate).to(torch.int64)\n",
    "        audio_segments = [\n",
    "            audio[start:end] for start, end in zip(start_samples, end_samples)\n",
    "        ]\n",
    "\n",
    "        # Batch process the audio segments\n",
    "        inputs = self.audio_processor(\n",
    "            audio_segments,\n",
    "            sampling_rate=self.audio_sample_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            do_normalize=True,\n",
    "            hop_length=self.hop_length,\n",
    "            max_length=self.audio_sample_rate * self.window_size,\n",
    "        )\n",
    "\n",
    "        return inputs[\"input_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train.training_session_v1 import load_training_session\n",
    "# import multiprocessing\n",
    "# import torch\n",
    "\n",
    "# device = \"cuda\"\n",
    "\n",
    "# session = load_training_session(\n",
    "#     save_path=\"saves/phase2/architecture/task/transformers/4C4Con_d256/epoch_39\",\n",
    "#     studies={\"gwilliams2023\": \"audio\"},\n",
    "#     data_path=\"/home/ubuntu/storage-texas/data\",\n",
    "#     cache_name=\"cache\",\n",
    "# )\n",
    "\n",
    "# dataloader = session.get_dataloader(buffer_size=1, num_workers=1, max_cache_size=100)\n",
    "\n",
    "# # Unseen both\n",
    "# # recording = session.studies[\"gwilliams2023\"].recordings[19][0][0]\n",
    "\n",
    "# # Seen\n",
    "# # recording = session.studies[\"gwilliams2023\"].recordings[15][0][1]\n",
    "\n",
    "# # Unseen task\n",
    "# # recording = session.studies[\"gwilliams2023\"].recordings[18][0][0]\n",
    "\n",
    "# # Unseen subject\n",
    "# recording = session.studies[\"gwilliams2023\"].recordings[19][0][1]\n",
    "\n",
    "# print(\n",
    "#     f\"Showing recording: {recording.study_name}_{recording.subject_id}_{recording.task_id}\"\n",
    "# )\n",
    "\n",
    "# dataloader.start_fetching(recordings=[recording])\n",
    "# batch = dataloader.get_recording()\n",
    "# brain, audio, recording = (\n",
    "#     batch.brain_segments[\"all\"].to(device),\n",
    "#     batch.audio_segments.to(device),\n",
    "#     batch.recording,\n",
    "# )\n",
    "\n",
    "# conditions = {\n",
    "#     \"study\": f\"{recording.study_name}\",\n",
    "#     \"subject\": f\"{recording.study_name}_{recording.subject_id}\",\n",
    "# }\n",
    "# session.model.to(device).eval()\n",
    "\n",
    "# # with torch.no_grad():\n",
    "# #     (\n",
    "# #         x,  # [B, C, T]\n",
    "# #         quantizer_metrics,\n",
    "# #         channel_weights,\n",
    "# #         hidden_outputs,\n",
    "# #         encoder_hidden_states,  # L * [B, T, D]\n",
    "# #     ) = session.model(\n",
    "# #         x=[brain],\n",
    "# #         recording=[recording],\n",
    "# #         conditions=[conditions],\n",
    "# #         mel=[audio],\n",
    "# #         train=False,\n",
    "# #         return_hidden_outputs=False,\n",
    "# #     )\n",
    "\n",
    "# dataloader.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
