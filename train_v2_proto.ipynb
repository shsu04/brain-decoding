{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gwilliams2023 with batch type audiotext\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "onset",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "duration",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "word",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e4af2ca5-4d5f-4b09-9cf8-9d3e42f864cc",
       "rows": [
        [
         "0",
         "23.506",
         "0.3",
         "Tara"
        ],
        [
         "1",
         "23.816",
         "0.24",
         "stood"
        ],
        [
         "2",
         "24.056",
         "0.37",
         "stock"
        ],
        [
         "3",
         "24.586",
         "0.3999999999999999",
         "still"
        ],
        [
         "4",
         "25.136",
         "0.4100000000000001",
         "waiting"
        ],
        [
         "5",
         "25.546",
         "0.1299999999999999",
         "for"
        ],
        [
         "6",
         "25.676",
         "0.0899999999999998",
         "the"
        ],
        [
         "7",
         "25.766",
         "0.27",
         "first"
        ],
        [
         "8",
         "26.046",
         "0.31",
         "tiny"
        ],
        [
         "9",
         "26.356",
         "0.2399999999999997",
         "gleam"
        ],
        [
         "10",
         "26.606",
         "0.1799999999999997",
         "from"
        ],
        [
         "11",
         "26.786",
         "0.0700000000000002",
         "the"
        ],
        [
         "12",
         "26.856",
         "0.29",
         "scout"
        ],
        [
         "13",
         "27.146",
         "0.29",
         "craft"
        ],
        [
         "14",
         "27.446",
         "0.1499999999999999",
         "to"
        ],
        [
         "15",
         "27.596",
         "0.2800000000000002",
         "appear"
        ],
        [
         "16",
         "27.876",
         "0.08",
         "in"
        ],
        [
         "17",
         "27.956",
         "0.0899999999999998",
         "the"
        ],
        [
         "18",
         "28.046",
         "0.4199999999999999",
         "darkness"
        ],
        [
         "19",
         "28.466",
         "0.1399999999999996",
         "of"
        ],
        [
         "20",
         "28.606",
         "0.0900000000000007",
         "the"
        ],
        [
         "21",
         "29.756",
         "0.1399999999999996",
         "The"
        ],
        [
         "22",
         "29.896",
         "0.3900000000000005",
         "gentle"
        ],
        [
         "23",
         "30.286",
         "0.4199999999999999",
         "constant"
        ],
        [
         "24",
         "30.706",
         "0.2099999999999999",
         "breeze"
        ],
        [
         "25",
         "30.916",
         "0.1200000000000001",
         "of"
        ],
        [
         "26",
         "31.036",
         "0.4100000000000001",
         "recycled"
        ],
        [
         "27",
         "31.476",
         "0.160000000000001",
         "air"
        ],
        [
         "28",
         "31.636",
         "0.1599999999999983",
         "from"
        ],
        [
         "29",
         "31.796",
         "0.0900000000000016",
         "the"
        ],
        [
         "30",
         "31.886",
         "0.2399999999999984",
         "vent"
        ],
        [
         "31",
         "32.126",
         "0.200000000000001",
         "above"
        ],
        [
         "32",
         "32.336",
         "0.1999999999999993",
         "blew"
        ],
        [
         "33",
         "32.536",
         "0.1100000000000012",
         "an"
        ],
        [
         "34",
         "32.646",
         "0.3900000000000005",
         "annoying"
        ],
        [
         "35",
         "33.036",
         "0.1699999999999999",
         "hair"
        ],
        [
         "36",
         "33.206",
         "0.3500000000000014",
         "against"
        ],
        [
         "37",
         "33.556",
         "0.1399999999999988",
         "her"
        ],
        [
         "38",
         "33.696",
         "0.3900000000000005",
         "nose"
        ],
        [
         "39",
         "34.246",
         "0.1999999999999993",
         "but"
        ],
        [
         "40",
         "34.446",
         "0.1699999999999999",
         "she"
        ],
        [
         "41",
         "34.616",
         "0.3000000000000007",
         "ignored"
        ],
        [
         "42",
         "34.916",
         "0.1600000000000001",
         "it"
        ],
        [
         "43",
         "35.396",
         "0.1100000000000012",
         "A"
        ],
        [
         "44",
         "35.506",
         "0.4700000000000006",
         "gasp"
        ],
        [
         "45",
         "35.986",
         "0.2699999999999996",
         "from"
        ],
        [
         "46",
         "36.256",
         "0.1300000000000007",
         "the"
        ],
        [
         "47",
         "36.386",
         "0.5399999999999991",
         "psychic"
        ],
        [
         "48",
         "36.926",
         "0.3200000000000003",
         "broke"
        ],
        [
         "49",
         "37.256",
         "0.1999999999999993",
         "her"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 668
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onset</th>\n",
       "      <th>duration</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.506</td>\n",
       "      <td>0.30</td>\n",
       "      <td>Tara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.816</td>\n",
       "      <td>0.24</td>\n",
       "      <td>stood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.056</td>\n",
       "      <td>0.37</td>\n",
       "      <td>stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.586</td>\n",
       "      <td>0.40</td>\n",
       "      <td>still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.136</td>\n",
       "      <td>0.41</td>\n",
       "      <td>waiting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>361.097</td>\n",
       "      <td>0.17</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>361.277</td>\n",
       "      <td>0.14</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>361.487</td>\n",
       "      <td>0.58</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>362.207</td>\n",
       "      <td>0.15</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>362.817</td>\n",
       "      <td>0.34</td>\n",
       "      <td>species</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>668 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       onset  duration     word\n",
       "0     23.506      0.30     Tara\n",
       "1     23.816      0.24    stood\n",
       "2     24.056      0.37    stock\n",
       "3     24.586      0.40    still\n",
       "4     25.136      0.41  waiting\n",
       "..       ...       ...      ...\n",
       "663  361.097      0.17      end\n",
       "664  361.277      0.14      for\n",
       "665  361.487      0.58  project\n",
       "666  362.207      0.15      and\n",
       "667  362.817      0.34  species\n",
       "\n",
       "[668 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from studies.gwilliams2023 import Gwilliams2023\n",
    "from studies.armeini2022 import Armeini2022\n",
    "\n",
    "\n",
    "study = Gwilliams2023(\n",
    "    batch_type=\"audiotext\",\n",
    "    download=False,\n",
    ")\n",
    "\n",
    "rec = study.recordings[0][0][0]\n",
    "raw = rec.load_raw(load_data=True)\n",
    "events = rec.load_events(raw, options=\"both\")\n",
    "word_events = events[\"word\"]\n",
    "word_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import TRUE\n",
    "from dataloader import DataLoader\n",
    "\n",
    "add_timestamps = True\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    buffer_size=30,\n",
    "    max_cache_size_gb=400,\n",
    "    cache_dir=\"cache\",\n",
    "    notch_filter=True,\n",
    "    frequency_bands={\"all\": (0.5, 80)},\n",
    "    scaling=\"both\",\n",
    "    brain_clipping=None,\n",
    "    baseline_window=0.5,\n",
    "    new_freq=200,\n",
    "    delay=0.15,\n",
    "    batch_types={\"audiotext\": 1},\n",
    "    batch_kwargs={\n",
    "        \"audiotext\": {\n",
    "            \"max_random_shift\": 1.0,\n",
    "            \"window_size\": 4,\n",
    "            \"window_stride\": 1,\n",
    "            \"audio_sample_rate\": 16000,\n",
    "            \"hop_length\": 160,\n",
    "            \"audio_processor\": \"openai/whisper-tiny.en\",\n",
    "            \"add_timestamps\": add_timestamps,\n",
    "            \"tokenize\": TRUE,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "dataloader.start_fetching(recordings=[rec])\n",
    "batch = dataloader.get_recording()\n",
    "\n",
    "brain, audio, transcript, transcript_attention_masks, recording = (\n",
    "    batch.brain_segments[\"all\"],  # .to(device)\n",
    "    batch.audio_segments,  # .to(device)\n",
    "    batch.transcript,\n",
    "    batch.transcript_attention_masks,\n",
    "    batch.recording,\n",
    ")\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "\n",
    "# def remove_timestamps(transcript_line: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Removes <|x.xx|> and with space patterns from a single transcript line.\n",
    "#     \"\"\"\n",
    "#     pattern = r\"<\\|\\d+(\\.\\d+)?\\|>\\s?\"\n",
    "#     return re.sub(pattern, \"\", transcript_line).strip()\n",
    "\n",
    "\n",
    "# def clean_timestamped_transcript(transcript_lines):\n",
    "#     \"\"\"\n",
    "#     Removes timestamp tokens from a list of transcript lines.\n",
    "#     Returns a list of cleaned lines.\n",
    "#     \"\"\"\n",
    "#     return [remove_timestamps(line) for line in transcript_lines]\n",
    "\n",
    "\n",
    "# transcript_no_timestamps = clean_timestamped_transcript(transcript)\n",
    "# transcript_no_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': Can't extract `str` to `Vec`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m skip_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m decode_with_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranscript\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_with_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_with_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m decoded \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(word\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m decoded]\n\u001b[1;32m     31\u001b[0m decoded\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3716\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3694\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3697\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3698\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3699\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3701\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3714\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3715\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m   3717\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3718\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3719\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3721\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3723\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\n\u001b[1;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3717\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3694\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3697\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3698\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3699\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3701\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3714\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3715\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3717\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3718\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3719\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3721\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3723\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3724\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/transformers/models/whisper/tokenization_whisper_fast.py:391\u001b[0m, in \u001b[0;36mWhisperTokenizerFast.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, output_offsets, time_precision, decode_with_timestamps, normalize, basic_normalize, remove_diacritics, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m filtered_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_token_ids(\n\u001b[1;32m    387\u001b[0m     token_ids,\n\u001b[1;32m    388\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m    389\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbasic_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasic_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_diacritics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_diacritics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decode_with_timestamps:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# legacy method to decode timestamps when not included in the tokenizer vocabulary\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_with_timestamps(\n\u001b[1;32m    403\u001b[0m         filtered_ids, time_precision\u001b[38;5;241m=\u001b[39mtime_precision, skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens\n\u001b[1;32m    404\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3756\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3753\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3754\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3761\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/transformers/models/whisper/tokenization_whisper_fast.py:417\u001b[0m, in \u001b[0;36mWhisperTokenizerFast._decode\u001b[0;34m(self, normalize, basic_normalize, remove_diacritics, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, basic_normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, remove_diacritics: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 417\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[1;32m    420\u001b[0m         clean_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:625\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    624\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 625\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    628\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    631\u001b[0m )\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': Can't extract `str` to `Vec`"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperTokenizerFast\n",
    "\n",
    "predict_timestamps = add_timestamps\n",
    "add_prefix_space = True\n",
    "\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(\n",
    "    \"openai/whisper-tiny.en\",\n",
    "    predict_timestamps=predict_timestamps,\n",
    "    add_prefix_space=add_prefix_space,\n",
    ")\n",
    "\n",
    "# encoded = tokenizer(\n",
    "#     transcript,\n",
    "#     return_tensors=\"pt\",\n",
    "#     padding=\"max_length\",\n",
    "#     truncation=True,\n",
    "#     max_length=64,  # 16 * int(windows)\n",
    "# )\n",
    "# input_ids, attention_mask = encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "skip_special_tokens = True\n",
    "decode_with_timestamps = False\n",
    "\n",
    "decoded = tokenizer.batch_decode(\n",
    "    sequences=transcript,\n",
    "    skip_special_tokens=skip_special_tokens,\n",
    "    decode_with_timestamps=decode_with_timestamps,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "decoded = [\" \".join(word.split()) for word in decoded]\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNEncoder initialized as conformer with 4 layers, 256 d_model, 4 nhead\n",
      "\tEmbedding: sinusoidal, params: 6075392\n",
      "SimpleConv initialized with 8927984 parameters, cond: ['study', 'subject']\n",
      "Merger False, merger channels 0\n",
      "ConvBlocks: 4, hidden_dim: 256, params 2626048\n",
      "Found 40 target modules for AdaLora: ['model.decoder.layers.0.self_attn.k_proj', 'model.decoder.layers.0.self_attn.v_proj', 'model.decoder.layers.0.self_attn.q_proj', 'model.decoder.layers.0.self_attn.out_proj', 'model.decoder.layers.0.encoder_attn.k_proj', 'model.decoder.layers.0.encoder_attn.v_proj', 'model.decoder.layers.0.encoder_attn.q_proj', 'model.decoder.layers.0.encoder_attn.out_proj', 'model.decoder.layers.0.fc1', 'model.decoder.layers.0.fc2', 'model.decoder.layers.1.self_attn.k_proj', 'model.decoder.layers.1.self_attn.v_proj', 'model.decoder.layers.1.self_attn.q_proj', 'model.decoder.layers.1.self_attn.out_proj', 'model.decoder.layers.1.encoder_attn.k_proj', 'model.decoder.layers.1.encoder_attn.v_proj', 'model.decoder.layers.1.encoder_attn.q_proj', 'model.decoder.layers.1.encoder_attn.out_proj', 'model.decoder.layers.1.fc1', 'model.decoder.layers.1.fc2', 'model.decoder.layers.2.self_attn.k_proj', 'model.decoder.layers.2.self_attn.v_proj', 'model.decoder.layers.2.self_attn.q_proj', 'model.decoder.layers.2.self_attn.out_proj', 'model.decoder.layers.2.encoder_attn.k_proj', 'model.decoder.layers.2.encoder_attn.v_proj', 'model.decoder.layers.2.encoder_attn.q_proj', 'model.decoder.layers.2.encoder_attn.out_proj', 'model.decoder.layers.2.fc1', 'model.decoder.layers.2.fc2', 'model.decoder.layers.3.self_attn.k_proj', 'model.decoder.layers.3.self_attn.v_proj', 'model.decoder.layers.3.self_attn.q_proj', 'model.decoder.layers.3.self_attn.out_proj', 'model.decoder.layers.3.encoder_attn.k_proj', 'model.decoder.layers.3.encoder_attn.v_proj', 'model.decoder.layers.3.encoder_attn.q_proj', 'model.decoder.layers.3.encoder_attn.out_proj', 'model.decoder.layers.3.fc1', 'model.decoder.layers.3.fc2']\n",
      "openai/whisper-tiny.en loaded with total params = 38240008.\n",
      "AdaLora has 479712 trainable params after target module setup.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from config import SimpleConvConfig\n",
    "from peft import AdaLoraConfig\n",
    "from models.whisper_decoder import WhisperDecoder\n",
    "\n",
    "\n",
    "brain_module_config = SimpleConvConfig(\n",
    "    # Str to list of possible conditions\n",
    "    mel_normalization=False,\n",
    "    conditions={\n",
    "        \"study\": [],\n",
    "        \"subject\": [],\n",
    "    },\n",
    "    # Channels\n",
    "    in_channels=208,\n",
    "    out_channels=80,\n",
    "    hidden_dim=256,\n",
    "    dropout=0.2,\n",
    "    initial_batch_norm=True,\n",
    "    # Sensor layout settings\n",
    "    layout_dim=2,\n",
    "    layout_proj=True,\n",
    "    layout_scaling=\"minmax\",\n",
    "    # Merger with spatial attn\n",
    "    merger=False,\n",
    "    merger_emb_type=None,\n",
    "    merger_emb_dim=0,\n",
    "    merger_channels=0,\n",
    "    merger_dropout=0.0,  # Float\n",
    "    merger_conditional=None,\n",
    "    # Inital\n",
    "    initial_linear=256,\n",
    "    initial_depth=1,\n",
    "    # Conditional layers\n",
    "    conditional_layers=False,\n",
    "    conditional_layers_dim=None,  # input or hidden_dim\n",
    "    # Conv layer overall structure\n",
    "    depth=4,\n",
    "    kernel_size=3,\n",
    "    growth=1.0,\n",
    "    dilation_growth=2,\n",
    "    dilation_period=5,\n",
    "    glu=1,\n",
    "    conv_dropout=0.2,\n",
    "    dropout_input=0.1,\n",
    "    batch_norm=True,\n",
    "    half=True,\n",
    "    cnn_pos_encoding=False,\n",
    "    # Quantizer\n",
    "    quantizer=False,\n",
    "    num_codebooks=0,\n",
    "    codebook_size=0,\n",
    "    quantizer_commitment=0,\n",
    "    quantizer_temp_init=0,\n",
    "    quantizer_temp_min=0,\n",
    "    quantizer_temp_decay=0,\n",
    "    # Transformers Encoders\n",
    "    transformer_input=\"continuous\",\n",
    "    transformer_encoder_emb=\"sinusoidal\",\n",
    "    transformer_encoder_layers=4,\n",
    "    transformer_encoder_heads=4,\n",
    "    # Conformer encoder variant\n",
    "    rnn_type=\"conformer\",\n",
    "    depthwise_conv_kernel_size=15,\n",
    "    use_group_norm=False,\n",
    "    convolution_first=False,\n",
    "    # Transformer Decoders\n",
    "    transformer_decoder_emb=None,\n",
    "    transformer_decoder_layers=0,\n",
    "    transformer_decoder_heads=0,\n",
    "    transformer_decoder_dim=0,\n",
    ")\n",
    "\n",
    "adalora_init_r = 12\n",
    "adalora_target_r = 4\n",
    "adalora_tinit = 450 * 3  # 5% total steps\n",
    "adalora_tfinal = 450 * 8  # 50-80% total steps\n",
    "adalora_deltaT = 450 * 1  # 1-5% total steps\n",
    "adalora_lora_alpha = 32\n",
    "adalora_lora_dropout = 0.1\n",
    "adalora_total_step = 450 * 50\n",
    "\n",
    "adalora_config = AdaLoraConfig(\n",
    "    peft_type=\"ADALORA\",\n",
    "    task_type=\"SPEECH_RECOGNITION\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"],\n",
    "    init_r=adalora_init_r,\n",
    "    target_r=adalora_target_r,\n",
    "    tinit=adalora_tinit,\n",
    "    tfinal=adalora_tfinal,\n",
    "    deltaT=adalora_deltaT,\n",
    "    lora_alpha=adalora_lora_alpha,\n",
    "    lora_dropout=adalora_lora_dropout,\n",
    "    total_step=adalora_total_step,\n",
    ")\n",
    "\n",
    "model = WhisperDecoder(\n",
    "    brain_module_config=brain_module_config,\n",
    "    adalora_config=adalora_config,\n",
    "    device=\"mps\",\n",
    "    audio_model_id=\"openai/whisper-tiny.en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Terror stood stock, still, waiting for the first tiny gleam from the scout craft',\n",
       " ' Waiting for the first tiny gleam from the scout craft to appear in the darkness of the wormhole',\n",
       " ' The scene from the scout craft to appear in the darkness of the wormhole. The gentle con',\n",
       " ' In the darkness of the wormhole, the gentle constant rays of recycled air from the',\n",
       " ' The gentle constant rays of recycled air from the vent above blue and annoying hair again.',\n",
       " ' air from the vent above Lewin annoying hair against her nose, but she ignored it.',\n",
       " ' Annoying her against her nose, but she ignored it. A gasp from the side',\n",
       " ' She ignored it. A gasp from the psychic broke her silent vigil.',\n",
       " ' She asked from the psychic broke her silent vigil, and she turned',\n",
       " ' silent vigil, and she turned, results, harm',\n",
       " ' And she turned. Results, harm and rah she suppressed.',\n",
       " ' As a result, harm in Rashi suppressed the surge of annoyance that Rashi',\n",
       " ' In Rauh she suppressed the surge of annoyance that ran through her as she called her.',\n",
       " ' The search of annoyance that Ryan threw her as she contemplated the sound of the man.',\n",
       " ' And through her, she contemplated the size gift of getting all the heart',\n",
       " ' plated the size gift of getting all the hot news first.',\n",
       " ' all the hot news first. Harments face slowly.',\n",
       " ' first. Harments face slowly animated.',\n",
       " \" Harmon's face slowly animated, joy sweeping and swa-\",\n",
       " ' The animated, joy-swoeping and swarded to replace stern cons']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 20\n",
    "\n",
    "brain_input = brain[:idx].to(\"mps\")\n",
    "attention_mask_input = transcript_attention_masks[:idx].to(\"mps\")\n",
    "input_ids_input = transcript[:idx].to(\"mps\")\n",
    "audio_input = audio[:idx].to(\"mps\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     output = model(\n",
    "#         x=[brain_input],\n",
    "#         recording=[recording],\n",
    "#         conditions=[{}],\n",
    "#         mel=None,\n",
    "#         train=False,\n",
    "#         return_hidden_outputs=False,\n",
    "#         attention_mask=None,\n",
    "#         labels=input_ids_input,\n",
    "#         decoder_attention_mask=attention_mask_input,\n",
    "#     )\n",
    "\n",
    "# (\n",
    "#     x,  # predicted mel\n",
    "#     quantizer_metrics,\n",
    "#     channel_weights,\n",
    "#     hidden_outputs,\n",
    "#     encoder_last_hidden_state,\n",
    "#     ce_loss,\n",
    "# ) = output\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    (\n",
    "        output_token_ids,  # [B, T]\n",
    "        x,  # [B, 80, T']\n",
    "        quantizer_metrics,\n",
    "        channel_weights,\n",
    "        hidden_outputs,\n",
    "    ) = model.generate(\n",
    "        x=None,  # [brain_input],\n",
    "        recording=[recording],\n",
    "        conditions=[{}],\n",
    "        mel=audio_input,\n",
    "        max_new_tokens=128,\n",
    "        attention_mask=None,\n",
    "        return_hidden_outputs=False,\n",
    "    )\n",
    "output_token_ids_decoded = tokenizer.batch_decode(\n",
    "    sequences=output_token_ids,\n",
    "    skip_special_tokens=True,\n",
    "    decode_with_timestamps=False,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "output_token_ids_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257, 50362, 50363,  ..., 50256, 50256, 50256],\n",
       "        [50257, 50362, 50367,  ..., 50256, 50256, 50256],\n",
       "        [50257, 50362, 50370,  ..., 50256, 50256, 50256],\n",
       "        ...,\n",
       "        [50257, 50362, 50412,  ..., 50256, 50256, 50256],\n",
       "        [50257, 50362, 50370,  ..., 50256, 50256, 50256],\n",
       "        [50257, 50362, 50376,  ..., 50256, 50256, 50256]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tara stood stock still waiting for the first tiny gleam from the scout craft',\n",
       " 'waiting for the first tiny gleam from the scout craft to appear in the darkness of the',\n",
       " 'from the scout craft to appear in the darkness of the The gentle',\n",
       " 'the darkness of the The gentle constant breeze of recycled air from the',\n",
       " 'The gentle constant breeze of recycled air from the vent above blew an annoying hair',\n",
       " 'air from the vent above blew an annoying hair against her nose but she ignored it',\n",
       " 'hair against her nose but she ignored it A gasp from the',\n",
       " 'ignored it A gasp from the psychic broke her silent',\n",
       " 'from the psychic broke her silent vigil and she',\n",
       " 'vigil and she turned Results',\n",
       " 'and she turned Results Harmon she',\n",
       " 'Harmon she suppressed the surge of annoyance that',\n",
       " 'she suppressed the surge of annoyance that ran through her as she',\n",
       " 'the surge of annoyance that ran through her as she contemplated the',\n",
       " 'through her as she contemplated the gift of getting all the',\n",
       " 'the gift of getting all the hot news first',\n",
       " 'all the hot news first face',\n",
       " 'face slowly animated',\n",
       " 'face slowly animated joy sweeping in',\n",
       " 'animated joy sweeping in to replace stern']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_input_decoded = tokenizer.batch_decode(\n",
    "    sequences=input_ids_input,\n",
    "    skip_special_tokens=True,\n",
    "    decode_with_timestamps=False,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "input_ids_input_decoded = [\" \".join(word.split()) for word in input_ids_input_decoded]\n",
    "input_ids_input_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.5525711421877697,\n",
       " 'rouge_f': 0.801990348361316,\n",
       " 'bert_score': 0.7350820302963257,\n",
       " 'cer': 0.3835767566453689,\n",
       " 'self_bleu': 0.034513324789303225}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.nlp_metrics import nlp_metrics\n",
    "\n",
    "metrics = nlp_metrics(output_token_ids_decoded, input_ids_input_decoded)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
