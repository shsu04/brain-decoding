{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.compression import compress_directories, decompress_directories\n",
    "\n",
    "# base_path = \"downloaded_data/gwilliams\"\n",
    "# destination_path = \"data/gwilliams\"\n",
    "\n",
    "# decompress_directories(\n",
    "#     base_path,\n",
    "#     destination_path,\n",
    "#     checksum_file_name=\"checksums.txt\",\n",
    "#     delete_compressed_files=True,\n",
    "#     num_workers=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data partitioned on studies ['gwilliams']. Recordings:\n",
      "Train: 135, Unseen Task: 12, Unseen Subject: 45, Unseen Both: 4.\n",
      "\n",
      "\n",
      "SimpleConv: \n",
      "\tParams: 14432128\n",
      "\tConv blocks: 5\n",
      "\tTrans layers: 0\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import random\n",
    "import time\n",
    "from tracemalloc import start\n",
    "from tqdm import tqdm\n",
    "from config.simpleconv_config import SimpleConvConfig\n",
    "from models.simpleconv import SimpleConv\n",
    "from studies.study_factory import StudyFactory\n",
    "from utils.dataloader import ParallelDataLoader\n",
    "from utils.pre_processor import PreProcessor\n",
    "import typing as tp\n",
    "import json\n",
    "from itertools import product\n",
    "from torch.optim import AdamW, Adam\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "from utils.dataloader import ParallelDataLoader\n",
    "from utils.clip import CLIPLoss\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "from config import SimpleConvConfig, Config\n",
    "\n",
    "\n",
    "class TrainingConfigV0(Config):\n",
    "    def __init__(\n",
    "        self,\n",
    "        brain_encoder_config: SimpleConvConfig,\n",
    "        data_partition: tp.Dict[str, tp.Dict[str, tp.List[str]]],\n",
    "        # Pre-processing parameters\n",
    "        # Brain\n",
    "        new_freq: int = 100,\n",
    "        frequency_bands: tp.Tuple[str, tp.Tuple[int, int]] = {\"all\": (0.5, 100)},\n",
    "        max_random_shift: float = 2.0,\n",
    "        window_size: int = 4,\n",
    "        window_stride: int = 1,\n",
    "        brain_clipping: float = 20,\n",
    "        baseline_window: int = 0.5,\n",
    "        notch_filter: bool = True,\n",
    "        # Audio\n",
    "        audio_model: str = \"openai/whisper-large-v3\",\n",
    "        audio_sample_rate: int = 16000,\n",
    "        hop_length: int = 160,\n",
    "        # Hyperparameters\n",
    "        learning_rate: float = 3e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "        epochs: int = 50,\n",
    "        batch_size: int = 128,\n",
    "        use_clip_loss: bool = True,\n",
    "        use_mse_loss: bool = True,\n",
    "        alpha: float = 0.5,\n",
    "        random_test_size: int = 3,\n",
    "    ):\n",
    "        self.brain_encoder_config = brain_encoder_config\n",
    "        # key: study_name, value: dict with keys: \"testing_subjects\", \"testing_tasks\",\n",
    "        # where each value is a list of int. Ones not specified in either lists are\n",
    "        # used for training.\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "        # Pre-processing parameters\n",
    "        # Brain\n",
    "        self.new_freq = new_freq\n",
    "        self.frequency_bands = frequency_bands\n",
    "        self.max_random_shift = max_random_shift\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.baseline_window = baseline_window\n",
    "        self.notch_filter = notch_filter\n",
    "        self.brain_clipping = brain_clipping\n",
    "\n",
    "        # Audio\n",
    "        self.audio_model = audio_model\n",
    "        self.audio_sample_rate = audio_sample_rate\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.use_clip_loss = use_clip_loss\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        self.alpha = alpha\n",
    "        self.random_test_size = random_test_size\n",
    "\n",
    "        assert 0 <= self.alpha <= 1, \"Alpha must be between 0 and 1\"\n",
    "        assert use_clip_loss or use_mse_loss, \"At least one loss function must be used\"\n",
    "\n",
    "    # does not overide parent method\n",
    "    def to_dict_(self):\n",
    "        brain_encoder_config = self.brain_encoder_config.to_dict()\n",
    "        config = self.to_dict()\n",
    "        config[\"brain_encoder_config\"] = brain_encoder_config\n",
    "        return config\n",
    "\n",
    "\n",
    "class TrainingSessionV0:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TrainingConfigV0,\n",
    "        studies: tp.List[str],\n",
    "        data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "        save_path: str = \"/home/ubuntu/brain-decoding/saves\",\n",
    "        clear_cache: bool = False,\n",
    "    ):\n",
    "        \"\"\"Initializes a training session with the provided configuration and data.\n",
    "\n",
    "        Arguments:\n",
    "            config -- The configuration for the training session.\n",
    "            studies -- list of studies to train on. Partition policy determined in TrainingConfig\n",
    "            data_path -- The path to the data directory.\n",
    "            save_path -- The path to the directory where the model and logs will be saved.\n",
    "        \"\"\"\n",
    "        assert len(studies) > 0, \"At least one study root path must be provided\"\n",
    "        assert all(\n",
    "            os.path.exists(data_path + \"/\" + study) for study in studies\n",
    "        ), \"All study root paths must exist\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(save_path, \"training_log.log\"),\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s %(message)s\",\n",
    "            filemode=\"w\",\n",
    "        )\n",
    "        self.logger = logging.getLogger()\n",
    "\n",
    "        self.config = config\n",
    "        self.data_path = data_path\n",
    "        self.save_path = save_path\n",
    "\n",
    "        # Create studies accessor\n",
    "        self.studies = {}\n",
    "        for study in studies:\n",
    "            path = os.path.join(data_path, study)\n",
    "            try:\n",
    "                self.studies[study] = StudyFactory.create_study(study, path)\n",
    "                if clear_cache:\n",
    "                    shutil.rmtree(study.cache_dir)\n",
    "                    os.makedirs(study.cache_dir)\n",
    "                    self.log_print(f\"Cleared cache for study {study}\")\n",
    "            except ValueError as e:\n",
    "                self.log_print(f\"Error loading study {study}: {e}\")\n",
    "\n",
    "        # Create preprocessor\n",
    "        self.pre_processor = PreProcessor()\n",
    "\n",
    "        self.dataset = {\n",
    "            \"train\": [],\n",
    "            \"test\": {\n",
    "                \"unseen_subject\": [],\n",
    "                \"unseen_task\": [],\n",
    "                \"unseen_both\": [],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        self.partition_data()\n",
    "\n",
    "        self.metrics = {\n",
    "            \"train\": [],\n",
    "            \"test\": {\n",
    "                \"unseen_subject\": [],\n",
    "                \"unseen_task\": [],\n",
    "                \"unseen_both\": [],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # If subject layer, assign a unique int to each subject in the dataset\n",
    "        self.subject_mapping, n = {}, 0\n",
    "        for study_name, study in self.studies.items():\n",
    "            for subject in study.subjects_list:\n",
    "                self.subject_mapping[f\"{study_name}_{subject}\"] = n\n",
    "                n += 1\n",
    "        self.config.brain_encoder_config.n_subjects = n\n",
    "\n",
    "        self.model = SimpleConv(self.config.brain_encoder_config)\n",
    "        self.error = None\n",
    "        self.optimizer = Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "        )\n",
    "\n",
    "    def partition_data(self):\n",
    "        \"\"\"\n",
    "        Partitions the data into training and various testing sets, based on\n",
    "        the named holdout sessions and tasks specified in TrainingConfig\n",
    "        \"\"\"\n",
    "        for study_name, study in self.studies.items():\n",
    "\n",
    "            if study_name not in self.config.data_partition:\n",
    "                raise ValueError(f\"Study {study_name} not found in data partition\")\n",
    "\n",
    "            data_partition = self.config.data_partition[study_name]\n",
    "\n",
    "            for subject, task, session in product(\n",
    "                [i for i in range(len(study.subjects_list))],\n",
    "                [i for i in range(len(study.tasks))],\n",
    "                [i for i in range(len(study.sessions))],\n",
    "            ):\n",
    "                # If recording exists\n",
    "                try:\n",
    "                    recording = study.recordings[subject][task][session]\n",
    "                except IndexError:\n",
    "                    # self.logger.error(f\"Recording not found for {study_name} {subject} {task} {session}\")\n",
    "                    continue\n",
    "\n",
    "                # Unseen both and task\n",
    "                if subject in data_partition[\"testing_subjects\"]:\n",
    "                    if task in data_partition[\"testing_tasks\"]:\n",
    "                        self.dataset[\"test\"][\"unseen_both\"].append(\n",
    "                            (study_name, subject, task, session)\n",
    "                        )\n",
    "                    else:\n",
    "                        self.dataset[\"test\"][\"unseen_task\"].append(\n",
    "                            (study_name, subject, task, session)\n",
    "                        )\n",
    "                # Unseen subject and train\n",
    "                else:\n",
    "                    if task in data_partition[\"testing_tasks\"]:\n",
    "                        self.dataset[\"test\"][\"unseen_subject\"].append(\n",
    "                            (study_name, subject, task, session)\n",
    "                        )\n",
    "                    else:\n",
    "                        self.dataset[\"train\"].append(\n",
    "                            (study_name, subject, task, session)\n",
    "                        )\n",
    "\n",
    "        self.log_print(\n",
    "            f\"Data partitioned on studies {list(self.studies.keys())}. Recordings:\"\n",
    "        )\n",
    "        self.log_print(\n",
    "            f\"Train: {len(self.dataset['train'])}, Unseen Task: {len(self.dataset['test']['unseen_task'])}, Unseen Subject: {len(self.dataset['test']['unseen_subject'])}, Unseen Both: {len(self.dataset['test']['unseen_both'])}.\\n\"\n",
    "        )\n",
    "\n",
    "    def train(self, device: str, buffer_size: int, num_workers: int):\n",
    "\n",
    "        # Set all training parameters\n",
    "        self.device = device\n",
    "        gpu_ok = False\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "        training_size = len(self.dataset[\"train\"])\n",
    "        self.scaler = GradScaler()\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Check if GPU is NVIDIA V100, A100, or H100\n",
    "        if torch.cuda.is_available():\n",
    "            device_cap = torch.cuda.get_device_capability()\n",
    "            if device_cap in ((7, 0), (8, 0), (9, 0)):\n",
    "                gpu_ok = True\n",
    "\n",
    "        if not gpu_ok:\n",
    "            self.log_print(\n",
    "                \"GPU is not NVIDIA V100, A100, or H100. Speedup numbers may be lower than expected.\"\n",
    "            )\n",
    "\n",
    "        for epoch in range(1, self.config.epochs + 1):\n",
    "            try:\n",
    "                self.model.to(device).train()\n",
    "                \n",
    "                epoch_start_time = time.time()\n",
    "\n",
    "                # Shuffle for each epoch, and start fetching\n",
    "                epoch_training_dataset, remaining = (\n",
    "                    self.dataset[\"train\"].copy(),\n",
    "                    training_size,\n",
    "                )\n",
    "                random.shuffle(epoch_training_dataset)\n",
    "                loader = self.get_data_loader(\n",
    "                    buffer_size=buffer_size, num_workers=num_workers\n",
    "                )\n",
    "                loader.start_fetching(epoch_training_dataset)\n",
    "\n",
    "                # Run each batch\n",
    "                while True:\n",
    "\n",
    "                    recording = loader.get_recording()\n",
    "\n",
    "                    if recording is None:\n",
    "                        break\n",
    "\n",
    "                    try:\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        results = self.run_recording(recording, train=True)\n",
    "                        self.metrics[\"train\"].append(results)\n",
    "\n",
    "                        # Don't print, just log\n",
    "                        self.logger.info(\n",
    "                            f\"Epoch {epoch}, Remaining {remaining}/{training_size}. Runtime {time.time() - start_time:.2f}s.\"\n",
    "                        )\n",
    "                        self.logger.info(\n",
    "                            f'Loss: {results[\"loss\"]:.4f}, Clip Loss: {results[\"clip_loss\"]:.4f}, MSE Loss: {results[\"mse_loss\"]:.4f}'\n",
    "                        )\n",
    "                        self.logger.info(\n",
    "                            f'Accuracy: {results[\"accuracy\"]:.4f}, Top 1: {results[\"top_1_accuracy\"]:.4f}, Top 5: {results[\"top_5_accuracy\"]:.4f}, Top 10: {results[\"top_10_accuracy\"]:.4f}'\n",
    "                        )\n",
    "                        remaining -= 1\n",
    "                    except:\n",
    "                        # Do log errors\n",
    "                        self.log_print(\n",
    "                            f'Error in epoch {epoch}, {recording.metadata[\"study\"]} {recording.metadata[\"subject\"]} {recording.metadata[\"task\"]} {recording.metadata[\"session\"]}'\n",
    "                        )\n",
    "                        continue\n",
    "                \n",
    "                # Save model after each epoch\n",
    "                elapsed_minutes = (time.time() - epoch_start_time) / 60\n",
    "                self.logger.info(\n",
    "                    f\"Epoch {epoch} completed in {elapsed_minutes:.2f}m. {elapsed_minutes / training_size:.2f}m per recording.\"\n",
    "                )\n",
    "                # Testing\n",
    "                self.log_print(f\"Testing at epoch {epoch}\")\n",
    "                with torch.no_grad():\n",
    "                    self.test(buffer_size=9, num_workers=3)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log_print(f\"Error in epoch {epoch}, {e}\")\n",
    "                self.save(f\"error_epoch_{epoch}\")\n",
    "                raise e\n",
    "            \n",
    "            self.save(f\"epoch_{epoch}\")\n",
    "\n",
    "        self.log_print(\"Training completed.\")\n",
    "\n",
    "    def run_recording(self, recording, train: bool) -> tp.Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Per recording processing for training and testing. Returns average metrics\n",
    "        and losses for the recording. Returns metrics on CPU.\n",
    "        \"\"\"\n",
    "        recording_loss, recording_clip_loss, recording_mse_loss = 0, 0, 0\n",
    "        (total, missed_recordings, missed_batches) = (\n",
    "            brain_segments.shape[0],\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "        (\n",
    "            recording_correct,\n",
    "            recording_top_1,\n",
    "            recording_top_5,\n",
    "            recording_top_10,\n",
    "        ) = (\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        with autocast(dtype=torch.bfloat16):\n",
    "            # Some processing to ensure dims match\n",
    "            brain_segments, audio_segments, layout, metadata = recording\n",
    "            brain_segments, audio_segments = self.discard_nan(\n",
    "                brain_segments[\"all\"], audio_segments\n",
    "            )\n",
    "\n",
    "            # Subject indices from the session mapping\n",
    "            subject_indices = torch.full(\n",
    "                (brain_segments.shape[0]),\n",
    "                self.subject_mapping[metadata[\"study\"] + \"_\" + metadata[\"subject\"]],\n",
    "                dtype=torch.int64,\n",
    "                requires_grad=False,\n",
    "            ).to(\n",
    "                self.device\n",
    "            )  # [B]\n",
    "\n",
    "            # Move self.device\n",
    "            brain_segments, audio_segments, layout = (\n",
    "                brain_segments.to(self.device),\n",
    "                audio_segments.to(self.device),\n",
    "                layout.to(self.device),\n",
    "            )  # [B, C, T], [B, mel_bins, T], [C, 2]\n",
    "\n",
    "            # Process by specified batch size\n",
    "            batch_indices = [\n",
    "                (i, min(i + self.config.batch_size, total))\n",
    "                for i in range(0, total, self.config.batch_size)\n",
    "            ]\n",
    "            for start, end in batch_indices:\n",
    "\n",
    "                if train:\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                # Slice by batch\n",
    "                brain_batch, audio_batch, subject_batch = (\n",
    "                    brain_segments[start:end],\n",
    "                    audio_segments[start:end],\n",
    "                    subject_indices[start:end],\n",
    "                )\n",
    "                # Forward pass\n",
    "                output = self.model(\n",
    "                    inputs={\"meg\": brain_batch},\n",
    "                    layout=layout,\n",
    "                    subjects=subject_batch,\n",
    "                )  # [B, C, T]\n",
    "\n",
    "                # Compute loss\n",
    "                mse_loss = torch.nn.functional.mse_loss(\n",
    "                    input=output, target=audio_batch, size_average=True\n",
    "                )\n",
    "                clip_results = CLIPLoss().forward(x_1=output, x_2=audio_batch)\n",
    "                clip_loss, clip_metrics = clip_results[\"loss\"], clip_results[\"metrics\"]\n",
    "\n",
    "                if self.config.use_clip_loss and self.config.use_mse_loss:\n",
    "                    loss = ((1 - self.config.alpha) * mse_loss) + (\n",
    "                        self.config.alpha * clip_loss\n",
    "                    )\n",
    "                elif not self.config.use_clip_loss and self.config.use_mse_loss:\n",
    "                    loss = mse_loss\n",
    "                elif self.config.use_clip_loss and not self.config.use_mse_loss:\n",
    "                    loss = clip_loss\n",
    "\n",
    "                if not torch.isnan(loss).any():\n",
    "                    # Backward pass\n",
    "                    if train:\n",
    "                        self.scaler.scale(loss).backward()\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                    # Store losses, move to CPU\n",
    "                    recording_loss += loss.detach().to(\"cpu\").item()\n",
    "                    recording_clip_loss += clip_loss.detach().to(\"cpu\").item()\n",
    "                    recording_mse_loss += mse_loss.detach().to(\"cpu\").item()\n",
    "                    # Store metrics, already on CPU\n",
    "                    recording_correct += clip_metrics[\"correct\"]\n",
    "                    recording_top_1 += clip_metrics[\"top_1_correct\"]\n",
    "                    recording_top_5 += clip_metrics[\"top_5_correct\"]\n",
    "                    recording_top_10 += clip_metrics[\"top_10_correct\"]\n",
    "                else:\n",
    "                    self.logger.info(\n",
    "                        f'Loss is NaN for recording {metadata[\"study\"]} {metadata[\"subject\"]} {metadata[\"task\"]} {metadata[\"session\"]}'\n",
    "                    )\n",
    "                    missed_recordings += end - start\n",
    "                    missed_batches += 1\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Correct for missed recordings and batches\n",
    "        total -= missed_recordings\n",
    "        batches = len(batch_indices) - missed_batches\n",
    "\n",
    "        # Loss divided by batches, metrics by total\n",
    "        return {\n",
    "            \"loss\": recording_loss / batches if batches > 0 else 0,\n",
    "            \"clip_loss\": recording_clip_loss / batches if batches > 0 else 0,\n",
    "            \"mse_loss\": recording_mse_loss / batches if batches > 0 else 0,\n",
    "            \"accuracy\": recording_correct / total,\n",
    "            \"top_1_accuracy\": recording_top_1 / total,\n",
    "            \"top_5_accuracy\": recording_top_5 / total,\n",
    "            \"top_10_accuracy\": recording_top_10 / total,\n",
    "        }\n",
    "\n",
    "    def test(self, buffer_size: int, num_workers: int):\n",
    "        \n",
    "        self.model.eval().to(self.device)\n",
    "        \n",
    "        test_start_time = time.time()\n",
    "        \n",
    "        test_datasets, test_dataloader = {}, {}\n",
    "        # Create dataset and loader\n",
    "        for test in self.dataset[\"test\"].keys():\n",
    "            # Randomly subsample recordings for each type of test\n",
    "            if len(self.dataset[\"test\"][test]) < self.config.random_test_size:\n",
    "                test_datasets[test] = self.dataset[\"test\"][test]\n",
    "            else:\n",
    "                test_datasets[test] = random.sample(\n",
    "                    self.dataset[\"test\"][test], self.config.random_test_size\n",
    "                )\n",
    "            # Create dataloader for each test\n",
    "            test_dataloader[test] = self.get_data_loader(\n",
    "                buffer_size=buffer_size, num_workers=num_workers\n",
    "            )\n",
    "            test_dataloader[test].start_fetching(test_datasets[test])\n",
    "                \n",
    "        test_sizes = {test: len(test_datasets[test]) for test in test_datasets.keys()}\n",
    "        \n",
    "        # Run tests\n",
    "        for test in test_datasets.keys():\n",
    "            while True:\n",
    "                recording = test_dataloader[test].get_recording()\n",
    "                \n",
    "                if recording is None:\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    results = self.run_recording(recording, train=False)\n",
    "                    self.metrics[\"test\"][test].append(results)\n",
    "\n",
    "                    # Log results\n",
    "                    self.logger.info(\n",
    "                        f\"Testing {test} {test_sizes[test]}/{len(test_datasets[test])}. Runtime {time.time() - start_time:.2f}s.\"\n",
    "                    )\n",
    "                    self.logger.info(\n",
    "                        f'Loss: {results[\"loss\"]:.4f}, Clip Loss: {results[\"clip_loss\"]:.4f}, MSE Loss: {results[\"mse_loss\"]:.4f}'\n",
    "                    )\n",
    "                    self.logger.info(\n",
    "                        f'Accuracy: {results[\"accuracy\"]:.4f}, Top 1: {results[\"top_1_accuracy\"]:.4f}, Top 5: {results[\"top_5_accuracy\"]:.4f}, Top 10: {results[\"top_10_accuracy\"]:.4f}'\n",
    "                    )\n",
    "                    \n",
    "                except:\n",
    "                    self.log_print(f'Error in testing {test} {recording.metadata[\"study\"]} {recording.metadata[\"subject\"]} {recording.metadata[\"task\"]} {recording.metadata[\"session\"]}')\n",
    "                    test_sizes[test] -= 1\n",
    "                    continue    \n",
    "                \n",
    "        # Log info\n",
    "        elapsed_minutes = (time.time() - test_start_time) / 60\n",
    "        self.logger.info(\n",
    "            f\"Testing completed in {elapsed_minutes:.2f}m.\"\n",
    "        )\n",
    "\n",
    "        return\n",
    "                \n",
    "    def pre_process_all_recordings(self, buffer_size: int, num_workers: int):\n",
    "        \"\"\"Pre-processes all data and saves as .pt in cache at once.\"\"\"\n",
    "\n",
    "        loader = self.get_data_loader(buffer_size=buffer_size, num_workers=num_workers)\n",
    "\n",
    "        if (\n",
    "            not self.dataset[\"train\"]\n",
    "            or not self.dataset[\"test\"][\"unseen_subject\"]\n",
    "            or not self.dataset[\"test\"][\"unseen_task\"]\n",
    "            or not self.dataset[\"test\"][\"unseen_both\"]\n",
    "        ):\n",
    "            self.partition_data()\n",
    "\n",
    "        all_recordings = (\n",
    "            self.dataset[\"train\"]\n",
    "            + self.dataset[\"test\"][\"unseen_subject\"]\n",
    "            + self.dataset[\"test\"][\"unseen_task\"]\n",
    "            + self.dataset[\"test\"][\"unseen_both\"]\n",
    "        )  # (study, subject, task, session)\n",
    "\n",
    "        random.shuffle(all_recordings)\n",
    "\n",
    "        total_recordings, remaining = len(all_recordings), len(all_recordings)\n",
    "        pbar = tqdm(total=total_recordings, desc=\"Loading recordings\")\n",
    "\n",
    "        loader.start_fetching(all_recordings)\n",
    "\n",
    "        while True:\n",
    "            recording = loader.get_recording()\n",
    "            if recording is None:\n",
    "                break\n",
    "            remaining -= 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    def get_data_loader(self, buffer_size: int, num_workers: int) -> ParallelDataLoader:\n",
    "        return ParallelDataLoader(\n",
    "            studies=self.studies,\n",
    "            pre_processor=self.pre_processor,\n",
    "            buffer_size=buffer_size,\n",
    "            num_workers=num_workers,\n",
    "            max_random_shift=self.config.max_random_shift,\n",
    "            window_size=self.config.window_size,\n",
    "            window_stride=self.config.window_stride,\n",
    "            baseline_window=self.config.baseline_window,\n",
    "            frequency_bands=self.config.frequency_bands,\n",
    "            brain_clipping=self.config.brain_clipping,\n",
    "            new_freq=self.config.new_freq,\n",
    "            notch_filter=self.config.notch_filter,\n",
    "            audio_sample_rate=self.config.audio_sample_rate,\n",
    "            hop_length=self.config.hop_length,\n",
    "            audio_processor=self.config.audio_model,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "    def discard_nan(\n",
    "        self,\n",
    "        brain: torch.Tensor,\n",
    "        audio: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        If any nan in brain or audio data, discard the batch.\n",
    "\n",
    "        Arguments:\n",
    "            brain -- The brain data tensor, [B, C, T]\n",
    "            audio -- The audio data, [B, mel_bins, T]\n",
    "        \"\"\"\n",
    "\n",
    "        valid_mask = ~(\n",
    "            torch.isnan(brain).any(dim=(1, 2)) | torch.isnan(audio).any(dim=(1, 2))\n",
    "        )\n",
    "\n",
    "        if valid_mask.all():\n",
    "            return brain, audio\n",
    "\n",
    "        # Apply the same mask to both tensors\n",
    "        filtered_brain = brain[valid_mask]\n",
    "        filtered_audio = audio[valid_mask]\n",
    "\n",
    "        if filtered_brain.shape[0] != filtered_audio.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Filtered brain and audio data must have the same number of samples\"\n",
    "            )\n",
    "\n",
    "        return filtered_brain, filtered_audio\n",
    "\n",
    "    def log_print(self, message):\n",
    "        print(message)\n",
    "        self.logger.info(message)\n",
    "\n",
    "\n",
    "def load_training_session():\n",
    "    pass\n",
    "\n",
    "\n",
    "training_config = TrainingConfigV0(\n",
    "    brain_encoder_config=SimpleConvConfig(),\n",
    "    data_partition={\n",
    "        \"gwilliams\": {\n",
    "            \"testing_subjects\": [19, 20, 21],\n",
    "            \"testing_tasks\": [0],\n",
    "        },\n",
    "        # \"schoffelen\": {\n",
    "        #     \"testing_subjects\": [],\n",
    "        #     \"testing_tasks\": [8, 9],\n",
    "        # },\n",
    "    },\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "session = TrainingSessionV0(\n",
    "    training_config,\n",
    "    studies=[\"gwilliams\"],  # \"schoffelen\"\n",
    "    data_path=\"data\",\n",
    "    save_path=\"saves/test\",\n",
    "    clear_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 22:02:49,974\tINFO worker.py:1821 -- Started a local Ray instance.\n",
      "Loading recordings:   5%|â–Œ         | 10/196 [01:24<24:39,  7.96s/it]\u001b[36m(raylet)\u001b[0m Spilled 2074 MiB, 11 objects, write throughput 1569 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_process_all_recordings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 265\u001b[0m, in \u001b[0;36mTrainingSessionV0.pre_process_all_recordings\u001b[0;34m(self, buffer_size, num_workers)\u001b[0m\n\u001b[1;32m    262\u001b[0m loader\u001b[38;5;241m.\u001b[39mstart_fetching(all_recordings)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Brain/brain-decoding/utils/dataloader.py:201\u001b[0m, in \u001b[0;36mParallelDataLoader.get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mempty() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(raylet)\u001b[0m Spilled 4306 MiB, 21 objects, write throughput 1751 MiB/s.\n"
     ]
    }
   ],
   "source": [
    "session.pre_process_all_recordings(buffer_size=8, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_sample_rate = 100\n",
    "frequency_bands = {\"all\": (0.5, 100)}\n",
    "subject, task, session = 0, 0, 0\n",
    "seed = 42\n",
    "max_random_shift = 1\n",
    "window_size = 4\n",
    "n_jobs = -1\n",
    "\n",
    "study = StudyFactory().create_study(\"gwilliams\", path=\"data/gwilliams\")\n",
    "pre_processor = PreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw, word, sound = study.clean_recording(\n",
    "    subject, task, session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.subjects_list[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 208, 400]),\n",
       " torch.Size([200, 128, 400]),\n",
       " torch.Size([208, 2]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.fetch import fetch_audio_and_brain_pairs\n",
    "\n",
    "brain_segments, audio_segments, layout = fetch_audio_and_brain_pairs(\n",
    "    subject=subject,\n",
    "    task=task,\n",
    "    session=session,\n",
    "    max_random_shift=max_random_shift,\n",
    "    window_size=window_size,\n",
    "    window_stride=1,\n",
    "    study=study,\n",
    "    pre_processor=pre_processor,\n",
    "    frequency_bands=frequency_bands,\n",
    "    new_freq=100,\n",
    "    audio_sample_rate=16000,\n",
    "    hop_length=160,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "brain_segments['all'].shape, audio_segments.shape, layout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SimpleConv: \n",
      "\tParams: 14432128\n",
      "\tConv blocks: 5\n",
      "\tTrans layers: 0\n"
     ]
    }
   ],
   "source": [
    "model = SimpleConv(SimpleConvConfig(transformer_layers=0)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "    {\"meg\": brain_segments[\"all\"].to(device)},\n",
    "    layout=layout.to(device),\n",
    "    subjects=torch.full((brain_segments[\"all\"].shape[0],), subject).to(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([199, 128, 400])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
