{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.compression import compress_directories, decompress_directories\n",
    "\n",
    "# base_path = \"downloaded_data/gwilliams\"\n",
    "# destination_path = \"data/gwilliams\"\n",
    "\n",
    "# decompress_directories(\n",
    "#     base_path,\n",
    "#     destination_path,\n",
    "#     checksum_file_name=\"checksums.txt\",\n",
    "#     delete_compressed_files=True,\n",
    "#     num_workers=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data partitioned on studies ['gwilliams']. Recordings:\n",
      "Train: 135, Unseen Task: 12, Unseen Subject: 45, Unseen Both: 4.\n",
      "\n",
      "\n",
      "SimpleConv: \n",
      "\tParams: 13973376\n",
      "\tConv blocks: 5\n",
      "\tTrans layers: 0\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import random\n",
    "import time\n",
    "from tracemalloc import start\n",
    "from tqdm import tqdm\n",
    "from config.simpleconv_config import SimpleConvConfig\n",
    "from models.simpleconv import SimpleConv\n",
    "from studies.study_factory import StudyFactory\n",
    "from utils.dataloader import ParallelDataLoader\n",
    "from utils.pre_processor import PreProcessor\n",
    "import typing as tp\n",
    "import json\n",
    "from itertools import product\n",
    "from torch.optim import AdamW, Adam\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "from utils.dataloader import ParallelDataLoader\n",
    "from utils.clip import CLIPLoss\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "from config import SimpleConvConfig, Config\n",
    "\n",
    "\n",
    "class TrainingConfigV0(Config):\n",
    "    def __init__(\n",
    "        self,\n",
    "        brain_encoder_config: SimpleConvConfig,\n",
    "        data_partition: tp.Dict[str, tp.Dict[str, tp.List[str]]],\n",
    "        # Pre-processing parameters\n",
    "        # Brain\n",
    "        new_freq: int = 100,\n",
    "        frequency_bands: tp.Tuple[str, tp.Tuple[int, int]] = {\"all\": (0.5, 100)},\n",
    "        max_random_shift: float = 2.0,\n",
    "        window_size: int = 4,\n",
    "        window_stride: int = 1,\n",
    "        brain_clipping: float = 20,\n",
    "        baseline_window: int = 0.5,\n",
    "        notch_filter: bool = True,\n",
    "        # Audio\n",
    "        audio_model: str = \"openai/whisper-large-v3\",\n",
    "        audio_sample_rate: int = 16000,\n",
    "        hop_length: int = 160,\n",
    "        # Hyperparameters\n",
    "        learning_rate: float = 3e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "        epochs: int = 50,\n",
    "        batch_size: int = 128,\n",
    "        use_clip_loss: bool = True,\n",
    "        use_mse_loss: bool = True,\n",
    "        alpha: float = 0.5,\n",
    "        random_test_size: int = 3,\n",
    "    ):\n",
    "        self.brain_encoder_config = brain_encoder_config\n",
    "        # key: study_name, value: dict with keys: \"testing_subjects\", \"testing_tasks\",\n",
    "        # where each value is a list of int. Ones not specified in either lists are\n",
    "        # used for training.\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "        # Pre-processing parameters\n",
    "        # Brain\n",
    "        self.new_freq = new_freq\n",
    "        self.frequency_bands = frequency_bands\n",
    "        self.max_random_shift = max_random_shift\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.baseline_window = baseline_window\n",
    "        self.notch_filter = notch_filter\n",
    "        self.brain_clipping = brain_clipping\n",
    "\n",
    "        # Audio\n",
    "        self.audio_model = audio_model\n",
    "        self.audio_sample_rate = audio_sample_rate\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.use_clip_loss = use_clip_loss\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        self.alpha = alpha\n",
    "        self.random_test_size = random_test_size\n",
    "\n",
    "        assert 0 <= self.alpha <= 1, \"Alpha must be between 0 and 1\"\n",
    "        assert use_clip_loss or use_mse_loss, \"At least one loss function must be used\"\n",
    "\n",
    "    # does not overide parent method\n",
    "    def to_dict_(self):\n",
    "        brain_encoder_config = self.brain_encoder_config.to_dict()\n",
    "        config = self.to_dict()\n",
    "        config[\"brain_encoder_config\"] = brain_encoder_config\n",
    "        return config\n",
    "\n",
    "\n",
    "class TrainingSessionV0:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TrainingConfigV0,\n",
    "        studies: tp.List[str],\n",
    "        data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "        save_path: str = \"/home/ubuntu/brain-decoding/saves\",\n",
    "        clear_cache: bool = False,\n",
    "    ):\n",
    "        \"\"\"Initializes a training session with the provided configuration and data.\n",
    "\n",
    "        Arguments:\n",
    "            config -- The configuration for the training session.\n",
    "            studies -- list of studies to train on. Partition policy determined in TrainingConfig\n",
    "            data_path -- The path to the data directory.\n",
    "            save_path -- The path to the directory where the model and logs will be saved.\n",
    "        \"\"\"\n",
    "        assert len(studies) > 0, \"At least one study root path must be provided\"\n",
    "        assert all(\n",
    "            os.path.exists(data_path + \"/\" + study) for study in studies\n",
    "        ), \"All study root paths must exist\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        logging.basicConfig(\n",
    "            filename=os.path.join(save_path, \"training_log.log\"),\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s %(message)s\",\n",
    "            filemode=\"w\",\n",
    "        )\n",
    "        self.logger = logging.getLogger()\n",
    "\n",
    "        self.config = config\n",
    "        self.data_path = data_path\n",
    "        self.save_path = save_path\n",
    "\n",
    "        # Create studies accessor\n",
    "        self.studies = {}\n",
    "        for study in studies:\n",
    "            path = os.path.join(data_path, study)\n",
    "            try:\n",
    "                self.studies[study] = StudyFactory.create_study(study, path)\n",
    "                if clear_cache:\n",
    "                    shutil.rmtree(study.cache_dir)\n",
    "                    os.makedirs(study.cache_dir)\n",
    "                    self.log_print(f\"Cleared cache for study {study}\")\n",
    "            except ValueError as e:\n",
    "                self.log_print(f\"Error loading study {study}: {e}\")\n",
    "\n",
    "        # Create preprocessor\n",
    "        self.pre_processor = PreProcessor()\n",
    "\n",
    "        self.dataset = {\n",
    "            \"train\": [],\n",
    "            \"test\": {\n",
    "                \"unseen_subject\": [],\n",
    "                \"unseen_task\": [],\n",
    "                \"unseen_both\": [],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        self.partition_data()\n",
    "\n",
    "        self.metrics = {\n",
    "            \"train\": [],\n",
    "            \"test\": {\n",
    "                \"unseen_subject\": [],\n",
    "                \"unseen_task\": [],\n",
    "                \"unseen_both\": [],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # If subject layer, assign a unique int to each subject in the dataset\n",
    "        self.subject_mapping, n = {}, 0\n",
    "        for study_name, study in self.studies.items():\n",
    "            for subject in study.subjects_list:\n",
    "                self.subject_mapping[f\"{study_name}_{subject}\"] = n\n",
    "                n += 1\n",
    "        self.config.brain_encoder_config.n_subjects = n\n",
    "\n",
    "        self.model = SimpleConv(self.config.brain_encoder_config)\n",
    "        self.error = None\n",
    "        self.optimizer = Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "        )\n",
    "\n",
    "    def partition_data(self):\n",
    "        \"\"\"\n",
    "        Partitions the data into training and various testing sets, based on\n",
    "        the named holdout sessions and tasks specified in TrainingConfig\n",
    "        \"\"\"\n",
    "        for study_name, study in self.studies.items():\n",
    "\n",
    "            if study_name not in self.config.data_partition:\n",
    "                raise ValueError(f\"Study {study_name} not found in data partition\")\n",
    "\n",
    "            data_partition = self.config.data_partition[study_name]\n",
    "\n",
    "            for subject, task, session in product(\n",
    "                [i for i in range(len(study.subjects_list))],\n",
    "                [i for i in range(len(study.tasks))],\n",
    "                [i for i in range(len(study.sessions))],\n",
    "            ):\n",
    "                # If recording exists\n",
    "                try:\n",
    "                    recording = study.recordings[subject][task][session]\n",
    "                except IndexError:\n",
    "                    # self.logger.error(f\"Recording not found for {study_name} {subject} {task} {session}\")\n",
    "                    continue\n",
    "\n",
    "                # Unseen both and task\n",
    "                if subject in data_partition[\"testing_subjects\"]:\n",
    "                    if task in data_partition[\"testing_tasks\"]:\n",
    "                        self.dataset[\"test\"][\"unseen_both\"].append(\n",
    "                            (study_name, subject, task, session)\n",
    "                        )\n",
    "                    else:\n",
    "                        self.dataset[\"test\"][\"unseen_task\"].append(\n",
    "                            (study_name, subject, task, session)\n",
    "                        )\n",
    "                # Unseen subject and train\n",
    "                else:\n",
    "                    if task in data_partition[\"testing_tasks\"]:\n",
    "                        self.dataset[\"test\"][\"unseen_subject\"].append(\n",
    "                            (study_name, subject, task, session)\n",
    "                        )\n",
    "                    else:\n",
    "                        self.dataset[\"train\"].append(\n",
    "                            (study_name, subject, task, session)\n",
    "                        )\n",
    "\n",
    "        self.log_print(\n",
    "            f\"Data partitioned on studies {list(self.studies.keys())}. Recordings:\"\n",
    "        )\n",
    "        self.log_print(\n",
    "            f\"Train: {len(self.dataset['train'])}, Unseen Task: {len(self.dataset['test']['unseen_task'])}, Unseen Subject: {len(self.dataset['test']['unseen_subject'])}, Unseen Both: {len(self.dataset['test']['unseen_both'])}.\\n\"\n",
    "        )\n",
    "\n",
    "    def train(self, device: str, buffer_size: int, num_workers: int):\n",
    "\n",
    "        # Set all training parameters\n",
    "        self.device = device\n",
    "        gpu_ok = False\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "        training_size = len(self.dataset[\"train\"])\n",
    "        self.scaler = GradScaler()\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Check if GPU is NVIDIA V100, A100, or H100\n",
    "        if torch.cuda.is_available():\n",
    "            device_cap = torch.cuda.get_device_capability()\n",
    "            if device_cap in ((7, 0), (8, 0), (9, 0)):\n",
    "                gpu_ok = True\n",
    "\n",
    "        if not gpu_ok:\n",
    "            self.log_print(\n",
    "                \"GPU is not NVIDIA V100, A100, or H100. Speedup numbers may be lower than expected.\"\n",
    "            )\n",
    "\n",
    "        for epoch in range(1, self.config.epochs + 1):\n",
    "            try:\n",
    "                self.model.to(device).train()\n",
    "                \n",
    "                epoch_start_time = time.time()\n",
    "\n",
    "                # Shuffle for each epoch, and start fetching\n",
    "                epoch_training_dataset, remaining = (\n",
    "                    self.dataset[\"train\"].copy(),\n",
    "                    training_size,\n",
    "                )\n",
    "                random.shuffle(epoch_training_dataset)\n",
    "                loader = self.get_data_loader(\n",
    "                    buffer_size=buffer_size, num_workers=num_workers\n",
    "                )\n",
    "                loader.start_fetching(epoch_training_dataset)\n",
    "\n",
    "                # Run each batch\n",
    "                while True:\n",
    "\n",
    "                    recording = loader.get_recording()\n",
    "\n",
    "                    if recording is None:\n",
    "                        break\n",
    "\n",
    "                    try:\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        results = self.run_recording(recording, train=True)\n",
    "                        self.metrics[\"train\"].append(results)\n",
    "\n",
    "                        # Don't print, just log\n",
    "                        self.logger.info(\n",
    "                            f\"Epoch {epoch}, Remaining {remaining}/{training_size}. Runtime {time.time() - start_time:.2f}s.\"\n",
    "                        )\n",
    "                        self.logger.info(\n",
    "                            f'Loss: {results[\"loss\"]:.4f}, Clip Loss: {results[\"clip_loss\"]:.4f}, MSE Loss: {results[\"mse_loss\"]:.4f}'\n",
    "                        )\n",
    "                        self.logger.info(\n",
    "                            f'Accuracy: {results[\"accuracy\"]:.4f}, Top 1: {results[\"top_1_accuracy\"]:.4f}, Top 5: {results[\"top_5_accuracy\"]:.4f}, Top 10: {results[\"top_10_accuracy\"]:.4f}'\n",
    "                        )\n",
    "                        remaining -= 1\n",
    "                    except:\n",
    "                        # Do log errors\n",
    "                        self.log_print(\n",
    "                            f'Error in epoch {epoch}, {recording.metadata[\"study\"]} {recording.metadata[\"subject\"]} {recording.metadata[\"task\"]} {recording.metadata[\"session\"]}'\n",
    "                        )\n",
    "                        continue\n",
    "                \n",
    "                # Save model after each epoch\n",
    "                elapsed_minutes = (time.time() - epoch_start_time) / 60\n",
    "                self.logger.info(\n",
    "                    f\"Epoch {epoch} completed in {elapsed_minutes:.2f}m. {elapsed_minutes / training_size:.2f}m per recording.\"\n",
    "                )\n",
    "                # Testing\n",
    "                self.log_print(f\"Testing at epoch {epoch}\")\n",
    "                with torch.no_grad():\n",
    "                    self.test(buffer_size=9, num_workers=3)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log_print(f\"Error in epoch {epoch}, {e}\")\n",
    "                self.save(f\"error_epoch_{epoch}\")\n",
    "                raise e\n",
    "            \n",
    "            self.save(f\"epoch_{epoch}\")\n",
    "\n",
    "        self.log_print(\"Training completed.\")\n",
    "\n",
    "    def run_recording(self, recording, train: bool) -> tp.Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Per recording processing for training and testing. Returns average metrics\n",
    "        and losses for the recording. Returns metrics on CPU.\n",
    "        \"\"\"\n",
    "        recording_loss, recording_clip_loss, recording_mse_loss = 0, 0, 0\n",
    "        (total, missed_recordings, missed_batches) = (\n",
    "            brain_segments.shape[0],\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "        (\n",
    "            recording_correct,\n",
    "            recording_top_1,\n",
    "            recording_top_5,\n",
    "            recording_top_10,\n",
    "        ) = (\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        with autocast(dtype=torch.bfloat16):\n",
    "            # Some processing to ensure dims match\n",
    "            brain_segments, audio_segments, layout, metadata = recording\n",
    "            brain_segments, audio_segments = self.discard_nan(\n",
    "                brain_segments[\"all\"], audio_segments\n",
    "            )\n",
    "\n",
    "            # Subject indices from the session mapping\n",
    "            subject_indices = torch.full(\n",
    "                (brain_segments.shape[0]),\n",
    "                self.subject_mapping[metadata[\"study\"] + \"_\" + metadata[\"subject\"]],\n",
    "                dtype=torch.int64,\n",
    "                requires_grad=False,\n",
    "            ).to(\n",
    "                self.device\n",
    "            )  # [B]\n",
    "\n",
    "            # Move self.device\n",
    "            brain_segments, audio_segments, layout = (\n",
    "                brain_segments.to(self.device),\n",
    "                audio_segments.to(self.device),\n",
    "                layout.to(self.device),\n",
    "            )  # [B, C, T], [B, mel_bins, T], [C, 2]\n",
    "\n",
    "            # Process by specified batch size\n",
    "            batch_indices = [\n",
    "                (i, min(i + self.config.batch_size, total))\n",
    "                for i in range(0, total, self.config.batch_size)\n",
    "            ]\n",
    "            for start, end in batch_indices:\n",
    "\n",
    "                if train:\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                # Slice by batch\n",
    "                brain_batch, audio_batch, subject_batch = (\n",
    "                    brain_segments[start:end],\n",
    "                    audio_segments[start:end],\n",
    "                    subject_indices[start:end],\n",
    "                )\n",
    "                # Forward pass\n",
    "                output = self.model(\n",
    "                    inputs={\"meg\": brain_batch},\n",
    "                    layout=layout,\n",
    "                    subjects=subject_batch,\n",
    "                )  # [B, C, T]\n",
    "\n",
    "                # Compute loss\n",
    "                mse_loss = torch.nn.functional.mse_loss(\n",
    "                    input=output, target=audio_batch, size_average=True\n",
    "                )\n",
    "                clip_results = CLIPLoss().forward(x_1=output, x_2=audio_batch)\n",
    "                clip_loss, clip_metrics = clip_results[\"loss\"], clip_results[\"metrics\"]\n",
    "\n",
    "                if self.config.use_clip_loss and self.config.use_mse_loss:\n",
    "                    loss = ((1 - self.config.alpha) * mse_loss) + (\n",
    "                        self.config.alpha * clip_loss\n",
    "                    )\n",
    "                elif not self.config.use_clip_loss and self.config.use_mse_loss:\n",
    "                    loss = mse_loss\n",
    "                elif self.config.use_clip_loss and not self.config.use_mse_loss:\n",
    "                    loss = clip_loss\n",
    "\n",
    "                if not torch.isnan(loss).any():\n",
    "                    # Backward pass\n",
    "                    if train:\n",
    "                        self.scaler.scale(loss).backward()\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                    # Store losses, move to CPU\n",
    "                    recording_loss += loss.detach().to(\"cpu\").item()\n",
    "                    recording_clip_loss += clip_loss.detach().to(\"cpu\").item()\n",
    "                    recording_mse_loss += mse_loss.detach().to(\"cpu\").item()\n",
    "                    # Store metrics, already on CPU\n",
    "                    recording_correct += clip_metrics[\"correct\"]\n",
    "                    recording_top_1 += clip_metrics[\"top_1_correct\"]\n",
    "                    recording_top_5 += clip_metrics[\"top_5_correct\"]\n",
    "                    recording_top_10 += clip_metrics[\"top_10_correct\"]\n",
    "                else:\n",
    "                    self.logger.info(\n",
    "                        f'Loss is NaN for recording {metadata[\"study\"]} {metadata[\"subject\"]} {metadata[\"task\"]} {metadata[\"session\"]}'\n",
    "                    )\n",
    "                    missed_recordings += end - start\n",
    "                    missed_batches += 1\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Correct for missed recordings and batches\n",
    "        total -= missed_recordings\n",
    "        batches = len(batch_indices) - missed_batches\n",
    "\n",
    "        # Loss divided by batches, metrics by total\n",
    "        return {\n",
    "            \"loss\": recording_loss / batches if batches > 0 else 0,\n",
    "            \"clip_loss\": recording_clip_loss / batches if batches > 0 else 0,\n",
    "            \"mse_loss\": recording_mse_loss / batches if batches > 0 else 0,\n",
    "            \"accuracy\": recording_correct / total,\n",
    "            \"top_1_accuracy\": recording_top_1 / total,\n",
    "            \"top_5_accuracy\": recording_top_5 / total,\n",
    "            \"top_10_accuracy\": recording_top_10 / total,\n",
    "        }\n",
    "\n",
    "    def test(self, buffer_size: int, num_workers: int):\n",
    "        \n",
    "        self.model.eval().to(self.device)\n",
    "        \n",
    "        test_start_time = time.time()\n",
    "        \n",
    "        test_datasets, test_dataloader = {}, {}\n",
    "        # Create dataset and loader\n",
    "        for test in self.dataset[\"test\"].keys():\n",
    "            # Randomly subsample recordings for each type of test\n",
    "            if len(self.dataset[\"test\"][test]) < self.config.random_test_size:\n",
    "                test_datasets[test] = self.dataset[\"test\"][test]\n",
    "            else:\n",
    "                test_datasets[test] = random.sample(\n",
    "                    self.dataset[\"test\"][test], self.config.random_test_size\n",
    "                )\n",
    "            # Create dataloader for each test\n",
    "            test_dataloader[test] = self.get_data_loader(\n",
    "                buffer_size=buffer_size, num_workers=num_workers\n",
    "            )\n",
    "            test_dataloader[test].start_fetching(test_datasets[test])\n",
    "                \n",
    "        test_sizes = {test: len(test_datasets[test]) for test in test_datasets.keys()}\n",
    "        \n",
    "        # Run tests\n",
    "        for test in test_datasets.keys():\n",
    "            while True:\n",
    "                recording = test_dataloader[test].get_recording()\n",
    "                \n",
    "                if recording is None:\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    results = self.run_recording(recording, train=False)\n",
    "                    self.metrics[\"test\"][test].append(results)\n",
    "\n",
    "                    # Log results\n",
    "                    self.logger.info(\n",
    "                        f\"Testing {test} {test_sizes[test]}/{len(test_datasets[test])}. Runtime {time.time() - start_time:.2f}s.\"\n",
    "                    )\n",
    "                    self.logger.info(\n",
    "                        f'Loss: {results[\"loss\"]:.4f}, Clip Loss: {results[\"clip_loss\"]:.4f}, MSE Loss: {results[\"mse_loss\"]:.4f}'\n",
    "                    )\n",
    "                    self.logger.info(\n",
    "                        f'Accuracy: {results[\"accuracy\"]:.4f}, Top 1: {results[\"top_1_accuracy\"]:.4f}, Top 5: {results[\"top_5_accuracy\"]:.4f}, Top 10: {results[\"top_10_accuracy\"]:.4f}'\n",
    "                    )\n",
    "                    \n",
    "                except:\n",
    "                    self.log_print(f'Error in testing {test} {recording.metadata[\"study\"]} {recording.metadata[\"subject\"]} {recording.metadata[\"task\"]} {recording.metadata[\"session\"]}')\n",
    "                    test_sizes[test] -= 1\n",
    "                    continue    \n",
    "                \n",
    "        # Log info\n",
    "        elapsed_minutes = (time.time() - test_start_time) / 60\n",
    "        self.logger.info(\n",
    "            f\"Testing completed in {elapsed_minutes:.2f}m.\"\n",
    "        )\n",
    "\n",
    "        return\n",
    "                \n",
    "    def pre_process_all_recordings(self, buffer_size: int, num_workers: int):\n",
    "        \"\"\"Pre-processes all data and saves as .pt in cache at once.\"\"\"\n",
    "\n",
    "        loader = self.get_data_loader(buffer_size=buffer_size, num_workers=num_workers)\n",
    "\n",
    "        if (\n",
    "            not self.dataset[\"train\"]\n",
    "            or not self.dataset[\"test\"][\"unseen_subject\"]\n",
    "            or not self.dataset[\"test\"][\"unseen_task\"]\n",
    "            or not self.dataset[\"test\"][\"unseen_both\"]\n",
    "        ):\n",
    "            self.partition_data()\n",
    "\n",
    "        all_recordings = (\n",
    "            self.dataset[\"train\"]\n",
    "            + self.dataset[\"test\"][\"unseen_subject\"]\n",
    "            + self.dataset[\"test\"][\"unseen_task\"]\n",
    "            + self.dataset[\"test\"][\"unseen_both\"]\n",
    "        )  # (study, subject, task, session)\n",
    "\n",
    "        random.shuffle(all_recordings)\n",
    "\n",
    "        total_recordings, remaining = len(all_recordings), len(all_recordings)\n",
    "        pbar = tqdm(total=total_recordings, desc=\"Loading recordings\")\n",
    "\n",
    "        loader.start_fetching(all_recordings)\n",
    "\n",
    "        while True:\n",
    "            recording = loader.get_recording()\n",
    "            if recording is None:\n",
    "                break\n",
    "            remaining -= 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    def get_data_loader(self, buffer_size: int, num_workers: int) -> ParallelDataLoader:\n",
    "        return ParallelDataLoader(\n",
    "            studies=self.studies,\n",
    "            pre_processor=self.pre_processor,\n",
    "            buffer_size=buffer_size,\n",
    "            num_workers=num_workers,\n",
    "            max_random_shift=self.config.max_random_shift,\n",
    "            window_size=self.config.window_size,\n",
    "            window_stride=self.config.window_stride,\n",
    "            baseline_window=self.config.baseline_window,\n",
    "            frequency_bands=self.config.frequency_bands,\n",
    "            brain_clipping=self.config.brain_clipping,\n",
    "            new_freq=self.config.new_freq,\n",
    "            notch_filter=self.config.notch_filter,\n",
    "            audio_sample_rate=self.config.audio_sample_rate,\n",
    "            hop_length=self.config.hop_length,\n",
    "            audio_processor=self.config.audio_model,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "    def save(self):\n",
    "        pass\n",
    "\n",
    "    def discard_nan(\n",
    "        self,\n",
    "        brain: torch.Tensor,\n",
    "        audio: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        If any nan in brain or audio data, discard the batch.\n",
    "\n",
    "        Arguments:\n",
    "            brain -- The brain data tensor, [B, C, T]\n",
    "            audio -- The audio data, [B, mel_bins, T]\n",
    "        \"\"\"\n",
    "\n",
    "        valid_mask = ~(\n",
    "            torch.isnan(brain).any(dim=(1, 2)) | torch.isnan(audio).any(dim=(1, 2))\n",
    "        )\n",
    "\n",
    "        if valid_mask.all():\n",
    "            return brain, audio\n",
    "\n",
    "        # Apply the same mask to both tensors\n",
    "        filtered_brain = brain[valid_mask]\n",
    "        filtered_audio = audio[valid_mask]\n",
    "\n",
    "        if filtered_brain.shape[0] != filtered_audio.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Filtered brain and audio data must have the same number of samples\"\n",
    "            )\n",
    "\n",
    "        return filtered_brain, filtered_audio\n",
    "\n",
    "    def log_print(self, message):\n",
    "        print(message)\n",
    "        self.logger.info(message)\n",
    "\n",
    "\n",
    "def load_training_session():\n",
    "    pass\n",
    "\n",
    "\n",
    "training_config = TrainingConfigV0(\n",
    "    brain_encoder_config=SimpleConvConfig(merger_pos_dim=256),\n",
    "    data_partition={\n",
    "        \"gwilliams\": {\n",
    "            \"testing_subjects\": [19, 20, 21],\n",
    "            \"testing_tasks\": [0],\n",
    "        },\n",
    "        # \"schoffelen\": {\n",
    "        #     \"testing_subjects\": [],\n",
    "        #     \"testing_tasks\": [8, 9],\n",
    "        # },\n",
    "\n",
    "    },\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "session = TrainingSessionV0(\n",
    "    training_config,\n",
    "    studies=[\"gwilliams\"],  # \"schoffelen\"\n",
    "    data_path=\"data\",\n",
    "    save_path=\"saves/test\",\n",
    "    clear_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 21:55:50,939\tINFO worker.py:1821 -- Started a local Ray instance.\n",
      "Loading recordings:   0%|          | 0/196 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_process_all_recordings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 547\u001b[0m, in \u001b[0;36mTrainingSessionV0.pre_process_all_recordings\u001b[0;34m(self, buffer_size, num_workers)\u001b[0m\n\u001b[1;32m    544\u001b[0m loader\u001b[38;5;241m.\u001b[39mstart_fetching(all_recordings)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m     recording \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_recording\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Brain/brain-decoding/utils/dataloader.py:205\u001b[0m, in \u001b[0;36mParallelDataLoader.get_recording\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue\u001b[38;5;241m.\u001b[39mempty() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(raylet)\u001b[0m Spilled 2415 MiB, 13 objects, write throughput 1903 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    }
   ],
   "source": [
    "session.pre_process_all_recordings(buffer_size=8, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_sample_rate = 100\n",
    "frequency_bands = {\"all\": (0.5, 100)}\n",
    "subject, task, session = 0, 0, 0\n",
    "seed = 42\n",
    "max_random_shift = 1\n",
    "window_size = 4\n",
    "n_jobs = -1\n",
    "\n",
    "study = StudyFactory().create_study(\"schoffelen\", path=\"data/schoffelen\")\n",
    "pre_processor = PreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw, word, sound = study.clean_recording(\n",
    "    subject, task, session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmne\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m layout \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mfind_layout(\u001b[43minfo\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'info' is not defined"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "layout = mne.find_layout(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([269, 2])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "torch.tensor([raw.info[\"chs\"][i]['loc'][:2] for i in range(len(raw.info[\"chs\"]))]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw.info['chs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.113741, 0.11446, -0.127079, 0.126274)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "raw.info[\"chs\"][0][\"loc\"]\n",
    "\n",
    "pos = [raw.info[\"chs\"][i][\"loc\"][:2] for i in range(208)]\n",
    "x_min, x_max, y_min, y_max = (\n",
    "    min(pos, key=lambda x: x[0])[0],\n",
    "    max(pos, key=lambda x: x[0])[0],\n",
    "    min(pos, key=lambda x: x[1])[1],\n",
    "    max(pos, key=lambda x: x[1])[1],\n",
    ")\n",
    "pos = [\n",
    "    [(x[0] - x_min) / (x_max - x_min), (x[1] - y_min) / (y_max - y_min)]\n",
    "    for x in pos\n",
    "]\n",
    "# pos = np.array(pos)\n",
    "# pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.059961, -0.006786,  0.096597],\n",
       "       [ 0.051159, -0.031374,  0.09772 ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60483257, 0.5663939 , 0.01918367, 0.03836735],\n",
       "       [0.58561324, 0.50873588, 0.01918367, 0.03836735],\n",
       "       [0.47990688, 0.43185853, 0.01918367, 0.03836735]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not load any valid 3D backend\npyvistaqt: No module named 'qtpy'\nnotebook: No module named 'ipyevents'\n\n install pyvistaqt, using pip or conda:\n'pip install pyvistaqt'\n'conda install -c conda-forge pyvistaqt'\n\n or install ipywidgets, if using a notebook backend\n'pip install ipywidgets'\n'conda install -c conda-forge ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmne\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mmne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_alignment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meeg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msurfaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhelmet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoord_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m mne\u001b[38;5;241m.\u001b[39mviz\u001b[38;5;241m.\u001b[39mset_3d_view(fig, azimuth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, elevation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m, distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m<decorator-gen-185>:12\u001b[0m, in \u001b[0;36mplot_alignment\u001b[0;34m(info, trans, subject, subjects_dir, surfaces, coord_frame, meg, eeg, fwd, dig, ecog, src, mri_fiducials, bem, seeg, fnirs, show_axes, dbs, fig, interaction, sensor_colors, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/mne/viz/_3d.py:819\u001b[0m, in \u001b[0;36mplot_alignment\u001b[0;34m(info, trans, subject, subjects_dir, surfaces, coord_frame, meg, eeg, fwd, dig, ecog, src, mri_fiducials, bem, seeg, fnirs, show_axes, dbs, fig, interaction, sensor_colors, verbose)\u001b[0m\n\u001b[1;32m    816\u001b[0m fid_colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(defaults[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_color\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlpa\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnasion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrpa\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# initialize figure\u001b[39;00m\n\u001b[0;32m--> 819\u001b[0m renderer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_renderer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSensor alignment: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubject\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbgcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m renderer\u001b[38;5;241m.\u001b[39mset_interaction(interaction)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m# plot head\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/mne/viz/backends/renderer.py:56\u001b[0m, in \u001b[0;36m_get_renderer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_renderer\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 56\u001b[0m     \u001b[43m_get_3d_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39m_Renderer(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/brain/lib/python3.11/site-packages/mne/viz/backends/renderer.py:185\u001b[0m, in \u001b[0;36m_get_3d_backend\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load any valid 3D backend\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    188\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    189\u001b[0m                 (\n\u001b[1;32m    190\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m install pyvistaqt, using pip or conda:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    191\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install pyvistaqt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    192\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconda install -c conda-forge pyvistaqt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    193\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m or install ipywidgets, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m                     \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif using a notebook backend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    195\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install ipywidgets\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    196\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconda install -c conda-forge ipywidgets\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    197\u001b[0m                 )\n\u001b[1;32m    198\u001b[0m             )\n\u001b[1;32m    199\u001b[0m         )\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     MNE_3D_BACKEND \u001b[38;5;241m=\u001b[39m _check_3d_backend_name(MNE_3D_BACKEND)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not load any valid 3D backend\npyvistaqt: No module named 'qtpy'\nnotebook: No module named 'ipyevents'\n\n install pyvistaqt, using pip or conda:\n'pip install pyvistaqt'\n'conda install -c conda-forge pyvistaqt'\n\n or install ipywidgets, if using a notebook backend\n'pip install ipywidgets'\n'conda install -c conda-forge ipywidgets'"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "\n",
    "fig = mne.viz.plot_alignment(\n",
    "    raw.info,\n",
    "    dig=False,\n",
    "    eeg=False,\n",
    "    surfaces=[],\n",
    "    meg=[\"helmet\", \"sensors\"],\n",
    "    coord_frame=\"meg\",\n",
    ")\n",
    "mne.viz.set_3d_view(fig, azimuth=50, elevation=90, distance=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.subjects_list[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([202, 208, 400]),\n",
       " torch.Size([202, 128, 400]),\n",
       " torch.Size([208, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.fetch import fetch_audio_and_brain_pairs\n",
    "\n",
    "brain_segments, audio_segments, layout = fetch_audio_and_brain_pairs(\n",
    "    subject=subject,\n",
    "    task=task,\n",
    "    session=session,\n",
    "    max_random_shift=max_random_shift,\n",
    "    window_size=window_size,\n",
    "    window_stride=1,\n",
    "    study=study,\n",
    "    pre_processor=pre_processor,\n",
    "    frequency_bands=frequency_bands,\n",
    "    new_freq=100,\n",
    "    audio_sample_rate=16000,\n",
    "    hop_length=160,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "brain_segments['all'].shape, audio_segments.shape, layout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChannelMerger: torch.Size([256, 512]), FT params: 0\n",
      "\n",
      "SimpleConv: \n",
      "\tParams: 14038912\n",
      "\tConv blocks: 5\n",
      "\tTrans layers: 0\n"
     ]
    }
   ],
   "source": [
    "model = SimpleConv(SimpleConvConfig(transformer_layers=0, merger_pos_dim=512)).to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(\n",
    "    {\"meg\": brain_segments[\"all\"].to('mps')},\n",
    "    layout=layout.to('mps'),\n",
    "    subjects=torch.full((brain_segments[\"all\"].shape[0],), subject).to('mps'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([199, 128, 400])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
