{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.compression import compress_directories, decompress_directories\n",
    "\n",
    "# base_path = \"downloaded_data/gwilliams2023\"\n",
    "# destination_path = \"data/gwilliams2023\"\n",
    "\n",
    "# decompress_directories(\n",
    "#     base_path,\n",
    "#     destination_path,\n",
    "#     checksum_file_name=\"checksums.txt\",\n",
    "#     delete_compressed_files=True,\n",
    "#     num_workers=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SimpleConvConfig\n",
    "from models.simpleconv import SimpleConv\n",
    "import torch\n",
    "\n",
    "config = SimpleConvConfig(\n",
    "    # Str to list of possible conditions\n",
    "    conditions=None,\n",
    "    # Channels\n",
    "    in_channels=208,\n",
    "    out_channels=128,\n",
    "    hidden_dim=256,\n",
    "    dropout=0.3,\n",
    "    # Sensor layout settings\n",
    "    layout_dim=2,\n",
    "    layout_proj=False,\n",
    "    layout_scaling=\"minmax\",\n",
    "    # Merger with spatial attn\n",
    "    merger=False,\n",
    "    merger_emb_dim=0,\n",
    "    merger_channels=0,\n",
    "    merger_dropout=0.0,\n",
    "    merger_conditional=None,\n",
    "    # Inital\n",
    "    initial_linear=256,\n",
    "    initial_depth=1,\n",
    "    # Conditional layers\n",
    "    conditional_layers=False,\n",
    "    conditional_layers_dim=None,  # input or hidden_dim\n",
    "    # Conv layer overall structure\n",
    "    depth=4,\n",
    "    kernel_size=3,\n",
    "    growth=1.0,\n",
    "    dilation_growth=2,\n",
    "    dilation_period=5,\n",
    "    glu=1,\n",
    "    conv_dropout=0.2,\n",
    "    dropout_input=0.2,\n",
    "    batch_norm=True,\n",
    "    # Quantizer\n",
    "    quantizer=False,\n",
    "    num_codebooks=0,\n",
    "    codebook_size=0,\n",
    "    quantizer_commitment=0,\n",
    "    quantizer_temp_init=0,\n",
    "    quantizer_temp_min=0,\n",
    "    quantizer_temp_decay=0,\n",
    "    # Transformers Encoders\n",
    "    transformer_input=None,\n",
    "    transformer_encoder_emb=None,\n",
    "    transformer_encoder_layers=0,\n",
    "    transformer_encoder_heads=0,\n",
    "    # Transformer Decoders\n",
    "    transformer_decoder_emb=None,\n",
    "    transformer_decoder_layers=0,\n",
    "    transformer_decoder_heads=0,\n",
    "    transformer_decoder_dim=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import time\n",
    "from librosa import cache\n",
    "from regex import E\n",
    "from tqdm import tqdm\n",
    "from config.simpleconv_config import SimpleConvConfig\n",
    "from models.simpleconv import SimpleConv\n",
    "from studies.study_factory import StudyFactory\n",
    "import typing as tp\n",
    "import json\n",
    "from itertools import product\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from .dataloader import DataLoader\n",
    "from .dataloader.audio_batch import AudioBatch\n",
    "from .losses.clip import CLIPLoss\n",
    "from config import SimpleConvConfig, Config\n",
    "from train.training_session import TrainingSession\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "class TrainingConfigV0(Config):\n",
    "    def __init__(\n",
    "        self,\n",
    "        brain_encoder_config: SimpleConvConfig,\n",
    "        data_partition: tp.Dict[str, tp.Dict[str, tp.List[str]]],\n",
    "        # Pre-processing parameters\n",
    "        # Brain\n",
    "        new_freq: int = 100,\n",
    "        frequency_bands: tp.Tuple[str, tp.Tuple[float, float]] = {\"all\": (0.5, 100)},\n",
    "        max_random_shift: float = 2.0,\n",
    "        window_size: int = 4,\n",
    "        window_stride: int = 1,\n",
    "        brain_clipping: float = 20,\n",
    "        baseline_window: int = 0.5,\n",
    "        notch_filter: bool = True,\n",
    "        scaling: str = \"minmax\",\n",
    "        # Audio\n",
    "        audio_model: str = \"openai/whisper-large-v3\",\n",
    "        audio_sample_rate: int = 16000,\n",
    "        hop_length: int = 160,\n",
    "        # Hyperparameters\n",
    "        learning_rate: float = 3e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "        epochs: int = 50,\n",
    "        batch_size: int = 128,\n",
    "        use_clip_loss: bool = True,\n",
    "        use_mse_loss: bool = True,\n",
    "        alpha: float = 0.5,\n",
    "        random_test_size: int = 3,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        self.brain_encoder_config = brain_encoder_config\n",
    "        # key: study_name, value: dict with keys: \"testing_subjects\", \"testing_tasks\",\n",
    "        # where each value is a list of int. Ones not specified in either lists are\n",
    "        # used for training.\n",
    "        self.data_partition = data_partition\n",
    "\n",
    "        # Pre-processing parameters\n",
    "        # Brain\n",
    "        self.new_freq = new_freq\n",
    "        self.frequency_bands = frequency_bands\n",
    "        self.max_random_shift = max_random_shift\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.baseline_window = baseline_window\n",
    "        self.notch_filter = notch_filter\n",
    "        self.brain_clipping = brain_clipping\n",
    "        self.scaling = scaling\n",
    "\n",
    "        # Audio\n",
    "        self.audio_model = audio_model\n",
    "        self.audio_sample_rate = audio_sample_rate\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.use_clip_loss = use_clip_loss\n",
    "        self.use_mse_loss = use_mse_loss\n",
    "        self.alpha = alpha\n",
    "        self.random_test_size = random_test_size\n",
    "        self.seed = seed\n",
    "\n",
    "        assert 0 <= self.alpha <= 1, \"Alpha must be between 0 and 1\"\n",
    "        assert use_clip_loss or use_mse_loss, \"At least one loss function must be used\"\n",
    "\n",
    "    # does not overide parent method\n",
    "    def to_dict_(self):\n",
    "        brain_encoder_config = self.brain_encoder_config.to_dict()\n",
    "        config = self.to_dict()\n",
    "        config[\"brain_encoder_config\"] = brain_encoder_config\n",
    "        return config\n",
    "\n",
    "    def from_dict(self, config: tp.Dict[str, tp.Any]):\n",
    "        self.brain_encoder_config = SimpleConvConfig.from_dict(\n",
    "            config[\"brain_encoder_config\"]\n",
    "        )\n",
    "        self.data_partition = config[\"data_partition\"]\n",
    "        self.new_freq = config[\"new_freq\"]\n",
    "        self.frequency_bands = config[\"frequency_bands\"]\n",
    "        self.max_random_shift = config[\"max_random_shift\"]\n",
    "        self.window_size = config[\"window_size\"]\n",
    "        self.window_stride = config[\"window_stride\"]\n",
    "        self.baseline_window = config[\"baseline_window\"]\n",
    "        self.notch_filter = config[\"notch_filter\"]\n",
    "        self.brain_clipping = config[\"brain_clipping\"]\n",
    "        self.scaling = config[\"scaling\"]\n",
    "        self.audio_model = config[\"audio_model\"]\n",
    "        self.audio_sample_rate = config[\"audio_sample_rate\"]\n",
    "        self.hop_length = config[\"hop_length\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.weight_decay = config[\"weight_decay\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.use_clip_loss = config[\"use_clip_loss\"]\n",
    "        self.use_mse_loss = config[\"use_mse_loss\"]\n",
    "        self.alpha = config[\"alpha\"]\n",
    "        self.random_test_size = config[\"random_test_size\"]\n",
    "        self.seed = config[\"seed\"]\n",
    "        return self\n",
    "\n",
    "\n",
    "class TrainingSessionV0(TrainingSession):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TrainingConfigV0 = None,\n",
    "        studies: tp.Dict[str, str] = None,\n",
    "        data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "        save_path: str = \"/home/ubuntu/brain-decoding/saves\",\n",
    "        clear_cache: bool = False,\n",
    "        cache_enabled: bool = True,\n",
    "        max_cache_size: int = 100,\n",
    "    ):\n",
    "        \"\"\"Initializes a training session with the provided configuration and data.\n",
    "        This version deals with audio batches.\n",
    "\n",
    "        Arguments:\n",
    "            config -- The configuration for the training session.\n",
    "            studies -- dict of studies, batch type. Partition policy determined in TrainingConfig\n",
    "                    Batch type determines how to load data from study.\n",
    "\n",
    "            data_path -- The path to the data directory.\n",
    "            save_path -- The path to the directory where the model and logs will be saved.\n",
    "            clear_cache -- Whether to clear the cache for the studies.\n",
    "            cache_enabled -- Whether to enable caching for the studies.\n",
    "            max_cache_size -- The maximum number of stimulis in cache.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(\n",
    "            config=config,\n",
    "            studies=studies,\n",
    "            data_path=data_path,\n",
    "            save_path=save_path,\n",
    "            clear_cache=clear_cache,\n",
    "            cache_enabled=cache_enabled,\n",
    "            max_cache_size=max_cache_size,\n",
    "        )\n",
    "\n",
    "        # Set conditions\n",
    "        if self.config.brain_encoder_config.conditions:\n",
    "\n",
    "            if \"study\" in self.config.brain_encoder_config.conditions:\n",
    "                self.config.brain_encoder_config.conditions[\"study\"] = list(\n",
    "                    studies.keys()\n",
    "                )\n",
    "\n",
    "            if \"subjects\" in self.config.brain_encoder_config.conditions:\n",
    "                subjects = set()\n",
    "                for recording in self.recordings:\n",
    "                    subjects.add(f\"{recording.study_name}_{recording.subject_id}\")\n",
    "                self.config.brain_encoder_config.conditions[\"subjects\"] = list(subjects)\n",
    "\n",
    "        self.model = SimpleConv(self.config.brain_encoder_config)\n",
    "\n",
    "        self.optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "        )\n",
    "        self.clip_loss, self.mse_loss = CLIPLoss(), torch.nn.functional.mse_loss()\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        device: str,\n",
    "        buffer_size: int,\n",
    "        num_workers: int,\n",
    "        max_cache_size: int,\n",
    "        current_epoch: int = 0,\n",
    "    ):\n",
    "\n",
    "        # Set all training parameters\n",
    "        self.device = device\n",
    "        gpu_ok = False\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "        training_size = len(self.dataset[\"train\"])\n",
    "        self.scaler = GradScaler()\n",
    "        self.model.to(device)\n",
    "        self.clip_loss.to(device)\n",
    "\n",
    "        # Check if GPU is NVIDIA V100, A100, or H100\n",
    "        if torch.cuda.is_available():\n",
    "            device_cap = torch.cuda.get_device_capability()\n",
    "            if device_cap in ((7, 0), (8, 0), (9, 0)):\n",
    "                gpu_ok = True\n",
    "        if not gpu_ok:\n",
    "            self.log_print(\n",
    "                \"GPU is not NVIDIA V100, A100, or H100. Speedup numbers may be lower than expected.\"\n",
    "            )\n",
    "\n",
    "        # Fetch recordings\n",
    "        if self.dataloader is None:\n",
    "            self.dataloader = self.get_dataloader(\n",
    "                buffer_size=buffer_size,\n",
    "                num_workers=num_workers,\n",
    "                max_cache_size=max_cache_size,\n",
    "            )\n",
    "\n",
    "        for epoch in range(current_epoch + 1, self.config.epochs + 1):\n",
    "            try:\n",
    "                self.model.to(device).train()\n",
    "                epoch_start_time = time.time()\n",
    "\n",
    "                # Shuffle for each epoch, and start fetching\n",
    "                epoch_training_dataset, remaining = (\n",
    "                    self.dataset[\"train\"].copy(),\n",
    "                    training_size,\n",
    "                )\n",
    "                # For reproducibility\n",
    "                self.set_seed(int(self.config.seed + epoch))\n",
    "                random.shuffle(epoch_training_dataset)\n",
    "                self.dataloader.start_fetching(epoch_training_dataset, cache=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.log_print(f\"Error in epoch {epoch} during initialization, {e}\")\n",
    "                self.save(f\"error_epoch_{epoch}\")\n",
    "\n",
    "            # Run each batch\n",
    "            while True:\n",
    "\n",
    "                batch = self.dataloader.get_recording()\n",
    "                if batch is None:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    results = self.run_batch(batch, train=True)\n",
    "                    self.metrics[\"train\"].append(results)\n",
    "\n",
    "                    # Don't print, just log\n",
    "                    self.logger.info(\n",
    "                        f\"Epoch {epoch}, Remaining {remaining}/{training_size}. Runtime {time.time() - start_time:.2f}s.\"\n",
    "                    )\n",
    "                    self.logger.info(\n",
    "                        f'Loss: {results[\"loss\"]:.4f}, Clip Loss: {results[\"clip_loss\"]:.4f}, MSE Loss: {results[\"mse_loss\"]:.4f}, Commitment Loss: {results[\"commitment_loss\"]:.4f}'\n",
    "                    )\n",
    "                    self.logger.info(\n",
    "                        f'Accuracy: {results[\"accuracy\"]:.4f}, Top 1: {results[\"top_1_accuracy\"]:.4f}, Top 5: {results[\"top_5_accuracy\"]:.4f}, Top 10: {results[\"top_10_accuracy\"]:.4f}, Perplexity: {results[\"perplexity\"]:.4f}'\n",
    "                    )\n",
    "                    remaining -= 1\n",
    "                except Exception as e:\n",
    "                    # Do log errors\n",
    "                    self.log_print(\n",
    "                        f\"Error in epoch {epoch}, {batch.recording.study_name} {batch.recording.subject_id} {batch.recording.session_id} {batch.recording.task_id}. Skipping.\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "            elapsed_minutes = (time.time() - epoch_start_time) / 60\n",
    "            self.log_print(\n",
    "                f\"Epoch {epoch} completed in {elapsed_minutes:.2f}m. {elapsed_minutes / training_size:.2f}m per recording.\"\n",
    "            )\n",
    "\n",
    "            # Testing\n",
    "            try:\n",
    "                self.log_print(f\"Testing at epoch {epoch}\")\n",
    "                with torch.no_grad():\n",
    "                    self.test(\n",
    "                        buffer_size=buffer_size,\n",
    "                        num_workers=num_workers,\n",
    "                        max_cache_size=max_cache_size,\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                self.log_print(f\"Error in epoch {epoch} during testing, {e}\")\n",
    "                self.save(f\"error_epoch_{epoch}\")\n",
    "                raise e\n",
    "\n",
    "            # Save model\n",
    "            self.save(f\"epoch_{epoch}\")\n",
    "\n",
    "        self.log_print(\"Training completed.\")\n",
    "\n",
    "    def run_batch(self, batch: AudioBatch, train: bool) -> tp.Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Per recording processing for training and testing. Returns average metrics\n",
    "        and losses for the recording. Returns metrics on CPU.\n",
    "        \"\"\"\n",
    "\n",
    "        # Some processing to ensure dims match\n",
    "        brain_segments, audio_segments, recording = batch\n",
    "        brain_segments, audio_segments = self.discard_nan(\n",
    "            brain_segments[\"all\"], audio_segments\n",
    "        )\n",
    "\n",
    "        # Models config decides if it is used\n",
    "        conditions = {\n",
    "            \"study\": str(recording.study_name),\n",
    "            \"subject\": str(recording.study_name) + \"_\" + str(recording.subject_id),\n",
    "        }\n",
    "\n",
    "        # Shuffle segments\n",
    "        shuffle_indices = torch.randperm(brain_segments.shape[0])\n",
    "        brain_segments, audio_segments = (\n",
    "            brain_segments[shuffle_indices].to(self.device),\n",
    "            audio_segments[shuffle_indices].to(self.device),\n",
    "        )  # [B, C, T], [B, mel_bins, T]\n",
    "\n",
    "        # Process by specified batch size\n",
    "        batch_indices = [\n",
    "            (i, min(i + self.config.batch_size, total))\n",
    "            for i in range(0, total, self.config.batch_size)\n",
    "        ]\n",
    "\n",
    "        # Initialize recording metrics\n",
    "        (\n",
    "            recording_loss,\n",
    "            recording_clip_loss,\n",
    "            recording_mse_loss,\n",
    "            recording_commitment_loss,\n",
    "        ) = (0, 0, 0, 0)\n",
    "\n",
    "        (total, missed_recordings, missed_batches) = (\n",
    "            brain_segments.shape[0],\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "        (\n",
    "            recording_correct,\n",
    "            recording_top_1,\n",
    "            recording_top_5,\n",
    "            recording_top_10,\n",
    "            recording_perplexity,\n",
    "            recording_temp,\n",
    "        ) = (\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        with autocast(dtype=torch.bfloat16):\n",
    "\n",
    "            for start, end in batch_indices:\n",
    "\n",
    "                try:\n",
    "                        \n",
    "                    if train:\n",
    "                        self.optimizer.zero_grad()\n",
    "\n",
    "                    # Slice by batch\n",
    "                    brain_batch, audio_batch = (\n",
    "                        brain_segments[start:end],\n",
    "                        audio_segments[start:end],\n",
    "                    )\n",
    "\n",
    "                    # Forward pass\n",
    "                    (output, quantizer_metrics) = self.model(\n",
    "                        x=brain_batch,\n",
    "                        recording=recording,\n",
    "                        conditions=conditions,\n",
    "                        mel=audio_batch,\n",
    "                        train=True,\n",
    "                    )  # [B, C, T]\n",
    "\n",
    "                    # Compute loss\n",
    "                    mse_loss = self.mse_loss(\n",
    "                        input=output, target=audio_batch, reduction=\"mean\"\n",
    "                    )\n",
    "                    clip_results = self.clip_loss(x_1=output, x_2=audio_batch)\n",
    "                    clip_loss, clip_metrics = clip_results[\"loss\"], clip_results[\"metrics\"]\n",
    "\n",
    "                    # Sum loss based on config\n",
    "                    if self.config.use_clip_loss and self.config.use_mse_loss:\n",
    "                        loss = ((1 - self.config.alpha) * mse_loss) + (\n",
    "                            self.config.alpha * clip_loss\n",
    "                        )\n",
    "                    elif not self.config.use_clip_loss and self.config.use_mse_loss:\n",
    "                        loss = mse_loss\n",
    "                    elif self.config.use_clip_loss and not self.config.use_mse_loss:\n",
    "                        loss = clip_loss\n",
    "\n",
    "                    if quantizer_metrics is not None:\n",
    "                        if \"commitment_loss\" in quantizer_metrics:\n",
    "                            loss += quantizer_metrics[\"commitment_loss\"]\n",
    "\n",
    "                    # Backward pass\n",
    "                    if not torch.isnan(loss).any():\n",
    "\n",
    "                        if train:\n",
    "                            self.scaler.scale(loss).backward()\n",
    "                            self.scaler.step(self.optimizer)\n",
    "                            self.scaler.update()\n",
    "\n",
    "                        # Store losses, move to CPU\n",
    "                        recording_loss += loss.detach().to(\"cpu\").item()\n",
    "                        recording_clip_loss += clip_loss.detach().to(\"cpu\").item()\n",
    "                        recording_mse_loss += mse_loss.detach().to(\"cpu\").item()\n",
    "\n",
    "                        # Store metrics, already on CPU\n",
    "                        recording_correct += clip_metrics[\"correct\"]\n",
    "                        recording_top_1 += clip_metrics[\"top_1_correct\"]\n",
    "                        recording_top_5 += clip_metrics[\"top_5_correct\"]\n",
    "                        recording_top_10 += clip_metrics[\"top_10_correct\"]\n",
    "\n",
    "                        # Quantizer metrics\n",
    "                        if quantizer_metrics is not None:\n",
    "                            if \"perplexity\" in quantizer_metrics:\n",
    "                                perplexity = (\n",
    "                                    quantizer_metrics[\"perplexity\"]\n",
    "                                    .detach()\n",
    "                                    .to(\"cpu\")\n",
    "                                    .mean(dim=0)\n",
    "                                )\n",
    "                                recording_perplexity += perplexity.item()\n",
    "                            if \"temp\" in quantizer_metrics:\n",
    "                                recording_temp += (\n",
    "                                    quantizer_metrics[\"temp\"].detach().to(\"cpu\").item()\n",
    "                                )\n",
    "                            if \"commitment_loss\" in quantizer_metrics:\n",
    "                                recording_commitment_loss += (\n",
    "                                    quantizer_metrics[\"commitment_loss\"]\n",
    "                                    .detach()\n",
    "                                    .to(\"cpu\")\n",
    "                                    .item()\n",
    "                                )\n",
    "                    else:\n",
    "                        self.logger.info(\n",
    "                            f\"Loss is NaN for {recording.study_name} {recording.subject_id} {recording.session_id} {recording.task_id}.\"\n",
    "                        )\n",
    "                        missed_recordings += end - start\n",
    "                        missed_batches += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    self.logger.info(\n",
    "                        f\"Error in processing {recording.study_name} {recording.subject_id} {recording.session_id} {recording.task_id}.\"\n",
    "                    )\n",
    "                    missed_recordings += end - start\n",
    "                    missed_batches += 1\n",
    "                    continue\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Correct for missed recordings and batches\n",
    "        total -= missed_recordings\n",
    "        batches = len(batch_indices) - missed_batches\n",
    "\n",
    "        # Loss divided by batches, metrics by total\n",
    "        return {\n",
    "            \"loss\": recording_loss / batches if batches > 0 else 0,\n",
    "            \"clip_loss\": recording_clip_loss / batches if batches > 0 else 0,\n",
    "            \"mse_loss\": recording_mse_loss / batches if batches > 0 else 0,\n",
    "            \"commitment_loss\": (\n",
    "                recording_commitment_loss / batches if batches > 0 else 0\n",
    "            ),\n",
    "            \"perplexity\": recording_perplexity / batches if batches > 0 else 0,\n",
    "            \"accuracy\": recording_correct / total,\n",
    "            \"top_1_accuracy\": recording_top_1 / total,\n",
    "            \"top_5_accuracy\": recording_top_5 / total,\n",
    "            \"top_10_accuracy\": recording_top_10 / total,\n",
    "        }\n",
    "\n",
    "    def test(self, buffer_size: int, num_workers: int, max_cache_size: int):\n",
    "\n",
    "        self.model.eval().to(self.device)\n",
    "        self.set_seed(self.config.seed)\n",
    "        test_start_time = time.time()\n",
    "\n",
    "        test_datasets = {}\n",
    "\n",
    "        # Create dataset and loader\n",
    "        for test in self.dataset[\"test\"].keys():\n",
    "            # Randomly subsample recordings for each type of test\n",
    "            if len(self.dataset[\"test\"][test]) < self.config.random_test_size:\n",
    "                test_datasets[test] = self.dataset[\"test\"][test]\n",
    "            else:\n",
    "                test_datasets[test] = random.sample(\n",
    "                    self.dataset[\"test\"][test], self.config.random_test_size\n",
    "                )\n",
    "\n",
    "            if self.test_dataloader.get(test) is None:\n",
    "                self.test_dataloader[test] = self.get_dataloader(\n",
    "                    buffer_size=buffer_size,\n",
    "                    num_workers=num_workers,\n",
    "                    max_cache_size=max_cache_size,\n",
    "                )\n",
    "            self.test_dataloader[test].start_fetching(test_datasets[test], cache=True)\n",
    "\n",
    "        test_sizes = {test: len(test_datasets[test]) for test in test_datasets.keys()}\n",
    "\n",
    "        # Run tests\n",
    "        for test in test_datasets.keys():\n",
    "\n",
    "            while True:\n",
    "\n",
    "                batch = self.test_dataloader[test].get_recording()\n",
    "                if batch is None:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    results = self.run_batch(batch, train=False)\n",
    "                    self.metrics[\"test\"][test].append(results)\n",
    "\n",
    "                    # Log results\n",
    "                    self.logger.info(\n",
    "                        f\"Testing {test} {test_sizes[test]}/{len(test_datasets[test])}. Runtime {time.time() - start_time:.2f}s.\"\n",
    "                    )\n",
    "                    self.logger.info(\n",
    "                        f'Loss: {results[\"loss\"]:.4f}, Clip Loss: {results[\"clip_loss\"]:.4f}, MSE Loss: {results[\"mse_loss\"]:.4f}, Commitment Loss: {results[\"commitment_loss\"]:.4f}'\n",
    "                    )\n",
    "                    self.logger.info(\n",
    "                        f'Accuracy: {results[\"accuracy\"]:.4f}, Top 1: {results[\"top_1_accuracy\"]:.4f}, Top 5: {results[\"top_5_accuracy\"]:.4f}, Top 10: {results[\"top_10_accuracy\"]:.4f}, Perplexity: {results[\"perplexity\"]:.4f}'\n",
    "                    )\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.log_print(\n",
    "                        f\"Error in testing {test}, {batch.recording.study_name} {batch.recording.subject_id} {batch.recording.session_id} {batch.recording.task_id}. Skipping.\"\n",
    "                    )\n",
    "                    test_sizes[test] -= 1\n",
    "                    continue\n",
    "\n",
    "        # Log info\n",
    "        elapsed_minutes = (time.time() - test_start_time) / 60\n",
    "        self.logger.info(f\"Testing completed in {elapsed_minutes:.2f}m.\")\n",
    "        return\n",
    "\n",
    "    def get_dataloader(self, buffer_size, num_workers, max_cache_size):\n",
    "        dataloader = DataLoader(\n",
    "            buffer_size=buffer_size,\n",
    "            max_cache_size_gb=max_cache_size,\n",
    "            cache_dir=\"cache\",\n",
    "            notch_filter=self.config.notch_filter,\n",
    "            frequency_bands=self.config.frequency_bands,\n",
    "            scaling=self.config.scaling,\n",
    "            brain_clipping=self.config.brain_clipping,\n",
    "            baseline_window=self.config.baseline_window,\n",
    "            new_freq=self.config.new_freq,\n",
    "            batch_types={\"audio\": num_workers},\n",
    "            batch_kwargs={\n",
    "                \"audio\": {\n",
    "                    \"max_random_shift\": self.config.max_random_shift,\n",
    "                    \"window_size\": self.config.window_size,\n",
    "                    \"window_stride\": self.config.window_stride,\n",
    "                    \"audio_sample_rate\": self.config.audio_sample_rate,\n",
    "                    \"hop_length\": self.config.hop_length,\n",
    "                    \"audio_processor\": self.config.audio_model,\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def discard_nan(\n",
    "        self,\n",
    "        brain: torch.Tensor,\n",
    "        audio: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        If any nan in brain or audio data, discard the batch.\n",
    "\n",
    "        Arguments:\n",
    "            brain -- The brain data tensor, [B, C, T]\n",
    "            audio -- The audio data, [B, mel_bins, T]\n",
    "        \"\"\"\n",
    "\n",
    "        valid_mask = ~(\n",
    "            torch.isnan(brain).any(dim=(1, 2)) | torch.isnan(audio).any(dim=(1, 2))\n",
    "        )\n",
    "\n",
    "        if valid_mask.all():\n",
    "            return brain, audio\n",
    "\n",
    "        # Apply the same mask to both tensors\n",
    "        filtered_brain = brain[valid_mask]\n",
    "        filtered_audio = audio[valid_mask]\n",
    "\n",
    "        if filtered_brain.shape[0] != filtered_audio.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Filtered brain and audio data must have the same number of samples\"\n",
    "            )\n",
    "\n",
    "        return filtered_brain, filtered_audio\n",
    "\n",
    "    def pre_process_all_recordings(\n",
    "        self, buffer_size: int, num_workers: int, max_cache_size: int\n",
    "    ):\n",
    "        \"\"\"Pre-processes all data and saves as .pt in cache at once.\"\"\"\n",
    "\n",
    "        if self.recordings is None:\n",
    "            self.partition_datasets()\n",
    "\n",
    "        if self.dataloader is None:\n",
    "            self.dataloader = self.get_dataloader(\n",
    "                buffer_size, num_workers, max_cache_size\n",
    "            )\n",
    "\n",
    "        total_recordings, remaining = len(self.recordings), len(self.recordings)\n",
    "        pbar = tqdm(total=total_recordings, desc=\"Loading recordings\")\n",
    "\n",
    "        self.dataloader.start_fetching(self.recordings)\n",
    "\n",
    "        while True:\n",
    "            recording = self.dataloader.get_recording()\n",
    "            if recording is None:\n",
    "                break\n",
    "            remaining -= 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    def save(self, name: str):\n",
    "        \"\"\"Saves the model and logs to the save path.\"\"\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Training session config\n",
    "            if not os.path.exists(self.save_path):\n",
    "                os.makedirs(self.save_path)\n",
    "                config = self.config.to_dict()\n",
    "                with open(self.save_path + \"/training_config.json\", \"w\") as json_file:\n",
    "                    json.dump(config, json_file, indent=4)\n",
    "\n",
    "            checkpoint_path = f\"{self.save_path}/{name}\"\n",
    "\n",
    "            # Save model\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"config\": self.config.to_dict(),\n",
    "                    \"model\": self.model.cpu().state_dict(),\n",
    "                    \"conditions\": self.config.brain_encoder_config.self.condition_to_idx,\n",
    "                    \"optimizer\": self.optimizer.state_dict(),\n",
    "                    \"scaler\": self.scaler.state_dict(),\n",
    "                    \"error\": str(self.error) if self.error else \"No errors.\",\n",
    "                },\n",
    "                f\"{checkpoint_path}/model.pt\",\n",
    "            )\n",
    "\n",
    "            # Save metrics\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"metrics\": self.metrics,\n",
    "                    \"error\": str(self.error) if self.error else \"No errors.\",\n",
    "                },\n",
    "                f\"{checkpoint_path}/metrics.pt\",\n",
    "            )\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "def load_training_session(\n",
    "    save_path: str,\n",
    "    studies: tp.Dict[str, str] = None,\n",
    "    data_path: str = \"/home/ubuntu/brain-decoding/data\",\n",
    "    clear_cache: bool = False,\n",
    "    cache_enabled: bool = True,\n",
    "    max_cache_size: int = 100,\n",
    "):\n",
    "    \"\"\"Loads a training session from the save path.\"\"\"\n",
    "    # Load training session config\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        raise FileNotFoundError(f\"Save path {save_path} does not exist.\")\n",
    "\n",
    "    try:\n",
    "        load = torch.load(f\"{save_path}/model.pt\")\n",
    "        config = load[\"config\"]\n",
    "        config = TrainingConfigV0().from_dict(config)\n",
    "\n",
    "        training_session = TrainingSessionV0(\n",
    "            config=config,\n",
    "            studies=studies,\n",
    "            data_path=data_path,\n",
    "            save_path=save_path,\n",
    "            clear_cache=clear_cache,\n",
    "            cache_enabled=cache_enabled,\n",
    "            max_cache_size=max_cache_size,\n",
    "        )\n",
    "        \n",
    "        # Load model\n",
    "        training_session.model.load_state_dict(load[\"model\"])\n",
    "        training_session.optimizer.load_state_dict(load[\"optimizer\"])\n",
    "        training_session.scaler.load_state_dict(load[\"scaler\"])\n",
    "        training_session.error = load[\"error\"]\n",
    "\n",
    "        # Load metrics\n",
    "        metrics = torch.load(f\"{save_path}/metrics.pt\")\n",
    "        training_session.metrics = metrics[\"metrics\"]\n",
    "        \n",
    "        if training_session.model.condition_to_idx.keys() != load[\"conditions\"]:\n",
    "            raise ValueError(\"Condition to idx mismatch.\")\n",
    "\n",
    "        return training_session\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading training session config, {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
