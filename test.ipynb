{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 03:49:39,746\tINFO worker.py:1821 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from studies.study_factory import StudyFactory\n",
    "from dataloader.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    buffer_size=10,\n",
    "    max_cache_size_gb=100,\n",
    "    cache_dir=\"cache\",\n",
    "    notch_filter=True,\n",
    "    frequency_bands={\"all\": (0.5, 100)},\n",
    "    scaling=\"both\",\n",
    "    brain_clipping=20,\n",
    "    baseline_window=0.5,\n",
    "    new_freq=100,\n",
    "    batch_types={\"audio\": 12},\n",
    "    batch_kwargs={\n",
    "        'audio': {\n",
    "            'max_random_shift': 1,\n",
    "            'window_size': 4,\n",
    "            'window_stride': 1, \n",
    "            'audio_sample_rate': 16000,\n",
    "            'hop_length': 160,\n",
    "            'audio_processor': \"openai/whisper-large-v3\"\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gwilliams2023 with batch type audio\n"
     ]
    }
   ],
   "source": [
    "study = StudyFactory.create_study(\n",
    "    study_name='gwilliams2023',\n",
    "    batch_type='audio',\n",
    "    path='data/gwilliams2023',\n",
    "    cache_enabled=True,\n",
    "    max_cache_size=200, # in items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "flat_recordings = list(chain.from_iterable(chain.from_iterable(study.recordings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total recordings: 196\n",
      "Batch 3732 (10 recordings) processed in 7.29s\n",
      "Average processing time per recording: 0.73s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 10102 (20 recordings) processed in 17.08s\n",
      "Average processing time per recording: 0.88s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 17724 (30 recordings) processed in 27.15s\n",
      "Average processing time per recording: 0.91s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 23418 (40 recordings) processed in 37.23s\n",
      "Average processing time per recording: 0.93s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 30068 (50 recordings) processed in 46.93s\n",
      "Average processing time per recording: 0.94s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 35038 (60 recordings) processed in 49.15s\n",
      "Average processing time per recording: 0.82s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 39827 (70 recordings) processed in 58.14s\n",
      "Average processing time per recording: 0.83s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 48620 (80 recordings) processed in 73.35s\n",
      "Average processing time per recording: 0.92s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 54140 (90 recordings) processed in 85.08s\n",
      "Average processing time per recording: 0.95s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 61124 (100 recordings) processed in 96.25s\n",
      "Average processing time per recording: 0.96s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 67372 (110 recordings) processed in 106.16s\n",
      "Average processing time per recording: 0.97s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 73236 (120 recordings) processed in 118.15s\n",
      "Average processing time per recording: 0.98s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 78273 (130 recordings) processed in 124.28s\n",
      "Average processing time per recording: 0.96s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 84618 (140 recordings) processed in 129.17s\n",
      "Average processing time per recording: 0.92s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 90418 (150 recordings) processed in 137.15s\n",
      "Average processing time per recording: 0.91s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 96453 (160 recordings) processed in 148.67s\n",
      "Average processing time per recording: 0.93s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 103129 (170 recordings) processed in 159.01s\n",
      "Average processing time per recording: 0.94s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 109722 (180 recordings) processed in 165.47s\n",
      "Average processing time per recording: 0.92s\n",
      "Average processing time per batch: 0.00s\n",
      "Batch 115005 (190 recordings) processed in 169.03s\n",
      "Average processing time per recording: 0.89s\n",
      "Average processing time per batch: 0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(raylet)\u001b[0m Spilled 2607 MiB, 6 objects, write throughput 1056 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 4355 MiB, 9 objects, write throughput 1132 MiB/s.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 16605 MiB, 42 objects, write throughput 1800 MiB/s.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 33088 MiB, 85 objects, write throughput 2009 MiB/s.\n"
     ]
    }
   ],
   "source": [
    "# # Start background fetching\n",
    "import time\n",
    "\n",
    "\n",
    "dataloader.start_fetching(flat_recordings, cache=True)\n",
    "\n",
    "# Process batches as they become available\n",
    "try:\n",
    "    batches, recs, start_time = 0, 0, time.time()\n",
    "    print(f'Total recordings: {len(flat_recordings)}')\n",
    "    \n",
    "    while True:\n",
    "        batch = dataloader.get_recording()\n",
    "        \n",
    "        if batch is None:\n",
    "            break\n",
    "        \n",
    "        brain = batch.brain_segments['all']\n",
    "        batches += brain.shape[0]\n",
    "        recs += 1\n",
    "        \n",
    "        if recs % 10 == 0:\n",
    "            print(f\"Batch {batches} ({recs} recordings) processed in {time.time() - start_time:.2f}s\")\n",
    "            print(\n",
    "                f\"Average processing time per recording: {(time.time() - start_time) / recs:.2f}s\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Average processing time per batch: {(time.time() - start_time) / batches:.2f}s\"\n",
    "            )\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")\n",
    "    dataloader.stop()\n",
    "except Exception as e:\n",
    "    print(\"Error\", e)\n",
    "    dataloader.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
