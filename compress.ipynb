{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zstandard as zstd\n",
    "from hashlib import sha256\n",
    "import tarfile\n",
    "import shutil\n",
    "import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "\n",
    "def _calculate_checksum(file_path: str):\n",
    "    sha256_hash = sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "\n",
    "def _compress_single_directory(args):\n",
    "    \"\"\"Helper function to compress a single directory for parallel processing.\"\"\"\n",
    "    directory_path, output_path, compression_level = args\n",
    "    try:\n",
    "        cctx = zstd.ZstdCompressor(\n",
    "            level=compression_level, threads=-1  # Use all available threads within zstd\n",
    "        )\n",
    "        zst_path = output_path + \".tar.zst\"\n",
    "\n",
    "        # Create a zstd compressed tar archive without creating an intermediate tar file\n",
    "        with open(zst_path, \"wb\") as zst_out:\n",
    "            with cctx.stream_writer(zst_out) as compressor:\n",
    "                with tarfile.open(fileobj=compressor, mode=\"w|\") as tar:\n",
    "                    tar.add(directory_path, arcname=os.path.basename(directory_path))\n",
    "\n",
    "        # Calculate checksum for the compressed file\n",
    "        checksum = _calculate_checksum(zst_path)\n",
    "        return (True, os.path.basename(directory_path), checksum)\n",
    "    except Exception as e:\n",
    "        return (False, os.path.basename(directory_path), str(e))\n",
    "\n",
    "\n",
    "def compress_directories(\n",
    "    base_path: str,\n",
    "    destination_path: str,\n",
    "    checksum_file_name: str,\n",
    "    compression_level: int,\n",
    "    num_workers: int = None,\n",
    "):\n",
    "    \"\"\"Compresses all directories in the base_path into the destination_path.\n",
    "    Saving checksums for later corruption verification. All non-directory files\n",
    "    are copied to the destination directory as is.\n",
    "\n",
    "    Parameters:\n",
    "        base_path (str): The path to the base directory containing directories to compress.\n",
    "        destination_path (str): The path where compressed files and checksums will be stored.\n",
    "        checksum_file_name (str): The name of the checksum file.\n",
    "        compression_level (int): The compression level for zstd.\n",
    "        num_workers (int or str, optional): Number of worker processes to use. If 'mac', multiprocessing is disabled.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if num_workers is 'mac', disable multiprocessing\n",
    "    use_multiprocessing = True\n",
    "    if num_workers == \"mac\":\n",
    "        use_multiprocessing = False\n",
    "    elif num_workers is None:\n",
    "        num_workers = max(1, cpu_count() - 1)  # Leave one CPU for other tasks\n",
    "\n",
    "    directories = [\n",
    "        d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))\n",
    "    ]\n",
    "\n",
    "    checksums = {}\n",
    "\n",
    "    if not os.path.exists(destination_path):\n",
    "        os.makedirs(destination_path)\n",
    "\n",
    "    if use_multiprocessing:\n",
    "        # Prepare arguments for multiprocessing\n",
    "        tasks = []\n",
    "        for directory in sorted(directories):\n",
    "            directory_path = os.path.join(base_path, directory)\n",
    "            output_path = os.path.join(destination_path, directory)\n",
    "            task = (directory_path, output_path, compression_level)\n",
    "            tasks.append(task)\n",
    "\n",
    "        # Compress in parallel using multiprocessing\n",
    "        print(f\"Compressing {len(tasks)} directories using {num_workers} processes...\")\n",
    "        with Pool(processes=num_workers) as pool:\n",
    "            results = list(\n",
    "                tqdm.tqdm(\n",
    "                    pool.imap_unordered(_compress_single_directory, tasks),\n",
    "                    total=len(tasks),\n",
    "                    desc=\"Compressing directories\",\n",
    "                )\n",
    "            )\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    else:\n",
    "        # Single-threaded compression\n",
    "        results = []\n",
    "        for directory in tqdm.tqdm(sorted(directories), desc=\"Compressing directories\"):\n",
    "            directory_path = os.path.join(base_path, directory)\n",
    "            output_path = os.path.join(destination_path, directory)\n",
    "            success, dir_name, result = _compress_single_directory(\n",
    "                (directory_path, output_path, compression_level)\n",
    "            )\n",
    "            results.append((success, dir_name, result))\n",
    "\n",
    "    # Process results\n",
    "    for success, directory_name, result in results:\n",
    "        if success:\n",
    "            checksums[directory_name] = result  # result is the checksum\n",
    "        else:\n",
    "            print(f\"Failed to compress {directory_name}: {result}\")\n",
    "\n",
    "    # Save checksums to a file for later verification\n",
    "    with open(os.path.join(destination_path, checksum_file_name), \"w\") as f:\n",
    "        for directory, checksum in checksums.items():\n",
    "            f.write(f\"{directory}: {checksum}\\n\")\n",
    "\n",
    "    # Copy remaining files to the destination directory\n",
    "    for file in os.listdir(base_path):\n",
    "        item_path = os.path.join(base_path, file)\n",
    "        if os.path.isfile(item_path):\n",
    "            destination_file_path = os.path.join(destination_path, file)\n",
    "            shutil.copy2(item_path, destination_file_path)\n",
    "            print(f\"Copied {file} to {destination_path}\")\n",
    "\n",
    "    print(\"Compression complete.\")\n",
    "\n",
    "\n",
    "def _decompress_single_file(args):\n",
    "    \"\"\"Helper function to decompress a single file for parallel processing.\"\"\"\n",
    "    tar_path, destination_path, directory_name, expected_checksum = args\n",
    "    try:\n",
    "        # Verify checksum\n",
    "        actual_checksum = _calculate_checksum(tar_path)\n",
    "        if expected_checksum != actual_checksum:\n",
    "            return (False, os.path.basename(tar_path), \"Checksum verification failed\")\n",
    "\n",
    "        # Decompress and extract using streams\n",
    "        with open(tar_path, \"rb\") as compressed:\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            with dctx.stream_reader(compressed) as reader:\n",
    "                # Use tarfile in streaming mode\n",
    "                with tarfile.open(fileobj=reader, mode=\"r|*\") as tar:\n",
    "                    tar.extractall(path=destination_path)\n",
    "        return (True, os.path.basename(tar_path), \"Success\")\n",
    "    except Exception as e:\n",
    "        return (False, os.path.basename(tar_path), str(e))\n",
    "\n",
    "\n",
    "def decompress_directories(\n",
    "    source_path: str,\n",
    "    destination_path: str,\n",
    "    checksum_file_name: str,\n",
    "    num_workers: int = None,\n",
    "):\n",
    "    \"\"\"Decompresses directories from the source_path to the destination_path.\n",
    "    Verifies checksums to detect any corruption.\n",
    "\n",
    "    Parameters:\n",
    "        source_path (str): The path containing compressed files and checksum file.\n",
    "        destination_path (str): The path where decompressed files will be stored.\n",
    "        checksum_file_name (str): The name of the checksum file.\n",
    "        num_workers (int or str, optional): Number of worker processes to use. If 'mac', multiprocessing is disabled.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if num_workers is 'mac', disable multiprocessing\n",
    "    use_multiprocessing = True\n",
    "    if num_workers == \"mac\":\n",
    "        use_multiprocessing = False\n",
    "    elif num_workers is None:\n",
    "        num_workers = max(1, cpu_count() - 1)  # Leave one CPU for other tasks\n",
    "\n",
    "    with open(os.path.join(source_path, checksum_file_name), \"r\") as f:\n",
    "        checksums = {\n",
    "            line.split(\": \")[0]: line.split(\": \")[1].strip() for line in f.readlines()\n",
    "        }\n",
    "\n",
    "    tar_files = [\n",
    "        f\n",
    "        for f in os.listdir(source_path)\n",
    "        if os.path.isfile(os.path.join(source_path, f)) and f.endswith(\".tar.zst\")\n",
    "    ]\n",
    "\n",
    "    corrupted_files = []\n",
    "\n",
    "    if not os.path.exists(destination_path):\n",
    "        os.makedirs(destination_path)\n",
    "\n",
    "    if use_multiprocessing:\n",
    "        # Prepare arguments for multiprocessing\n",
    "        tasks = []\n",
    "        for tar_file in sorted(tar_files):\n",
    "            directory_name = tar_file.split(\".\")[0]\n",
    "            tar_path = os.path.join(source_path, tar_file)\n",
    "            expected_checksum = checksums.get(directory_name, None)\n",
    "            if expected_checksum is None:\n",
    "                print(f\"No checksum found for {directory_name}, skipping.\")\n",
    "                continue\n",
    "            task = (tar_path, destination_path, directory_name, expected_checksum)\n",
    "            tasks.append(task)\n",
    "\n",
    "        # Decompress in parallel using multiprocessing\n",
    "        print(f\"Decompressing {len(tasks)} files using {num_workers} processes...\")\n",
    "        with Pool(processes=num_workers) as pool:\n",
    "            results = list(\n",
    "                tqdm.tqdm(\n",
    "                    pool.imap_unordered(_decompress_single_file, tasks),\n",
    "                    total=len(tasks),\n",
    "                    desc=\"Decompressing files\",\n",
    "                )\n",
    "            )\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    else:\n",
    "        # Single-threaded decompression\n",
    "        results = []\n",
    "        for tar_file in tqdm.tqdm(sorted(tar_files), desc=\"Decompressing files\"):\n",
    "            directory_name = tar_file.split(\".\")[0]\n",
    "            tar_path = os.path.join(source_path, tar_file)\n",
    "            expected_checksum = checksums.get(directory_name, None)\n",
    "            if expected_checksum is None:\n",
    "                print(f\"No checksum found for {directory_name}, skipping.\")\n",
    "                continue\n",
    "            success, filename, message = _decompress_single_file(\n",
    "                (tar_path, destination_path, directory_name, expected_checksum)\n",
    "            )\n",
    "            results.append((success, filename, message))\n",
    "\n",
    "    # Process results\n",
    "    for success, filename, message in results:\n",
    "        if not success:\n",
    "            corrupted_files.append((filename, message))\n",
    "\n",
    "    # Copy non-compressed files to the destination\n",
    "    for item in os.listdir(source_path):\n",
    "        item_path = os.path.join(source_path, item)\n",
    "        if (\n",
    "            os.path.isfile(item_path)\n",
    "            and not item.endswith(\".tar.zst\")\n",
    "            and item != checksum_file_name\n",
    "        ):\n",
    "            destination_file_path = os.path.join(destination_path, item)\n",
    "            shutil.copy2(item_path, destination_file_path)\n",
    "            print(f\"Copied {item} to {destination_path}\")\n",
    "\n",
    "    print(\"Decompression and verification complete\")\n",
    "    if corrupted_files:\n",
    "        print(\"The following files failed:\")\n",
    "        for filename, error in corrupted_files:\n",
    "            print(f\"{filename}: {error}\")\n",
    "    else:\n",
    "        print(\"All files decompressed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing directories: 100%|██████████| 28/28 [14:06<00:00, 30.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied .DS_Store to compressed_data/gwilliams\n",
      "Copied dataset_description.json to compressed_data/gwilliams\n",
      "Copied README.txt to compressed_data/gwilliams\n",
      "Copied participants.json to compressed_data/gwilliams\n",
      "Copied participants.tsv to compressed_data/gwilliams\n",
      "Compression complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_path = \"data/gwilliams\"\n",
    "destination_path = \"compressed_data/gwilliams\"\n",
    "\n",
    "compress_directories(\n",
    "    base_path,\n",
    "    destination_path,\n",
    "    checksum_file_name=\"checksums.txt\",\n",
    "    compression_level=12,\n",
    "    num_workers='mac'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = \"data/schoffelen\"\n",
    "# destination_path = \"compressed_data/schoffelen\"\n",
    "\n",
    "# compress_directories(\n",
    "#     base_path,\n",
    "#     destination_path,\n",
    "#     checksum_file_name=\"checksums.txt\",\n",
    "#     compression_level=12,\n",
    "#     num_workers='mac'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing files: 100%|██████████| 28/28 [03:58<00:00,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied .DS_Store to decompressed_data/gwilliams\n",
      "Copied dataset_description.json to decompressed_data/gwilliams\n",
      "Copied README.txt to decompressed_data/gwilliams\n",
      "Copied participants.json to decompressed_data/gwilliams\n",
      "Copied participants.tsv to decompressed_data/gwilliams\n",
      "Decompression and verification complete\n",
      "All files decompressed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_path = \"compressed_data/gwilliams\"\n",
    "destination_path = \"decompressed_data/gwilliams\"\n",
    "\n",
    "decompress_directories(\n",
    "    base_path,\n",
    "    destination_path,\n",
    "    checksum_file_name=\"checksums.txt\",\n",
    "    num_workers='mac'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing files: 100%|██████████| 28/28 [04:02<00:00,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied .DS_Store to decompressed_data/gwilliams\n",
      "Copied dataset_description.json to decompressed_data/gwilliams\n",
      "Copied README.txt to decompressed_data/gwilliams\n",
      "Copied participants.json to decompressed_data/gwilliams\n",
      "Copied participants.tsv to decompressed_data/gwilliams\n",
      "Decompression and verification complete\n",
      "All files decompressed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_path = \"downloaded_data/gwilliams\"\n",
    "destination_path = \"decompressed_data/gwilliams\"\n",
    "\n",
    "decompress_directories(\n",
    "    base_path,\n",
    "    destination_path,\n",
    "    checksum_file_name=\"checksums.txt\",\n",
    "    num_workers='mac'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
